---
title: "Kaggle Housing Prices Competition"
description: "How I achieved a top 2% ranking."
author: "John King"
date: "7/29/2020"
format:
  html:
    toc: true
    code-fold: false
    code-tools: true
    code-copy: true
    df-print: paged
execute: 
  warning: false
  message: false
  echo: true
categories:
  - R
  - machine learning
image: "teaser.png"
---

## Purpose

This notebook was written to document the steps and techniques used to achieve a top 2% ranking on Kaggle's [housing prices competition](https://www.kaggle.com/c/home-data-for-ml-course).

## Import Libraries

This notebook uses the following packages. Commented out packages are used later in the notebook but are not explicitly loaded into the namespace.

```{r}
library(tidyverse)        # used for dataframe manipulation
library(forcats)          # functions for categorical variables
library(simputation)      # for imputing NAs
#library(dlookr)          # for detecting skewness
#library(bestNormalize)   # for normalizing skewed data
#library(h2o)             # for model building
```

## Import Data

I downloaded the training and test sets locally just because, so here I read the files and combine them into one data frame. I didn't use the test set for anything other than identifying all of the possible factor levels, and combining the data sets made that task easier.

```{r}
ames_train = read.csv("train.csv", stringsAsFactors = F)
ames_test = read.csv("test.csv", stringsAsFactors = F)

ames = bind_rows(ames_train %>% mutate(data = 'train'), 
                 ames_test %>% mutate(data= 'test'))
```

## Identify and Correct Feature Types

Next, I convert character columns to factors and get a list of the factor names. The `MSSubClass` feature is numeric, but after reading the data description, it really should be a factor. It also doesn't make sense to leave `MoSold` as a number since February isn't two times greater then January. I also will convert `YrSold` to a factor, but not yet since I'll do some math with it first.

```{r}
ames = ames %>% mutate_if(is_character, as_factor)

# convert from number to factor
ames$MSSubClass = as.factor(ames$MSSubClass)
ames$MoSold = as.factor(ames$MoSold)
```

## Numeric Features

I'll start with the numeric features, which have quite a few missing values. My strategy for replacing the NAs is in the code comments.

```{r}
# impute NAs as a function of LotArea grouped by Neighborhood
ames = impute_median(ames, LotFrontage ~ Neighborhood)

# replace NAs with 0
ames[is.na(ames$MasVnrArea), 'MasVnrArea'] = 0
ames[is.na(ames$BsmtFinSF1), 'BsmtFinSF1'] = 0
ames[is.na(ames$BsmtFinSF2), 'BsmtFinSF2'] = 0
ames[is.na(ames$BsmtUnfSF), 'BsmtUnfSF'] = 0
ames[is.na(ames$TotalBsmtSF), 'TotalBsmtSF'] = 0
ames[is.na(ames$BsmtFullBath), 'BsmtFullBath'] = 0
ames[is.na(ames$BsmtHalfBath), 'BsmtHalfBath'] = 0
ames[is.na(ames$GarageCars), 'GarageCars'] = 0
ames[is.na(ames$GarageArea), 'GarageArea'] = 0
ames[is.na(ames$GarageYrBlt), 'GarageYrBlt'] = 0
```

With the NAs taken care of, I created new features that I thought might improve the model performance.

```{r}
ames = ames %>% mutate(
  TotalPorchSF = OpenPorchSF + EnclosedPorch + X3SsnPorch + ScreenPorch + WoodDeckSF,
  Baths = FullBath + 0.5*HalfBath + BsmtFullBath + 0.5*BsmtHalfBath,
  AgeWhenSold = YrSold - YearBuilt,
  AgeOfRemodel = YrSold - YearRemodAdd,
  TotalWalledArea = TotalBsmtSF + GrLivArea,
  TotalOccupiedArea = TotalPorchSF + TotalWalledArea,
  OtherRooms = TotRmsAbvGrd - BedroomAbvGr - KitchenAbvGr,
  LotDepth = LotArea / LotFrontage,
  TotalSF = TotalBsmtSF + X1stFlrSF + X2ndFlrSF,
)
```

Now that I'm done with mathematical operations using `YrSold` , I convert it to a factor.

```{r}
ames$YrSold = as.factor(ames$YrSold)
```

Next, I find the features that contain the same value in more than 99.5% of the observations. These will provide virtually no information to help explain why `SalesPrice` varies, so I drop the features. Turns out there's just one: `PoolArea`.

```{r}
(to_drop = colnames(ames)[which(ames %>% summarise_all(funs(sum(.==0))) / nrow(ames) >0.995)])
ames = ames %>% select(-all_of(to_drop))
```

The last step is to normalize the numeric features that have a skewed distribution.

```{r}
sk = dlookr::find_skewness(ames, thres = 0.5)

for (i in sk){ames[, i] = bestNormalize::bestNormalize(ames[, i])$x.t}
```

## Categorical Features

The first step for the categorical features is to replace the NAs using the following strategy:

-   `MSZoning`: impute missing values using k-nearest neighbors grouped by `MSSubClass`.

-   `Exterior1st` through `SaleType`: these each have only one missing value, so I impute missing values using the mode. Oddly, *R* doesn't have a built-in mode function, so I wrote my own.

-   All others: replace missing values with a new level.

```{r}
# impute missing MSZoning values using KNN grouped by MSSubclass
ames = impute_knn(ames, MSZoning~. -SalePrice | MSSubClass)

# replace with mode of feature
Mode <- function(x) {
    ux <- unique(x)
    return(as.character(ux[which.max(tabulate(match(x, ux)))]))}

ames$Exterior1st = forcats::fct_explicit_na(ames$Exterior1st, Mode(ames$Exterior1st))
ames$Exterior2nd = forcats::fct_explicit_na(ames$Exterior2nd, Mode(ames$Exterior2nd))
ames$Electrical = forcats::fct_explicit_na(ames$Electrical, Mode(ames$Electrical))
ames$KitchenQual = forcats::fct_explicit_na(ames$KitchenQual, Mode(ames$KitchenQual))
ames$Functional = forcats::fct_explicit_na(ames$Functional, Mode(ames$Functional))
ames$SaleType = forcats::fct_explicit_na(ames$SaleType, Mode(ames$SaleType))

# replace other missing NAs
ames$Alley = forcats::fct_explicit_na(ames$Alley, 'Missing')
ames$MasVnrType = forcats::fct_explicit_na(ames$MasVnrType, 'None')
ames$GarageType = forcats::fct_explicit_na(ames$GarageType, "None")
ames$MiscFeature = forcats::fct_explicit_na(ames$MiscFeature, na_level="Missing")
ames$Fence = forcats::fct_explicit_na(ames$Fence, 'None')
```

Quite a few of the categorical features are Likert-scale responses, so I convert them to integers and group levels when there are less than five. Also, `h2o`, which I later use for model building, doesn't recognize *R*'s ordered factor data type, so integers it is.

```{r}
ames = ames %>% mutate(ExterQual = case_when(ExterQual == 'Fa' ~ 1,
                                             ExterQual == 'Po' ~ 1,
                                             ExterQual == 'TA' ~ 2,
                                             ExterQual == 'Gd' ~ 3,
                                             ExterQual == 'Ex' ~ 4))

ames = ames %>% mutate(ExterCond = case_when(ExterCond == 'Po' ~ 1,
                                             ExterCond == 'Fa' ~ 1,
                                             ExterCond == 'TA' ~ 2,
                                             ExterCond == 'Gd' ~ 3,
                                             ExterCond == 'Ex' ~ 3))

ames = ames %>% mutate(BsmtQual = case_when(is.na(BsmtQual) ~ 0,
                                            BsmtQual == 'Fa' ~ 1,
                                            BsmtQual == 'TA' ~ 2,
                                            BsmtQual == 'Gd' ~ 3,
                                            BsmtQual == 'Ex' ~ 4),
                       BsmtCond = case_when(is.na(BsmtCond) ~ 0,
                                            BsmtCond == 'Po' ~ 1,
                                            BsmtCond == 'Fa' ~ 1,
                                            BsmtCond == 'TA' ~ 2,
                                            BsmtCond == 'Gd' ~ 3,
                                            BsmtCond == 'Ex' ~ 4),
                       BsmtExposure = case_when(is.na(BsmtExposure) ~ 0,
                                                BsmtExposure == 'No' ~ 1,
                                                BsmtExposure == 'Mn' ~ 2,
                                                BsmtExposure == 'Av' ~ 3,
                                                BsmtExposure == 'Gd' ~ 4),
                       BsmtFinType1 = case_when(is.na(BsmtFinType1) ~ 0,
                                                BsmtFinType1 == 'Unf' ~ 1,
                                                BsmtFinType1 == 'LwQ' ~ 2,
                                                BsmtFinType1 == 'Rec' ~ 3,
                                                BsmtFinType1 == 'BLQ' ~ 4,
                                                BsmtFinType1 == 'ALQ' ~ 5,
                                                BsmtFinType1 == 'GLQ' ~ 6),
                       BsmtFinType2 = case_when(is.na(BsmtFinType2) ~ 0,
                                                BsmtFinType2 == 'Unf' ~ 1,
                                                BsmtFinType2 == 'LwQ' ~ 2,
                                                BsmtFinType2 == 'Rec' ~ 3,
                                                BsmtFinType2 == 'BLQ' ~ 4,
                                                BsmtFinType2 == 'ALQ' ~ 5,
                                                BsmtFinType2 == 'GLQ' ~ 6))

ames = ames %>% mutate(HeatingQC = case_when(HeatingQC == 'Po' ~ 1,
                                             HeatingQC == 'Fa' ~ 1,
                                             HeatingQC == 'TA' ~ 2,
                                             HeatingQC == 'Gd' ~ 3,
                                             HeatingQC == 'Ex' ~ 4))

ames = ames %>% mutate(KitchenQual = case_when(is.na(KitchenQual) ~ 0,
                                               KitchenQual == 'Fa' ~ 1,
                                               KitchenQual == 'TA' ~ 2,
                                               KitchenQual == 'Gd' ~ 3,
                                               KitchenQual == 'Ex' ~ 4))

ames = ames %>% mutate(Functional = case_when(is.na(Functional) ~ 0,
                                              Functional == 'Sev' ~ 1,
                                              Functional == 'Maj2' ~ 1,
                                              Functional == 'Maj1' ~ 2,
                                              Functional == 'Mod' ~ 3,
                                              Functional == 'Min2' ~ 4,
                                              Functional == 'Min1' ~ 5,
                                              Functional == 'Typ' ~ 6))

ames = ames %>% mutate(FireplaceQu = case_when(is.na(FireplaceQu) ~ 0,
                                               FireplaceQu == 'Po' ~ 1,
                                               FireplaceQu == 'Fa' ~ 2,
                                               FireplaceQu == 'TA' ~ 3,
                                               FireplaceQu == 'Gd' ~ 4,
                                               FireplaceQu == 'Ex' ~ 5))

ames = ames %>% mutate(GarageFinish = case_when(is.na(GarageFinish) ~ 0,
                                                GarageFinish == 'Unf' ~ 1,
                                                GarageFinish == 'RFn' ~ 2,
                                                GarageFinish == 'Fin' ~ 3))

ames = ames %>% mutate(GarageQual = case_when(is.na(GarageQual) ~ 0,
                                              GarageQual == 'Po' ~ 1,
                                              GarageQual == 'Fa' ~ 1,
                                              GarageQual == 'TA' ~ 2,
                                              GarageQual == 'Gd' ~ 3,
                                              GarageQual == 'Ex' ~ 3))

ames = ames %>% mutate(GarageCond = case_when(is.na(GarageCond) ~ 0,
                                              GarageCond == 'Po' ~ 1,
                                              GarageCond == 'Fa' ~ 2,
                                              GarageCond == 'TA' ~ 3,
                                              GarageCond == 'Gd' ~ 4,
                                              GarageCond == 'Ex' ~ 4))
```

As with the numeric features, I don't want to have a factor level that appears in less than 0.5% of the data, so for each feature, I lumped them into a single level.

```{r}
factor_cols = colnames(ames %>% select_if(is.factor))

for (fac in factor_cols){
  ames[ , fac] = fct_lump_prop(ames[, fac], prop=0.005)
}
```

Then I ceated a couple of new categorical features to describe toal garage and exterior quality. I tried several other new categorical features but they turned out to not be useful.

```{r}
ames$TotalGarageQual = ames$GarageQual * ames$GarageCond
ames$TotalExteriorQual = ames$ExterQual * ames$ExterCond
```

## Unusual Observations

The final step was to identify and drop unusual observations from the training data set. I took a simple approach of fitting a linear model to the numeric features and dropping the observations with a Cook's distance \> 0.5.

```{r}
df = ames %>% filter(data=='train') %>% select(-all_of(factor_cols))

df.lm = lm(SalePrice ~ ., data=df)
plot(df.lm, which =4)

ames = ames %>% filter(!Id %in% c(524, 1299))
```

A final scrub of the features revealed that four more features whould be dropped. `Id` is just the the row number, so of no use. The other three contained a single level that dominated the feature, just not at the 99.5% threshold I used earlier. I then split the data back into separate training and test sets. I also noticed that the response variable distribution was a little skewed, so transformed it using the log of the response.

```{r message=FALSE, warning=FALSE}
# drop factors
train_test = ames %>% select(-Id, -Street, -Utilities, -PoolQC)

# split back into train and test sets
train_df = train_test %>% filter(data=='train') %>% select(-SalePrice, everything()) %>% select(-data)
test_df = train_test %>% filter(data=='test') %>% select(-data, -SalePrice)

# transform the response to correct skewness
train_df$SalePrice = log(train_df$SalePrice)

#
# Model
#
library(h2o)
h2o.init()

# convert data to h2o frames
train_full = as.h2o(train_df)
test_set = as.h2o(test_df)
```

I used `h2o`'s `automl()` function to fit an array of distributed random forests, gradient boosted machines, and generalized linear models and also fit some ensembles models. The best performing model turned out to be a generalized linear model, which I didn't expect.

```{r}
aml = h2o.automl(x = 1:86, y = 87,
                 training_frame = train_full,
                 exclude_algos = "DeepLearning",
                 nfolds = 10, 
                 seed=1)

# aml@leader
# best yet!
# Mean Residual Deviance :  0.009144619

# View the AutoML Leaderboard
lb <- aml@leaderboard
print(lb, n = nrow(lb)) 
```

Lastly, I used the best performing model to make predictions on the test set (after un-doing the log transformation), and submitted those predictions to the Kaggle competition.

```{r}
aml_preds = exp(h2o.predict(aml@leader, newdata = test_set))

submisn = tibble(
  Id = 1461:2919,
  SalePrice = as.vector(aml_preds)
)

head(submisn)

h2o.shutdown(prompt=FALSE)
```
