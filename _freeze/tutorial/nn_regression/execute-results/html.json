{
  "hash": "0df4754512646a529053bfa610643967",
  "result": {
    "markdown": "---\ntitle: \"Neural Network Regression\"\nauthor: \"John King\"\ndate: \"6/2/2020\"\nformat:\n  html:\n    toc: true\n    code-copy: true\n    df-print: paged\nexecute: \n  warning: false\n  echo: true\nbibliography: references.bib\nlink-citations: true\n---\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n## Neural Network Regression\n\nLike support vector machines and tree-based models, neural networks can be applied to both regression and classification tasks. Neural networks originated in the field of neurophysiology as an attempt to model human brain activity at the neuron level [@mcculloch1943], but it wasn't until the 1980s and 1990s [@bishop1995] that they began to be developed into their current form. Neural network models are fit using a training algorithm that slowly reduces prediction error using a process called gradient descent.\n\n### Simple Neural Network Model\n\nBefore we get into that, let's look at visualization of a neural network regression model in it's simplest form: one to solve for $\\beta_0$ and $\\beta_1$ given the equation $y=\\beta_0x_0+\\beta_1x_1+\\epsilon$ where we know $x_0=1$. Below, the two blue circles are referred to as the **input layer** and consist of two **nodes**: an input node that will be used to solve for $\\beta_1$, and a bias node to solve for $\\beta_0$, the y-intercept. Each node of the input layer is connected to the output layer, which consists of just one node because we'll be predicting a single continuous variable, $\\hat{y}$. If this was a classification problem, and we were trying to classify the three types of irises found in the `iris` data set, then the output layer would have three nodes, each producing a probability. There is a model parameter, referred to as a **weight**, associated with each connected node as indicated by the $\\omega_0$ and $\\omega_1$ terms. The output node produces a prediction, $\\hat{y}$, using an **activation function**. In the case of linear regression, we use a linear activation function of the form $f \\left( \\sum\\limits_{h}{\\omega_h} x_h \\right)$. That's it - that's the model!\n\n![](images/simple_nn.png){fig-align=\"left\" width=\"600\"}\n\n### Gradient Descent\n\nThe algorithm used to train the model is called gradient descent, and to demonstrate how it works, we need to set the stage first. Let's assume that we're trying to find the $\\beta$s that have the following relationship with the predictor:\n\n$$\ny=1+0.5x+\\epsilon\n$$\n\nWe'll create a data set with 10 observations and fit a linear model for comparison later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nnn_reg = tibble(\n  x = runif(10, 0, 5),\n  y = 1 + 0.5*x + rnorm(10, sd=0.5)\n)\n\nggplot(nn_reg, aes(x=x, y=y)) +\n  geom_point() +\n  geom_smooth(formula='y~x', method='lm', se=FALSE) +\n  coord_fixed(xlim=c(0,5), ylim=c(0,5)) +\n  ggtitle(\"Linear Model Fit\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](nn_regression_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\nnn.lm = lm(y ~ x, data=nn_reg)\n\nsummary(nn.lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x, data = nn_reg)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.67369 -0.38063 -0.08963  0.41550  0.75801 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.4127     0.4516   0.914 0.387494    \nx             0.7641     0.1324   5.773 0.000418 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5164 on 8 degrees of freedom\nMultiple R-squared:  0.8064,\tAdjusted R-squared:  0.7822 \nF-statistic: 33.32 on 1 and 8 DF,  p-value: 0.000418\n```\n:::\n:::\n\n\nBefore the model is trained, its weights are initialized with random numbers. I'll just pick two random numbers between -1 and 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\n(w0 = runif(1, -1, 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8296121\n```\n:::\n\n```{.r .cell-code}\n(w1 = runif(1, -1, 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8741508\n```\n:::\n:::\n\n\nSince these model parameters are just random numbers, the predictions will not be very accurate. That's ok, though, and it's the starting point for all untrained neural network models. We'll go through the following iterative process to slowly train the model to make more and more accurate predictions.\n\n**The Training Process**\n\n1.  Make predictions for input values.\n2.  Measure the difference between those predictions and the true values (called the **loss**).\n3.  Compute the partial derivative (gradient) of the loss with respect to the model parameters.\n4.  Update the model parameters using the partial derivative values computed in the previous step.\n5.  Repeat this process until the loss is either unchanged or is sufficiently low.\n\nThe next several code chunks demonstrate this process one step at a time.\n\n**Step 1. Make predictions.**\n\nWe can make predictions manually using the randomly initialized weight and bias.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_estimate = function(omega0, omega1){nn_reg$x * omega1 + omega0}\n(y_hat = get_estimate(w0, w1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 4.828004 4.925338 2.080258 4.459294 3.634524 3.098453 4.049059 1.418207\n [9] 3.701164 3.911277\n```\n:::\n:::\n\n\n**Step 2. Calculate the loss.**\n\nThere are a number of ways we could do this, but for this example, we'll calculate the loss by determining the mean squared error of the predictions and the target values. Mean squared error is defined as:\n\n$$\nMSE = \\frac{1}{n} \\sum\\limits_{i=1}^{n}{\\left( y_i - \\hat{y}_i \\right)^2}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmse = function(predicted){1/length(predicted)* sum((nn_reg$y - predicted)^2)}\n# the loss\n(loss = mse(y_hat))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8201885\n```\n:::\n:::\n\n\n**Step 3. Compute the partial derivatives of the loss.**\n\nAt this point, we have two random values for $\\omega_0$ and $\\omega_1$ and an associated loss (error). Now we need to find new values for the ωs that will decrease the loss. How do we do that? For each ω, we need to determine whether we should increase or decrease it's value and by how much. We determine whether to increase or decrease its value by calculating the gradient of the loss function at the current ω values. To demonstrate graphically, the loss as a function of $\\omega_0$ and $\\omega_1$ are plotted below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# sequence of w0 values\nBvec = seq(from=(w0-1), to=(w0+1), length.out = 21)\n\n# calculate loss while holding w1 constant\nBloss = Bvec %>% \n  map(function(x) get_estimate(x, w1)) %>% \n  map_dbl(function(x) mse(x))\n\n# get a curve through the points and get the gradient\nBspl = smooth.spline(Bloss ~ Bvec)\n\n# get the gradient at w0\nBgrad = predict(Bspl, x=w0, deriv=1)$y\n\n# same thing for w1\nWvec = seq(from=(w1-1), to=(w1+1), length.out = 21)\n\nWloss = Wvec %>% \n  map(function(x) get_estimate(w0,x)) %>% map_dbl(function(x) mse(x))\n\nWspl = smooth.spline(Wloss ~ Wvec)\n\nWgrad = predict(Wspl, x=w1, deriv=1)$y\n\nw0plot = ggplot() + \n  geom_line(aes(x=Bvec, y=Bloss), color='red', size=1.5) + \n  geom_line(aes(x=c(w0-0.5, w0+0.5), \n                y=c(Bloss[11]-Bgrad/2, Bloss[11]+Bgrad/2)), \n            color='blue', size=1.5) +\n  geom_point(aes(x=Bvec[11], y = Bloss[11]), size=5) +\n  annotate(\"text\", x=0.5, y=2, \n           label=paste(\"Slope =\", round(Bgrad, 2))) +\n  coord_fixed(ylim=c(0,3.5)) +\n  xlab(\"w0\") + ylab(\"Loss\") +\n  theme_bw()\n\nw1plot = ggplot() + \n  geom_line(aes(x=Wvec, y=Wloss), color='red', size=1.5) + \n  geom_line(aes(x=c(w1-0.5, w1+0.5), \n                y=c(Wloss[11]-Wgrad/2, Wloss[11]+Wgrad/2)), \n            color='blue', size=1.5) +\n  geom_point(aes(x=Wvec[11], y=Wloss[11]), size=5) +\n  annotate(\"text\", x=1.4, y=0.5, label=paste(\"Slope =\", round(Wgrad, 2))) +\n  coord_fixed(ylim=c(0,3.5)) +\n  xlab(\"w1\") + ylab(\"Loss\") +\n  theme_bw()\n\ngridExtra::grid.arrange(w0plot, w1plot, nrow=1, ncol=2)\n```\n\n::: {.cell-output-display}\n![](nn_regression_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nIn practice, the partial derivatives are calculated as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(Bpartial = sum(-(nn_reg$y - y_hat)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7.670526\n```\n:::\n\n```{.r .cell-code}\n(Wpartial = sum(-(nn_reg$y - y_hat) * nn_reg$x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 26.07812\n```\n:::\n:::\n\n\n**Step 4. Update the model parameters.**\n\nFrom the above plots, we can see that to decrease the loss, we need to decrease both $\\omega$s. In fact, the following rules always apply:\n\n-   With a *positive* gradient, *decrease* the parameter value.\n\n-   With a *negative* gradient, *increase* the parameter value.\n\nWe know we need to decrease the parameter values, so now we need to determine *how much* to decrease them. Right now, all we have to go on are the magnitude of the gradients. If we decreased the parameters by their respective gradients, the new parameter values would be far to the left on both plots above - we would overshoot the bottom of the curve in both cases. Instead of using the full gradient value, it appears that the parameters should be updated as follows:\n\n$$\n\\omega_{new} = (\\omega_{old}) - (\\omega_{gradient})(\\alpha)\n$$\n\nWhere $\\alpha$ is a multiplier in the range \\[0, 1\\], and is referred to as the **learning rate**. For our example, an α of 0.01 will suffice, but keep in mind that α is a hyperparameter that often must be tuned. The code below selects $\\alpha$, updates the parameter values, and recalculates the loss. Notice that the loss has decreased as expected.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha = 0.001\nw0 = w0 - Bpartial * alpha\nw1 = w1 - Wpartial * alpha\nmse(get_estimate(w0, w1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6816571\n```\n:::\n:::\n\n\nNext we'll put all this in a loop, iterate through a number of times, and see what we get for parameter estimates. I'll start from the beginning and capture the parameters and loss as training progresses through 5000 iterations. Below, the parameter estimates are plotted for each iteration and compared to the linear model coefficients.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nw0 = runif(1, -1, 1)\nw1 = runif(1, -1, 1)\nalpha = 0.001\nw0s = w0\nw1s = w1\nfor (j in 1:500){\n  for (i in 1:nrow(nn_reg)){\n    y_hat = get_estimate(w0, w1)\n    Bgrad = sum(-(nn_reg$y - y_hat))\n    Wgrad = sum(-(nn_reg$y - y_hat) * nn_reg$x)\n    w0 = w0 - Bgrad * 0.001\n    w1 = w1 - Wgrad * 0.001\n    w0s = c(w0s, w0)\n    w1s = c(w1s, w1)\n  }\n}\n\nwHistory = tibble(\n  iter = 1:length(w0s),\n  b = w0s,\n  w = w1s\n)\n\nlibrary(gganimate)\n\nggplot(wHistory) +\n  geom_point(aes(x=iter, y=w0s, color='w_0 Estimate', group=seq_along(iter))) +\n  geom_hline(yintercept=coef(nn.lm)[1]) + \n  annotate(\"text\", x=1200, y=0.43, label=\"Linear Model Intercept Coefficient\") +\n  geom_point(aes(x=iter, y=w1s, color='w_1 Estimate', group=seq_along(iter))) +\n  geom_hline(yintercept=coef(nn.lm)[2]) + \n  annotate(\"text\", x=1300, y=0.785, label=\"Linear Model Slope Coefficient\") +\n  scale_color_manual(name=\"Legend\", values=c('blue', 'red')) + \n  xlab(\"Iteration\") + ylab(\"Parameter Value\") +\n  theme_bw() +\n  transition_reveal(iter)\n```\n\n::: {.cell-output-display}\n![](nn_regression_files/figure-html/unnamed-chunk-10-1.gif)\n:::\n:::\n\n\nTo visualize the gradient descent methodology, calculate the loss for a range of $\\omega_0$ and $\\omega_1$ values and plot the loss function as a surface.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloss_fn = expand_grid(bs=seq(0.3,0.9,length.out=20), ws=seq(0.5,1,length.out=20))\n  \nloss_fn$Loss = 1:nrow(loss_fn) %>% \n  map(function(x) get_estimate(loss_fn[x,'bs'] %>% .$bs, loss_fn[x,'ws']%>% .$ws)) %>% \n  map_dbl(function(x) mse(x))\n\nggplot(wHistory) +\n  geom_raster(data=loss_fn, aes(x=bs, y=ws, fill=Loss)) +\n  geom_contour(data=loss_fn, aes(x=bs, y=ws, z=Loss), color='white', bins=28) +\n  geom_point(aes(x=b, y=w, group=seq_along(iter)), color='yellow', size=5) +\n  xlab(\"Intercept\") + ylab(\"Slope\") +\n  theme_bw() +\n  transition_time(iter) +\n  labs(title = paste(\"Gradient Descent Iteration:\", \"{round(frame_time, 0)}\")) +\n  shadow_wake(wake_length = 0.2)\n```\n\n::: {.cell-output-display}\n![](nn_regression_files/figure-html/unnamed-chunk-11-1.gif)\n:::\n:::\n\n\nWe can also see the regression line update as training progresses.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(wHistory) +\n  geom_smooth(data=nn_reg, aes(x=x, y=y), formula='y~x', method='lm', se=FALSE) +\n  geom_abline(aes(intercept=b, slope=w), color='red') +\n  geom_point(data=nn_reg, aes(x=x, y=y)) +\n  coord_fixed(xlim=c(0,5), ylim=c(0,5)) +\n  theme_bw() +\n  transition_time(iter) +\n  labs(title = paste(\"Gradient Descent Iteration:\", \"{round(frame_time, 0)}\")) \n```\n\n::: {.cell-output-display}\n![](nn_regression_files/figure-html/unnamed-chunk-12-1.gif)\n:::\n:::\n\n\nLet's compare the final parameter estimates from the neural network model to the linear model coefficients.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  Model = c(\"Linear\", \"Neural Network\"),\n  w_0 = c(coef(nn.lm)[1], tail(w0s, 1)),\n  w_1 = c(coef(nn.lm)[2], tail(w1s, 1))\n)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Model\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"w_0\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"w_1\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Linear\",\"2\":\"0.4127478\",\"3\":\"0.7640741\"},{\"1\":\"Neural Network\",\"2\":\"0.4135672\",\"3\":\"0.7638478\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nThere are a variety of *R* packages to simplify the process of fitting a neural network model. Since we're doing regression and not something for complicated like image classification, natural language processing, or reinforcement learning, a package like `nnet` provides everything we need.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnnModel = nnet::nnet(y ~ x, data = nn_reg,   # formula notation is the same as lm()\n                     linout = TRUE,          # specifies linear output (instead of logistic)\n                     decay = 0.001,          # weight decay\n                     maxit = 100,            # stop training after 100 iterations\n                     size = 0, skip = TRUE)  # no hidden layer (covered later)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  2\ninitial  value 31.444863 \nfinal  value 2.134476 \nconverged\n```\n:::\n\n```{.r .cell-code}\nsummary(nnModel)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\na 1-0-1 network with 2 weights\noptions were - skip-layer connections  linear output units  decay=0.001\n b->o i1->o \n 0.41  0.76 \n```\n:::\n:::\n\n\nFrom the model summary, we see that has two weights: one for $\\omega_0$ and one for $\\omega_1$. The initial and final values are model error terms. Converged means that training stopped before it reached `maxit`. The model takes the form 1-0-1, which means it has one input node (the bias, or intercept, node is automatically included) in the input layer, 0 nodes in the hidden layer (we'll cover hidden layers later), and one node in the output layer. The `b->o` term is our $\\omega_0$ (intercept) parameter value, and `i1->o` is our $\\omega_1$ (slope) parameter value. We can extract the coefficients the usual way.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(nnModel)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     b->o     i1->o \n0.4125070 0.7641276 \n```\n:::\n:::\n\n\n### Multiple Linear Regression\n\nThe neural network model can be easily expanded to accommodate additional predictors by adding a node to the input layer for each additional predictor and connecting it to the output node. Below is a comparison of coefficients obtained from a linear model and a neural network model for a data set with three predictors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make up data\nset.seed(42)\nmlr = tibble(\n  x1 = runif(10, 0, 5),\n  x2 = runif(10, 0, 5),\n  x3 = runif(10, 0, 5),\n  y = 1 + 0.5*x1 - 0.5*x2 + x3 + rnorm(10, sd=0.5)\n)\n\n# linear model coefficients\ncoef(lm(y ~ ., data=mlr))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)          x1          x2          x3 \n  2.0447549   0.5355719  -0.7312496   0.8035532 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# neural network coefficients\ncoef(nnet::nnet(y ~ ., data = mlr, linout = TRUE, decay = 0.001, maxit = 100,\n                     size = 0, skip = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  4\ninitial  value 195.927838 \nfinal  value 4.149504 \nconverged\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n      b->o      i1->o      i2->o      i3->o \n 2.0399505  0.5361400 -0.7307887  0.8040190 \n```\n:::\n:::\n\n\nIf you think this seems like overkill just to model a linear relationship between two variables, I'd agree with you. But consider this:\n\n-   What if the relationship between two variables isn't linear?\n\n-   What if there are dozens of predictor variables and dozens of response variables and the underlying relationships are highly complex?\n\nIn cases like these, neural networks models can be very beneficial. To make that leap, however, we need to give our neural network model more power by giving it the ability to model these complexities. We do that by adding one or more **hidden layers** to the model.\n\n### Hidden Layer\n\nNeural network models become **universal function approximators** with the addition of one or more hidden layers. Hidden layers fall between the input layer and the output layer. Adding more predictor variables and one hidden layer, we get the following network.\n\n![](images/hidden_nn.png){#hidden_net width=\"800px\" fig-align=\"left\"}\n\nWe've introduced a new variable ν, and a new function *u*. The ν variables are trainable weights just like the *w* variables. The *u* functions are activation functions as described earlier. Typically, all nodes in a hidden layer share a common type of activation function. A variety of activation functions have been developed, a few of which are shown below. For many applications, a rectified linear, activation function is a good choice for hidden layers.\n\n| Activation Function | Activation Function Formula     | Output Type      |\n|-----------------------|-------------------------------|------------------|\n| Threshold           | $f(u) = \\begin{Bmatrix} 1, u\\gt0 \\\\ 0, u\\le0 \\end{Bmatrix}$  | Binary                |\n| Linear              | $f(u) = u$                                                   | Numeric               |\n| Logistic            | $f(u) = \\frac{e^u}{1+e^u}$                                   | Numeric Between 0 & 1 |\n| Rectified Linear    | $f(u) = \\begin{Bmatrix} u, u\\gt0 \\\\ 0, u\\le0 \\end{Bmatrix}$  | Numeric Positive      |\n\nAs stated, adding a hidden layer turns the neural network model into a universal function approximator. For the case of regression, we can think of this as giving the neural network the ability to model non-linear functions without knowing what the nature of the nonlinear relationship is. Compare that to linear regression. If we had the relationship $y=x^2$, we would need to transform either the response or predictor variable in a linear regression model, and this requires knowing the order of the polynomial to get a good fit. With neural network regression, we don't need this knowledge. Instead, we allow the hidden layer to learn the nature of the relationship through the model training process. To demonstrate, we'll use the `exa` data set from the `faraway` package, which looks like this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexa = faraway::exa\n\nggplot(exa) +\n  geom_line(aes(x=x, y=m), color='red', size=1.5) +\n  geom_point(aes(x=x, y=y)) +\n  ggtitle(\"Simulated Data (Black) and True Function (Red)\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](nn_regression_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nTo fit a linear model with a 10-node hidden layer, we specify `size = 10`, and since 100 iterations might not be enough to converge, we'll increase `maxit` to 500. Otherwise, everything else is the same. With 2 nodes in the input layer (1 for the bias, 1 for x) and 11 nodes in the hidden layer (1 for the bias, and 10 for those we specified), the model will have 31 weights to train.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnnModel2 = nnet::nnet(y ~ x, data = exa, linout = TRUE, decay = 10^(-4), maxit = 500, size = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  31\ninitial  value 499.220789 \niter  10 value 78.378021\niter  20 value 48.290911\niter  30 value 40.726235\niter  40 value 27.840931\niter  50 value 26.694405\niter  60 value 25.771768\niter  70 value 25.636428\niter  80 value 25.582253\niter  90 value 25.475199\niter 100 value 25.237757\niter 110 value 24.502290\niter 120 value 24.125822\niter 130 value 23.715466\niter 140 value 23.644963\niter 150 value 23.592656\niter 160 value 23.541215\niter 170 value 23.460968\niter 180 value 23.394390\niter 190 value 23.326002\niter 200 value 23.299889\niter 210 value 23.294752\niter 220 value 23.253858\niter 230 value 23.176888\niter 240 value 23.124385\niter 250 value 23.104052\niter 260 value 23.084055\niter 270 value 23.080214\niter 280 value 23.078985\niter 290 value 23.076738\niter 300 value 23.074305\niter 310 value 23.073038\niter 320 value 23.072321\nfinal  value 23.071986 \nconverged\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npreds = predict(nnModel2, newdata=data.frame(x=exa$x))\n\nggplot() +\n  geom_line(data=exa, aes(x=x, y=m, color='True Function'), size=1.5) +\n  geom_line(aes(x=exa$x, y=preds, color='Model Fit'), size=1.5) +\n  geom_point(data=exa, aes(x=x, y=y)) +\n  scale_color_manual(name=\"Legend\", values=c('blue', 'red')) + \n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](nn_regression_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nThe plot above indicates the model over fit the data somewhat, although we only know this because we have the benefit of knowing the true function. We could reduce the amount of over fitting by choosing different hyperparameter values (decay, or the number of hidden layer nodes) or by changing the default training stopping criteria.\n\nThe trained model's weights are saved with the model, and all 31 are shown below. This highlights a significant drawback of neural network regression. Earlier when we used a neural network model to estimate the slope and intercept, the model weights had meaning: we could directly interpret the weights as slope and intercept. How do we interpret the weights below? I have no idea! The point is that neural network models can be trained to make highly accurate predictions...but at the cost of interpretability.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnnModel2$wts\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] -16.5006608  20.3693099 -29.4278236  34.4051715 -20.7471248  28.4091270\n [7] -29.1180265  40.7327613   2.3060868   1.9215748  -1.0624319   2.9527926\n[13]   2.8259165 -15.1652645   1.6957639   0.5958946   3.7441769 -21.0803998\n[19]   6.3092685  -8.3407380   1.3998851  19.2726000 -11.2056916 -23.2575100\n[25]   9.5932587   2.2295197   3.1400221   6.4367782   0.4575623  -4.5860998\n[31]  -6.2028427\n```\n:::\n:::\n\n\n## Neural Network Classification\n\nNeural network models have been extremely successful when applied to classification tasks such as image classification and natural language processing. These models are highly complex and are built using sophisticated packages such as TensorFlow (developed by Google) and PyTorch (developed by Facebook). Building complex models for those kinds of classification tasks are beyond the scope of this tutorial. Instead, this section provides a high-level overview of classification using the `nnet` package and the `iris` data set.\n\nThe neural network training algorithm for classification is the same as for regression, but for classification, we need to change some of the attributes of the model itself. Instead of a linear activation function in the output layer, we need to use the softmax function. Doing so will cause the output layer to produce probabilities for each of the three flower species (this is accomplished by simply removing `linout = TRUE` from the `nnet()` function. Additionally, we use a categorical cross entropy loss function instead of mean squared error. Below, I also set `rang = 0.1` to scale the predictors to be in the range recommended in the function help. We'll also create the same training/test split as in the non-parametric regression chapter so we can directly compare results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a training and a test set\nset.seed(0)\ntrain = caTools::sample.split(iris, SplitRatio = 0.8)\niris_train = subset(iris, train == TRUE)\niris_test = subset(iris, train == FALSE)\n\n# train the model\nirisModel = nnet::nnet(Species ~ ., data = iris_train, \n                       size=2,       # only two nodes in the hidden layer\n                       maxit=200,    # stopping criteria\n                       entropy=TRUE, # switch for entropy\n                       decay=5e-4,   # weight decay hyperparameter\n                       rang=0.1)     # scale input values\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  19\ninitial  value 131.916499 \niter  10 value 75.933787\niter  20 value 56.873818\niter  30 value 55.958531\niter  40 value 55.852468\niter  50 value 55.797951\niter  60 value 51.382669\niter  70 value 11.286807\niter  80 value 7.667108\niter  90 value 7.657444\niter 100 value 7.643547\niter 110 value 7.642436\niter 120 value 7.641672\niter 130 value 7.640995\niter 140 value 7.640338\niter 150 value 7.627436\niter 160 value 7.377487\niter 170 value 5.671710\niter 180 value 4.882362\niter 190 value 4.810287\niter 200 value 4.793128\nfinal  value 4.793128 \nstopped after 200 iterations\n```\n:::\n\n```{.r .cell-code}\n# make predictions on test data\niris_preds = predict(irisModel, newdata=iris_test)\nhead(iris_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      setosa  versicolor    virginica\n5  0.9956710 0.004329043 6.376970e-16\n10 0.9911190 0.008881042 2.881866e-15\n15 0.9976175 0.002382547 1.824318e-16\n20 0.9957198 0.004280170 6.226948e-16\n25 0.9739313 0.026068739 2.794827e-14\n30 0.9901521 0.009847885 3.581197e-15\n```\n:::\n:::\n\n\nThis first six predictions for the test set are shown above, and notice that the values are in fact probabilities. The model is highly confident that each one of these first six predictions are setosa. Recall from the SVM and CART sections of the non-parametric regression chapter that both of those models misclassified test set observations #24 and #27 as versicolor that are actually virginica. Below we see that the neural network model correctly predicts both observations but is less confident about observation #24.\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_preds[c(24,27), ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          setosa versicolor virginica\n120 7.349090e-11 0.18967040 0.8103296\n135 5.460005e-13 0.03105806 0.9689419\n```\n:::\n:::\n\n\nThe confusion matrix for the entire test set reveals that the model has an accuracy of 100%.\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_cm = cvms::confusion_matrix(\n  targets = iris_test[, 5], \n  predictions = colnames(iris_preds)[max.col(iris_preds)])\n\ncvms::plot_confusion_matrix(iris_cm$`Confusion Matrix`[[1]], add_zero_shading = FALSE) + \n  ggtitle(\"Neural Network Confusion Matrix\")\n```\n\n::: {.cell-output-display}\n![](nn_regression_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "nn_regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}