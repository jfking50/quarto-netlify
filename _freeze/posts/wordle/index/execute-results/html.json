{
  "hash": "b250693ca83376fbb24ea8ceb78ae2e6",
  "result": {
    "markdown": "---\ntitle: \"Wordle\"\ndescription: \"A method to find the best Wordle starting word.\"\nauthor: \"John King\"\ndate: \"3/29/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: false\n    code-tools: true\n    code-copy: true\n    df-print: paged\nexecute: \n  warning: false\n  message: false\n  echo: true\ncategories:\n  - R\nimage: \"teaser.png\"\n---\n\n\nI was scrolling through the RStudio blog page, and [a post by Arthur Holtz](https://rviews.rstudio.com/2022/02/21/wordle-data-analysis/#disqus_thread) caught my attention. He wrote some code to identify what the optimal starting word is for [Wordle](https://www.nytimes.com/games/wordle/index.html). After some initial success with parsing the JavaScript code that contains the possible solution words, he ran into some difficulty when working out the logic for implementing a scoring system to rank the words. I've never played Wordle, but I thought what he was trying to do sounded interesting, so I decided to give it a go. The first few code chunks are either identical to or only slightly modified Arthur's code to retrieve and parse the JavaScript file, so all credit goes to him.\n\n::: callout-note\nRetrieving the JavaScript file at the link below no longer works, but I'm including it for reference. Fortunately, I saved the file so here I simply read it back in.\n:::\n\n## Gather the Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(httr)\n\n# this doesn't work any more!!\n#\n# wordle <- \n#   GET(\"https://www.nytimes.com/games/wordle/main.18637ca1.js\") %>%\n#   content(as = \"text\", encoding = \"UTF-8\")\n\nwordle <- readRDS(\"wordle.Rdata\")\n```\n:::\n\n\nThis retrieved the JavaScript for the game is a really long string. Here are the first 100 characters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsubstr(wordle, 1, 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"c(\\\"cigar\\\", \\\"rebut\\\", \\\"sissy\\\", \\\"humph\\\", \\\"awake\\\", \\\"blush\\\", \\\"focal\\\", \\\"evade\\\", \\\"naval\\\", \\\"serve\\\", \\\"heath\\\",\"\n```\n:::\n:::\n\n\nArthur discovered that the solutions are buried in the JavaScript in a list named `Ma`. The first word in the list is \"cigar\" and the last word is \"shave\", so we can get the entire list of words by grabbing those words and everything in between.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# the first word of the possible solutions follows Ma=\n# the character \"c\" is at the start index, r at the end index\nstr_locate(wordle, \"cigar\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     start end\n[1,]     4   8\n```\n:::\n:::\n\n\n`str_locate()` if from the `stringr` package, and it returns the indices of the first and last letters in \"cigar\". Note that it returns a matrix.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# it's a matrix, not a tibble\nclass(str_locate(wordle, \"cigar\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"matrix\" \"array\" \n```\n:::\n:::\n\n\nSimilarly, we get the indices for the last word.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# the last word\nstr_locate(wordle, \"shave\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     start   end\n[1,] 20817 20821\n```\n:::\n:::\n\n\nThis is where I start to deviate from Arthur's code. I end up with the same data he did, just in a slightly different format. This grabs all of the text, parses the words, and puts them in a tibble with one column called `words`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# put the words in a tibble column\nma <- \n  tibble(\n    words = substr(\n      wordle,\n      str_locate(wordle, \"cigar\")[, 1],\n      str_locate(wordle, \"shave\")[, 2]) %>%\n      str_remove_all(\"\\\"\") %>%\n      str_remove_all(\"\\\\s+\") %>%\n      str_split(\",\") %>%\n      unlist()\n  )\nhead(ma)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"words\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"cigar\"},{\"1\":\"rebut\"},{\"1\":\"sissy\"},{\"1\":\"humph\"},{\"1\":\"awake\"},{\"1\":\"blush\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nNext, like in Arthur's post, I create a tibble where each word is further parse into letters with one column per letter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# parse words to letters\ndf <-\n  str_split_fixed(ma$words, \"\", n=5) %>% \n  as_tibble(.name_repair = \"minimal\") %>% \n  set_names(c(\"one\", \"two\", \"three\", \"four\", \"five\"))\n\nhead(df)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"one\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"two\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"three\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"four\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"five\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"c\",\"2\":\"i\",\"3\":\"g\",\"4\":\"a\",\"5\":\"r\"},{\"1\":\"r\",\"2\":\"e\",\"3\":\"b\",\"4\":\"u\",\"5\":\"t\"},{\"1\":\"s\",\"2\":\"i\",\"3\":\"s\",\"4\":\"s\",\"5\":\"y\"},{\"1\":\"h\",\"2\":\"u\",\"3\":\"m\",\"4\":\"p\",\"5\":\"h\"},{\"1\":\"a\",\"2\":\"w\",\"3\":\"a\",\"4\":\"k\",\"5\":\"e\"},{\"1\":\"b\",\"2\":\"l\",\"3\":\"u\",\"4\":\"s\",\"5\":\"h\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n## Visualize the Data\n\nThese plots are also in Arthur's post, and I'm including them here because they give some insight into the characteristics of the solution words. First, a plot of the frequencies for all letters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot freq of all letters\ndf %>% pivot_longer(everything()) %>%\n  ggplot() +\n  geom_bar(aes(x = forcats::fct_infreq(value))) +\n  theme_bw() +\n  labs(title = \"All Letter Frequency\",\n       x = \"Letter\",\n       y = \"Count\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nThen plots of letters by position. First the starting letter. Note that \"s\" is by far the most common, so we might expect the best starting word to also start with \"s\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(df) +\n  geom_bar(aes(x = forcats::fct_infreq(one))) +\n  theme_bw() +\n  labs(title = \"First Letter Frequency\",\n       x = \"Letter\",\n       y = \"Count\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nThe second letter is heavier on vowels, which makes sense.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(df) +\n  geom_bar(aes(x = forcats::fct_infreq(two))) +\n  theme_bw() +\n  labs(title = \"Second Letter Frequency\",\n       x = \"Letter\",\n       y = \"Count\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThe third letter is even more vowel heavy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(df) +\n  geom_bar(aes(x = forcats::fct_infreq(three))) +\n  theme_bw() +\n  labs(title = \"Third Letter Frequency\",\n       x = \"Letter\",\n       y = \"Count\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nA lot of \"e\"'s for the fourth letter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(df) +\n  geom_bar(aes(x = forcats::fct_infreq(four))) +\n  theme_bw() +\n  labs(title = \"Fourth Letter Frequency\",\n       x = \"Letter\",\n       y = \"Count\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nAnd \"e\" and \"y\" for the last letter. Note that there aren't many \"s\"'s, as Arthur pointed out. Apparently, they got rid of the plural form of four letter words. I also noticed they got rid of four letter words that you could add \"r\" or \"d\" to make a five letter word. For example \"rate\" to \"rater\" or \"rated\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(df) +\n  geom_bar(aes(x = forcats::fct_infreq(five))) +\n  theme_bw() +\n  labs(title = \"Fifth Letter Frequency\",\n       x = \"Letter\",\n       y = \"Count\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nma %>% filter(words == \"rater\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"words\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nma %>% filter(words == \"rated\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"words\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n## Scoring Words\n\nAs I mentioned earlier, implementing a scoring system is where Arthur got stuck. My first attempt was a brute force approach where, one at a time, I compare one word in the list to all others. Matching letters get three points, and correct letters in the wrong position get one point. Since there are 2309 words in the list, that means there are 2309 \\* 2309 = 5,331,481 comparisons. Even with some parallelization using the `furrr` package, this script took more than 20 minutes on my laptop. For that reason, I'm not going to execute the code in this post, but it's below for reference. Using this code, the optimal starting word is \"slate\".\n\n    library(progress)\n    library(furrr)\n    plan(multisession, workers = 6)\n\n    pb <- progress_bar$new(total = 2309)\n\n    mean_score <- rep(0, nrow(df))\n    for (j in 1:nrow(df)){\n      scores <- rep(0, nrow(df))\n      base <- as.vector(t(df[j, ]))\n      mean_score[j] <- \n        1:nrow(df) %>%\n        future_map(function (i){\n          tgt <- df[i, ]\n          score <- sum((base == tgt) * 3)\n          score + sum(tgt[base != tgt] %in% base[base != tgt])}\n        ) %>% unlist() %>% mean()\n      pb$tick()\n    }\n\n    ma %>% slice(which.max(mean_score))\n\nAlthough brute force gives an answer, I could speed things up by vectorizing to work with all words simultaneously. I think this approach works for all cases except when there are two letters in the initial guess word (what I call `tgt` below) and the same two letters in the answer word that aren't in the same location. For example, given a `tgt` of \"erase\" and answer word of \"steed\", the \"e\"'s get a score of 1, not 2 like they should. I believe this is ok for a start word because my assumption is that a good start word should have all unique letters so that you get feedback on all five letters.\n\nI start with the first word in the list, \"cigar\", and parse it into a vector of five letters called `tgt`. I then extract the unique letters from that vector, pad it with \"\\#\" so that it has a length of 5, and call it `utgt` for later use.\n\nThen I create a tibble `df` to implement the scoring. The first five mutate functions do some bookkeeping that I found helpful to visualize what was going on as I iterated on this solution. I also use the two columns they create as the basis for calculating the `tgt` word score for each of the other words in the list. There's one mutate for each letter. In each case, if the letters match, I put the letter in the `match` column. If they don't match, I check to see if it's a correct letter in the wrong position, and if so, put it in the `notmatch` column.\n\nThe last mutate implements the scoring. The lines with `c1` through `c5` score the `notmatch` column. These are the cases when the correct letter is in the wrong position, and they get a score of 1, all other cases get a 0. The line with `score` uses the `match` column to give a score of 3 for the cases where the correct letter is in the correct location. The scores of 3's and 1's then get added together to give a score for each word. Before I iterate over all of the `tgt` words, I'll show what the `df` tibble looks like for \"cigar\", the first `tgt` word.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntgt <-  as.vector(t(df[1, ])) # get the target word\nutgt <- unique(tgt)           # get the unique letters\n\nif (length(unique(tgt)) < 5){\n  utgt <- c(utgt, rep(\"#\", length(tgt) - length(utgt)))} # add padding\n\ndf %>%\n  mutate(\n    match = ifelse(tgt[1] == one, one, \"_\"),\n    notmatch = ifelse(tgt[1] != one & one %in% tgt, one, \"_\")) %>%\n  mutate(\n    match = paste0(match, ifelse(tgt[2] == two, two, \"_\")),\n    notmatch = paste0(notmatch, ifelse(tgt[2] != two & two %in% tgt, two, \"_\"))) %>%\n  mutate(\n    match = paste0(match, ifelse(tgt[3] == three, three, \"_\")),\n    notmatch = paste0(notmatch, ifelse(tgt[3] != three & three %in% tgt, three, \"_\"))) %>% \n  mutate(\n    match = paste0(match, ifelse(tgt[4] == four, four, \"_\")),\n    notmatch = paste0(notmatch, ifelse(tgt[4] != four & four %in% tgt, four, \"_\"))) %>% \n  mutate(\n    match = paste0(match, ifelse(tgt[5] == five, five, \"_\")),\n    notmatch = paste0(notmatch, ifelse(tgt[5] != five & five %in% tgt, five, \"_\"))) %>%\n  mutate(c1 = ifelse(!is.na(str_match(notmatch, utgt[1])), 1, 0),\n         c2 = ifelse(!is.na(str_match(notmatch, utgt[2])), 1, 0),\n         c3 = ifelse(!is.na(str_match(notmatch, utgt[3])), 1, 0),\n         c4 = ifelse(!is.na(str_match(notmatch, utgt[4])), 1, 0),\n         c5 = ifelse(!is.na(str_match(notmatch, utgt[5])), 1, 0), \n         score = str_count(match, \"[:alpha:]\") * 3 + c1 + c2 + c3 + c4 + c5) %>%\n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"one\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"two\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"three\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"four\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"five\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"match\"],\"name\":[6],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"notmatch\"],\"name\":[7],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"c1\"],\"name\":[8],\"type\":[\"dbl[,1]\"],\"align\":[\"right\"]},{\"label\":[\"c2\"],\"name\":[9],\"type\":[\"dbl[,1]\"],\"align\":[\"right\"]},{\"label\":[\"c3\"],\"name\":[10],\"type\":[\"dbl[,1]\"],\"align\":[\"right\"]},{\"label\":[\"c4\"],\"name\":[11],\"type\":[\"dbl[,1]\"],\"align\":[\"right\"]},{\"label\":[\"c5\"],\"name\":[12],\"type\":[\"dbl[,1]\"],\"align\":[\"right\"]},{\"label\":[\"score\"],\"name\":[13],\"type\":[\"dbl[,1]\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"c\",\"2\":\"i\",\"3\":\"g\",\"4\":\"a\",\"5\":\"r\",\"6\":\"cigar\",\"7\":\"_____\",\"8\":\"0\",\"9\":\"0\",\"10\":\"0\",\"11\":\"0\",\"12\":\"0\",\"13\":\"15\"},{\"1\":\"r\",\"2\":\"e\",\"3\":\"b\",\"4\":\"u\",\"5\":\"t\",\"6\":\"_____\",\"7\":\"r____\",\"8\":\"0\",\"9\":\"0\",\"10\":\"0\",\"11\":\"0\",\"12\":\"1\",\"13\":\"1\"},{\"1\":\"s\",\"2\":\"i\",\"3\":\"s\",\"4\":\"s\",\"5\":\"y\",\"6\":\"_i___\",\"7\":\"_____\",\"8\":\"0\",\"9\":\"0\",\"10\":\"0\",\"11\":\"0\",\"12\":\"0\",\"13\":\"3\"},{\"1\":\"h\",\"2\":\"u\",\"3\":\"m\",\"4\":\"p\",\"5\":\"h\",\"6\":\"_____\",\"7\":\"_____\",\"8\":\"0\",\"9\":\"0\",\"10\":\"0\",\"11\":\"0\",\"12\":\"0\",\"13\":\"0\"},{\"1\":\"a\",\"2\":\"w\",\"3\":\"a\",\"4\":\"k\",\"5\":\"e\",\"6\":\"_____\",\"7\":\"a_a__\",\"8\":\"0\",\"9\":\"0\",\"10\":\"0\",\"11\":\"1\",\"12\":\"0\",\"13\":\"1\"},{\"1\":\"b\",\"2\":\"l\",\"3\":\"u\",\"4\":\"s\",\"5\":\"h\",\"6\":\"_____\",\"7\":\"_____\",\"8\":\"0\",\"9\":\"0\",\"10\":\"0\",\"11\":\"0\",\"12\":\"0\",\"13\":\"0\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nI'll use `future_map()` from the `furrr` package to iterate over all 2,309 starting words in parallel. Note that after creating the `df` tibble shown above, I calculate the mean of the `score` column and record that in the `scores` list. Once that completes, I display the top ten scoring starting words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(furrr) # since I didn't run the code above, load package\n\nplan(multisession, workers = 4) \n\nscores <-\n  1:nrow(df) %>%\n  future_map(function(x){\n    tgt <-  as.vector(t(df[x, ])) # get the target word\n    utgt <- unique(tgt)           # get the unique letters\n    \n    if (length(unique(tgt)) < 5){\n      utgt <- c(utgt, rep(\"#\", length(tgt) - length(utgt)))} # add padding\n    \n    df %>%\n      mutate(\n        match = ifelse(tgt[1] == one, one, \"_\"),\n        notmatch = ifelse(tgt[1] != one & one %in% tgt, one, \"_\")) %>%\n      mutate(\n        match = paste0(match, \n                       ifelse(tgt[2] == two, two, \"_\")),\n        notmatch = paste0(notmatch, \n                          ifelse(tgt[2] != two & two %in% tgt, two, \"_\"))) %>%\n      mutate(\n        match = paste0(match, \n                       ifelse(tgt[3] == three, three, \"_\")),\n        notmatch = paste0(notmatch, \n                          ifelse(tgt[3] != three & three %in% tgt, three, \"_\"))) %>% \n      mutate(\n        match = paste0(match, \n                       ifelse(tgt[4] == four, four, \"_\")),\n        notmatch = paste0(notmatch, \n                          ifelse(tgt[4] != four & four %in% tgt, four, \"_\"))) %>% \n      mutate(\n        match = paste0(match, \n                       ifelse(tgt[5] == five, five, \"_\")),\n        notmatch = paste0(notmatch, \n                          ifelse(tgt[5] != five & five %in% tgt, five, \"_\"))) %>%\n      mutate(c1 = ifelse(!is.na(str_match(notmatch, utgt[1])), 1, 0),\n             c2 = ifelse(!is.na(str_match(notmatch, utgt[2])), 1, 0),\n             c3 = ifelse(!is.na(str_match(notmatch, utgt[3])), 1, 0),\n             c4 = ifelse(!is.na(str_match(notmatch, utgt[4])), 1, 0),\n             c5 = ifelse(!is.na(str_match(notmatch, utgt[5])), 1, 0), \n             score = str_count(match, \"[:alpha:]\") * 3 + c1 + c2 + c3 + c4 + c5) %>%\n      summarise(mean_score = mean(score)) %>% unlist()\n    }\n  ) %>% unlist()\n\nma %>% \n  mutate(score = scores) %>% \n  arrange(desc(scores)) %>% \n  head(10)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"words\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"score\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"slate\",\"2\":\"2.988740\"},{\"1\":\"stare\",\"2\":\"2.970550\"},{\"1\":\"saner\",\"2\":\"2.954093\"},{\"1\":\"arose\",\"2\":\"2.941533\"},{\"1\":\"raise\",\"2\":\"2.933738\"},{\"1\":\"arise\",\"2\":\"2.927674\"},{\"1\":\"snare\",\"2\":\"2.927241\"},{\"1\":\"crate\",\"2\":\"2.919013\"},{\"1\":\"stale\",\"2\":\"2.915115\"},{\"1\":\"share\",\"2\":\"2.899957\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nHere we see \"slate\" is on top, followed by two other words that start with \"s\". All but one of the words end in \"e\". I didn't include it here, but I tried a different scoring system with correct matches given a 5 instead of a 3, and I got the same top 3 words. We could also try giving more points to correct letters in the wrong position and other variations.\n\nLastly, I was curious about the distribution of scores and so plotted a histogram. I didn't have an expectation of a shape, but I think it's interesting that it's normal looking.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(scores, breaks=30)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}