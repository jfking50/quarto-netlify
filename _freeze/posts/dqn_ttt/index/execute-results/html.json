{
  "hash": "b74fd767a2708921d86a3b8e420dc091",
  "result": {
    "markdown": "---\ntitle: \"Deep Q-Networks\"\ndescription: \"Train an AI agent to play tic-tac-toe.\"\nauthor: \"John King\"\ndate: \"4/10/2020\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-tools: true\n    code-copy: true\n    df-print: paged\nexecute: \n  warning: false\ncategories:\n  - python\n  - reinforcement learning\n  - AI\n  - tensorflow\n  - neural network\nimage: \"ttt.png\"\n---\n\nIn a [previous post](https://jfking.netlify.app/posts/qlearn) on Q-learning, I demonstrated how to train an agent to play tic-tac-toe. One of the challenges with Q-learning is that it doesn't scale well to environments with a large state space. Even after experiencing 200,000 tic-tac-toe games, the agent had visited only a fraction of the total state space. After training and when playing against an opponent, if the agent encountered a board that it didn't experience in training, the Q-values for all available moves were 0, and so the agent played randomly. This becomes even more problematic if we try to apply Q-learning to a game like chess or Go. We need a method that doesn't rely on a training agent visiting every state. Neural networks have been very successful in solving this problem. [DeepMind](https://deepmind.com/about#our_story), for example, has not only mastered 49 different Atari games, but has also defeated the raining Go champion. I'm not that ambitious. I'll just stick with tic-tac-toe. The most basic application of neural networks to this problem is a **deep Q-Network**, which I'll demonstrate here.\n\n## Tic-Tac-Toe Environment\n\nAs with Q-learning, I needed to define an environment that contains the game fundamentals. I started with the same environment I set up for Q-learning and then stripped out the Q-learning algorithm so I just have the basics of the game. The `env` class below does just three things: 1. It resets the game board to begin a new game. I define the board state as a list with nine elements to represent each of the nine game board positions. A 0 represents an empty position, 1 is an X, and -1 is an O. 2. In the `step` function, it makes a move for either player X or O. 3. In the `game_over` function, it checks the state of the board and returns a reward. The reward system is the same as before. An X win gets a reward of 1, an O win gets a reward of -1, and everything else gets a reward of 0.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport tensorflow as tf\nimport numpy as np\nimport copy\nimport random\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nclass env:\n\n    def __init__(self):\n        self.state = self.reset()\n\n    def reset(self):\n        return [0 for i in range(9)]\n\n    def game_over(self, s):\n        done = False\n        reward = 0\n        if (s[0] + s[1] + s[2]  == 3 or s[3] + s[4] + s[5]  == 3 or s[6] + s[7] + s[8]  == 3 or\n            s[0] + s[3] + s[6]  == 3 or s[1] + s[4] + s[7]  == 3 or s[2] + s[5] + s[8]  == 3 or\n            s[0] + s[4] + s[8]  == 3 or s[2] + s[4] + s[6]  == 3):\n            done = True\n            reward = 1\n        if (s[0] + s[1] + s[2]  == -3 or s[3] + s[4] + s[5]  == -3 or s[6] + s[7] + s[8]  == -3 or\n            s[0] + s[3] + s[6]  == -3 or s[1] + s[4] + s[7]  == -3 or s[2] + s[5] + s[8]  == -3 or\n            s[0] + s[4] + s[8]  == -3 or s[2] + s[4] + s[6]  == -3):\n            done = True\n            reward = -1\n        if sum(1 for i in s if i != 0)==9 and not done:\n            done = True\n        return done, reward\n\n    def step(self, state, action, player):\n        next_state = state.copy()\n        if player == 0: next_state[action] = 1\n        else: next_state[action] = -1\n        done, reward = self.game_over(next_state)\n        return next_state, done, reward\n```\n:::\n\n\n## Deep Q-Learning\n\nNext, I create a class `DQNagent` where the real work is done. I'll go through the three main functions in detail.\n\n**`build_model` Function.** This is where I create the neural network itself. It's a simple sequential model with two hidden layers that I built some flexibility into. The input shape, the number of nodes, and the output size are all variables because I intend to apply this basic set-up to other problems. In the tic-tac-toe problem, the neural network takes the board state as input, so in this case the `input_shape` is nine. The two hidden layers have 81 nodes each with the `relu` activation function. The output also has a length of nine that are the Q-values for each of the nine actions (i.e., moves on the game board). In other words, the network takes the game board as input, and provides the best move as output.\n\n**`play_ttt` Function.** It might make more sense to explain this function next. Basically, there's an inner loop to play one game of tic-tac-toe, and an outer loop to play the game multiple times. Most of the function just controls the mechanics of the game, but there are two aspects I want to point out. 1. The function uses an *epsilon-greedy policy* to determine whether to make a move based on the neural network Q-values or make a random move. The probability of playing randomly is initially high during training to encourage the algorithm to explore the state space. As training progresses, player X moves are more and more likely to be based on the neural network. Note that the neural network is set up to learn the game from player X's perspective. Player O always plays randomly. 2. During the course of playing one game, the game board states, player moves (actions), and rewards are recorded. These are passed to the `train_model` function at the end of a game.\n\n**`train_model` Function.** This is where the neural network gets trained and things get complicated. I'm going to let Aurélien Géron do the 'splainin from his book [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646).\n\n> Consider the approximate Q-Value computed by the DQN for a given state-action pair (s, a). Thanks to Bellman, we know we want this approximate Q-Value to be as close as possible to the reward r that we actually observe after playing action a in state s, plus the discounted value of playing optimally from then on. To estimate this sum of future discounted rewards, we can simply execute the DQN on the next state s′ and for all possible actions a′. We get an approximate future Q-Value for each possible action. We then pick the highest (since we assume we will be playing optimally) and discount it, and this gives us an estimate of the sum of future discounted rewards. By summing the reward r and the future discounted value estimate, we get a target Q-Value y(s, a) for the state-action pair (s, a), as shown in the equation:\n\n$$\nQ_{target}(s,a)=r\\gamma\\cdot \\underset{a'}{max}Q_{\\theta}\\left(s', a'\\right)\n$$\n\n> With this target Q-Value, we can run a training step using any Gradient Descent algorithm. Specifically, we generally try to minimize the squared error between the estimated Q-Value Q(s, a) and the target Q-Value (or the Huber loss to reduce the algorithm's sensitivity to large errors). And that's all for the basic Deep Q-Learning algorithm!\n\nRight. So that's what happening in the `train_model` function.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nclass DQNagent:\n\n    def __init__(self, state_size, action_size, iterations):\n        self.alpha0 = 0.05           # learning rate\n        self.decay = 0.005           # learning rate decay\n        self.gamma = 0.95            # discount factor\n        self.state_size = state_size\n        self.action_size = action_size\n        self.iterations = iterations\n        self.model = self.build_model()\n        self.optimizer = tf.keras.optimizers.SGD(learning_rate=0.02)\n        self.loss_fn = tf.keras.losses.mean_squared_error\n\n    def build_model(self):\n        model = tf.keras.models.Sequential([\n            tf.keras.layers.Dense(self.state_size**2, activation=\"relu\", input_shape=[self.state_size]),\n            tf.keras.layers.Dense(self.state_size**2, activation=\"relu\"),\n            tf.keras.layers.Dense(self.action_size)\n        ])\n        return model\n\n    def train_model(self, state_history, action_history, next_state_history, rewards, dones):\n        next_Q_values = self.model.predict(np.array(next_state_history), verbose=0)\n        max_next_Q_values = np.max(next_Q_values, axis=1)\n        target_Q_values = rewards + (1 - 1*np.array(dones)) * self.gamma * max_next_Q_values\n        target_Q_values = tf.reshape(target_Q_values, [len(rewards), 1])\n        mask = tf.one_hot(action_history, 9)\n        with tf.GradientTape() as tape:\n            all_Q_values = self.model(np.array(state_history))\n            Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n            loss = tf.reduce_mean(self.loss_fn(target_Q_values, Q_values))\n        grads = tape.gradient(loss, self.model.trainable_variables)\n        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n\n    def play_ttt(self):\n        for iteration in range(self.iterations):    # outer loop to play the game a bunch of times\n            state = env().reset()\n            next_state = state.copy()\n            done = False\n            dones = []\n            state_history = []\n            state_history.append(state)\n            action_history = []\n            rewards = []\n            epsilon = max(1 - iteration/(self.iterations*0.8), 0.01)\n            while not done:                          # inner loop to play one game\n                if random.random() < epsilon:        # epsilon-greedy policy\n                    action = random.choice([i for i in range(len(state)) if state[i] == 0])\n                else:\n                    action = np.argmax(self.model.predict(np.array(state)[np.newaxis], verbose=0)[0])\n                action_history.append(action)\n                next_state, done, reward = env().step(state, action, 0)\n                if done:\n                    state_history.append(next_state)\n                    dones.append(done)\n                    rewards.append(reward)\n                if not done:\n                    omove = random.choice([i for i in range(len(next_state)) if next_state[i] == 0])\n                    next_state, done, reward = env().step(next_state, omove, 1)\n                    state = next_state.copy()\n                    state_history.append(next_state)\n                    dones.append(done)\n                    rewards.append(reward)\n            next_state_history = state_history[1:len(state_history)]\n            state_history = state_history[0:len(action_history)]\n            self.train_model(state_history, action_history, next_state_history, rewards, dones)\n        return self.model\n```\n:::\n\n\n## Training\n\nHere I just provide the `DQNagent` with the number of input and output nodes for the neural network (9 each) and how many games to play (1000). Compare the number of games here with the 200,000 games I used for the Q-learning method - a huge difference!\n\n\n\n## Results\n\nI want to see real quick if the neural net has learned to play in the center of the board on the opening move, so I have the model give me the Q-values for an empty board. The fifth number returned represents the center of the board, and it is the highest Q-value, so the neural net has already learned it!\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nm.predict(np.zeros(9)[np.newaxis], verbose=0)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\narray([[0.89818525, 0.55678254, 0.7211838 , 0.5331467 , 0.55849266,\n        0.41149646, 0.41752434, 0.468462  , 0.5010151 ]], dtype=float32)\n```\n:::\n:::\n\n\nNext, the `play_v_random` function pits the AI-enabled player against an opponent that plays randomly.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndef play_v_random (games):\n    results = [0 for i in range(games)]\n    for i in range(games):\n        board = env().reset()\n        done = False\n        while not done:\n            #print(board[0:3])\n            #print(board[3:6])\n            #print(board[6:9])\n            xmoves = m.predict(np.array(board)[np.newaxis], verbose=0)[0]\n            xmoves[np.where(np.array(board)!=0)[0]] = -1\n            xmove = np.argmax(xmoves)\n            #print(\"move\", xmove)\n            board[xmove] = 1\n            done, reward = env().game_over(board)\n            if not done:\n                omove = random.choice([i for i in range(len(board)) if board[i] == 0])\n                board[omove] = -1\n                done, reward = env().game_over(board)\n        #print(board[0:3])\n        #print(board[3:6])\n        #print(board[6:9])\n        results[i] = reward\n    return results\n  \nresults = play_v_random(1000)\n\nsum(1 for i in results if i == 1)/10, sum(1 for i in results if i == -1)/10, sum(1 for i in results if i == 0)/10\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n(89.4, 3.8, 6.8)\n```\n:::\n:::\n\n\nSo here we have it. The AI-enabled player performed significantly better than the best performance using Q-learning (81.2% AI wins, 2.3% losses, and 16.5% ties). The deep Q-network also performed significantly better after being trained on 1,000 games than Q-learning did after being trained on 200,000 games. Remarkable!\n\nAs I mentioned at the beginning of the post, a deep Q-network is the most basic application of neural networks to reinforcement learning. More advanced applications include double deep Q-networks and dueling deep Q networks, and each of these can be enhanced with techniques like actor-critic algorithms, curiosity-based exploration, proximal policy optimization, and so on. There's a lot left to explore.\n\n",
    "supporting": [
      "index_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}