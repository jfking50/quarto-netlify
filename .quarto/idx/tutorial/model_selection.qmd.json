{"title":"Model Selection","markdown":{"yaml":{"title":"Model Selection","author":"John King","date":"5/24/2020","format":{"html":{"toc":true,"code-copy":true,"df-print":"paged"}},"execute":{"warning":false,"echo":true},"bibliography":"references.bib","link-citations":true},"headingText":"Testing-Based Methods","containsRefs":false,"markdown":"\n\nThis post presents methods for finding a balance between under fitting and over fitting a model. Under fitting is when the model is a poor predictor of the response. With linear regression, this is largely addressed through diagnostic checks, which was covered in the tutorial on linear model assumptions. A linear model is over fitted when it includes more predictors than are needed to represent the relationship to the response variable. Appropriately reducing the complexity of the model improves its ability to make predictions based on new data, and it helps with interpretability.\n\nThere are three general approaches to reducing model complexity:\n\n-   dimension reduction\n\n-   variable selection\n\n-   regularization\n\nDimension reduction is beyond the scope of this post and will not be covered. This tutorial presents two methods of variable selection (testing- and criterion-based methods) and regularization through lasso regression.\n\n\nTesting-based methods are the easiest to implement but should only be considered when there are only a few predictors. The idea is simple. In **forward elimination**, we start with a linear model with no predictors, manually add them one at a time, and keep only those predictors with a low p-value. **Backward elimination** is just the opposite: we start with a linear model that contains all predictors (including interactions, if suspected), remove the predictor with the highest p-value, build a new linear model with the reduced set or predictors, and continue that process until only those predictors with low p-values remain.\n\nWe'll use the `teengamb` dataset from the `faraway` package to demonstrate backward elimination. This dataset contains survey results from a study of teenage gambling in Britain. The response variable is `gamble`, which is the expenditure on gambling in pounds per year. The predictors are information regarding each survey respondent, such as gender and income.\n\n```{r}\n#| message: false\n#| warning: false\nlibrary(faraway)\ndata(teengamb)\nhead(teengamb)\n```\n\nA linear model with all predictors is as follows (we'll assume this model passes all of the required diagnostic checks):\n\n```{r}\ntg = lm(gamble~., data=teengamb)\nsummary(tg)\n```\n\nSince the p-value for `status` is the highest, we remove it first.\n\n```{r}\ntg = update(tg, . ~ . -status) # remove status\nsummary(tg)\n```\n\nThen we remove `verbal`.\n\n```{r}\ntg = update(tg, . ~ . -verbal) # remove verbal\nsummary(tg)\n```\n\nNotice that even though we eliminated half of the predictors from the model, we only slightly reduced the adjusted $R^{2}$. The simpler model explains almost as much variance in the response with only half the number of predictors. Something to keep in mind when conducting forward or backward elimination is that the predictor p-value does not necessarily have to be above 0.05 to eliminate the predictor from the model. You could also choose something higher - even up to around 0.15 to 0.20 if predictive performance is the goal. For example, note that the p-value for `verbal` in the second model was 0.14, and the adjusted $R^{2}$ for the model was the highest of the three. The coefficient for `verbal` was also negative, which is what we'd expect: teens with higher verbal scores spend less money on gambling. We should therefore consider keeping `verbal` in the model. As you can see, there's a little bit of an art to this method.\n\n## Criterion-Based Methods\n\nAs previously stated, testing-based procedures should only be considered when there are just a few factors to consider. The more potential factors in your model, the greater the chance that you'll miss the optimal combination. We saw in the previous section that we had two competing goals: model simplicity versus model fit. Akaike [@akaike1974] developed a method to measure this balance between simplicity and fit called the **Akaike Information Criterion (AIC)**, which takes the form of:\n\n$$AIC = 2(p+1) - 2ln(\\hat{L})$$\n\nwhere,\n\n-   $p$ is the number of predictors, and\n-   $\\hat{L}$ is the maximized likelihood for the predictive model.\n\nWe then choose the model with the lowest AIC.\n\nThe **Bayes Information Criterion (BIC)** is an alternative to AIC and replaces $2(p+1)$ with $ln(n)(p+1)$, where $n$ is the number of observations. Adding $ln(n)$ increases the penalty for the number of factors in the model more for larger data sets. Which criterion you use can therefore depend on the dataset you're working with.\n\nAnother common estimator of error is **Mallow's Cp**, which is defined as:\n\n$$C_{p}=\\frac{1}{n}(RSS+2p\\hat{\\sigma}^{2})$$\n\nwhere,\n\n-   $RSS$ is the root sum of squares,\n-   $p$ is the number of predictor, and\n-   $\\hat{\\sigma}^{2}$ is an estimate of the variance of the error, $\\varepsilon$, in the linear regression equation.\n\nAs with AIC and BIC, the penalty term (in this case $2p\\hat{\\sigma}^{2}$) increases as the number of predictors in the model increases, which is intended to balance the corresponding decrease in $RSS$. With each of these methods, as we vary $p$, we get an associated criterion value from which we select the minimum as the best model. In *R*, we can calculate AIC and BIC with the `bestglm()` function from the `bestglm` package. Be aware that `bestglm()` expects the data to be in a dataframe with the response variable in the last column.\n\n::: callout-important\n`bestglm()` is picky about how your dataset is structured. It expects a dataframe with the response variable in the last column and all other columns are predictors. Don't include any other \"extra\" columns. Fortunately, teengamb is already set up that way.\n:::\n\n```{r}\n#| message: false\n#| warning: false\nlibrary(bestglm)\n\ntg.AIC = bestglm(teengamb, IC=\"AIC\")\n\n# this will provide the best model\ntg.AIC\n```\n\nNotice that `verbal` is included in the best fit model even though its p-value is \\> 0.05. Using `summary()`, we get a likelihood-ratio test for the best model compared to the null model.\n\n```{r}\nsummary(tg.AIC)\n```\n\nTo get the best model in a `lm()` format:\n\n```{r}\ntg.AIC$BestModel\n```\n\nWe can also see a comparison of the best model (model 1) to the next 4 best models.\n\n```{r}\ntg.AIC$BestModels\n```\n\nWe can also see the best model (row 3 below) and its subsets. Row 0 contains just the y-intercept, and in each successive row one predictor is added tp the model.\n\n```{r}\ntg.AIC$Subsets\n```\n\nUsing BIC, however, `verbal` is excluded from the best fit model.\n\n```{r}\ntg.BIC = bestglm(teengamb, IC=\"BIC\")\ntg.BIC\n```\n\nFor Mallow's Cp, we can use the `leaps` package.\n\n```{r}\nlibrary(leaps)\n\n# leaps expects x and y to be passed separately\ntg.cp = leaps(x=teengamb[-5], y=teengamb$gamble, method=\"Cp\")\ntg.cp\n```\n\nIt takes a little finagling to get the predictors that we should include in the best model. We want the index of the minimum value in `$Cp`, and we use that to find the corresponding row in `$which` to determine the predictors that should remain in the model. Columns 1, 2, and 4 correspond to `sex`, `status`, and `verbal`, which is the same as the AIC result.\n\n```{r}\ntg.cp$which[which.min(tg.cp$Cp), ]\n```\n\n## Cross Validation\n\nAn alternative approach to using AIC, BIC, or Cp is to use cross validation (CV) to select the best model. The idea is that we randomly divide our data into a **training set** and a **test set**. An 80/20 split between the training set and test set is common but will depend on your sample size. For very large sample sizes (in the millions), the training set can contain a larger percentage, while for relatively small sample sizes, the split may be closer to 50/50.\n\nThe training set is further randomly divided into $k$ subsets (also called **folds**), and one of these folds is withheld as the **validation set**. We fit a model to the remaining training set, and then measure the prediction error using the validation set. Typically, the prediction error is measured by the mean squared error (MSE) for a quantitative response variable. We repeat this process by cycling though each of the folds and holding it out as the validation set. The cross validated error (CV error) is then the average prediction error for the $k$ folds.\n\nThe website for the `scikit-learn` module for Python has a good visualization (shown below) of these various data sets and a [good explanation](https://scikit-learn.org/stable/modules/cross_validation.html) of this and other cross validation methods. A more thorough, academic treatment of cross validation may be found in [Chapter 7.10](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) of Elements of Statistical Learning written by Trevor Hastie, Robert Tibshirani, and Jerome Friedman.\n\n![](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)\n\nOnce the CV process is complete, we re-combine each of the folds into a single training set for a final evaluation against the test set. With this approach, we can compare multiple CV methods and choose the method with the best performance.\n\nNotice that we are not using an Information Criterion (IC) anywhere in this method. Another difference is that with criterion-based methods, we chose the model with the lowest IC score, but with CV, we don't choose the model with the lowest CV error. Instead, we calculate the standard deviation ($\\sigma$) of the CV error for each of the $p$ predictors and then choose the smallest model that's CV error is within one standard error of the lowest. Standard error is defined as $se = \\sigma/\\sqrt(k)$. This is best shown graphically, which you'll see below.\n\nCV techniques are particularly useful for datasets with many predictors, but for consistency, we'll stick with the `teengamb` dataset. Below, we'll perform k-fold cross validation on the `teamgamb` dataset, once again using `bestglm()`. We'll use an 80/20 train/test split.\n\n```{r}\nset.seed(2)\ntest_set = sample(47, 10, replace=FALSE)  # randomly select row indices\ntg_test = teengamb[test_set, ]              # create test set\ntg_train = teengamb[-test_set, ]            # create training set \n```\n\nThe training set has only 24 observations, so if we further partition it into a large number of folds, we'll have a small number of observations in each of the validation folds. For this example, we'll choose just 3 folds. In the `bestglm()` function, we specify `CV` as the IC and pass three arguments to specify cross validation parameters. As mentioned, there are a variety of cross validation methods to choose from. For the method described above, we specify `Method=\"HTF\"`, which you might have noticed are the first letters of the last names of the authors mentioned in the \"Elements of Statistical Learning\" reference above. `K=3` specifies the number of k-folds, and we can chose one or more repetition with `REP`. Remember that cross validation randomly partitions the data into folds, so if we want to repeat the CV process with different random partitions, we increase the `REP` value. Due to the small sample size and number of folds, we'll do 10 repetitions.\n\n```{r}\ntg.cv = bestglm(tg_train, IC=\"CV\", CVArgs=list(Method=\"HTF\", K=3, REP=10))\ntg.cv\n```\n\nThe model above is the model with the fewest predictors that is within one standard error of the model with the lowest CV error. To illustrate this relationship, next we'll visualize how this model was determined based on the CV and standard errors. We can get the CV errors and the $se$ from the `tg.cv` object.\n\n```{r}\n#| echo: false\n#| message: false\n#| warning: false\nlibrary(tidyverse)\ntg.df = tibble(\n  cv = tg.cv$Subsets$CV,    # cross validation errors\n  se = tg.cv$Subsets$sdCV,  # standard errors\n  cv.low = cv - se,\n  cv.high= cv + se,\n  p = 0:(length(cv)-1)      # number of predictors\n)\n\nggplot(tg.df) +\n  geom_segment(aes(x=0:4, xend=0:4, y=cv.low, yend=cv.high), color='blue') +\n  geom_line(aes(x=p, y=cv), color='red') +\n  geom_point(aes(x=p, y=cv), color='red', size=3) +\n  geom_hline(yintercept = tg.df %>% filter(p==3) %>% .$cv.high, color='blue', linetype=2) +\n  xlab(\"Number of Predictors\") +\n  ylab(\"CV Prediction Error\") +\n  annotate(\"text\", label=\"Model With\\nLowest CV Error\", x=3, y=1300) +\n  annotate(\"text\", label=\"Best Model\", x=1, y=1300) +\n  annotate(\"text\", label=\"1 se\", x=2.9, y=910, angle=90, color='blue') +\n  theme_bw()\n\n```\n\n### What About The Test Set?\n\nThis model selection method included `income` as the only predictor variable in their respective best model. However, the coefficients differ between the two models, so now we can bring in the test set and compare against the best BIC model. For a fair comparison with the CV results, we'll find the best model using BIC on the training set only.\n\n```{r}\n# get the BIC model on the training set only\ntg_train.BIC = bestglm(tg_train, IC=\"BIC\")\nbic_preds = predict(tg_train.BIC$BestModel, newdata = data.frame(tg_test[, -5]))\n\nprint(\"BIC predictors included are:\")\nprint(tg_train.BIC$BestModel$coefficients)\n```\n\nNow we'll get the CV model.\n\n```{r}\n# based on the CV results, only income should be included as a factor\ncv.glm = glm(gamble~income, data=tg_train)\ncv_preds = predict(cv.glm, newdata = data.frame(tg_test[, -5]))\n```\n\nWe'll use mean absolute error as our measure of error.\n\n```{r}\n# calculate and compare mean absolute error\nprint(paste(\"BIC mean absolute error:\", round(mean(abs(bic_preds - tg_test$gamble)), 1)))\nprint(paste(\"CV mean absolute error:\", round(mean(abs(cv_preds - tg_test$gamble)), 1)))\n```\n\nUsing mean absolute error, BIC out-performed the cross-validated model. This result shouldn't be too surprising given that the BIC model contained additional predictor variables that appeared to be statistically significant.\n\n## Lasso Regression\n\nRidge and lasso regression are closely related regularization techniques to reduce model complexity. The primary difference between the two methods is that ridge regression reduces factor coefficients close to (but not equal to) zero, while lasso regression reduces the coefficients all the way to zero, which makes it useful for reducing model complexity by eliminating factors.\n\n### Background Reading\n\nFor the theoretical framework, please refer to [this article](https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b). Don't worry about the Python code if you're not familiar with it. Just read the text portions of the article that explain the how ridge and, more importantly, lasso regression work.\n\n### Lasso Regression In R\n\nLasso regression is particularly useful when a dataset has many factors, but we'll continue to use the `teengamb` data so we can compare the results with the `stepAIC()` method. Performing lasso regression with the `glmnet` package is straight forward. The function has two required arguments, an `x` and a `y`, where `x` are the data associated with the predictors (note `x` must be a `data.matrix`, not a `data.frame`), and `y` is the response as a vector. By default, `glmnet` automatically scales and centers the data, and then converts them back to the original scale when providing results. If we plot the results, we get the following.\n\n```{r}\n#| message: false\n#| warning: false\nlibrary(glmnet)\n\n# for some reason, glmnet works best with data.matrix instead of as.matrix\nx = data.matrix(tg_train[-5])\ny = tg_train$gamble\n\ntg.lasso = glmnet(x, y)\nplot(tg.lasso, xvar=\"lambda\", label=TRUE)\n```\n\nEach of the above lines represents a predictor. The number next to each line on the left side of the plot refers to the column number in the `x` matrix. The vertical axis represents the factor coefficient. The bottom x axis is $log(\\lambda)$, and the top x axis is the associated number of predictors included in the model.\n\nSo how do we interpret this plot? At the far right, we can see that the coefficient for every predictor is zero. In other words, this is the null model. As $\\lambda$ decreases, predictors are added one at a time to the model. Since predictor #3 (`income`) is the first to have a non-zero coefficient, it is the most significant. `sex` (predictor #1) is the next non-zero coefficient followed by `verbal` (predictor #4) and then `status` (predictor #2). If we compare this order with the p-values from the best fit linear model, we see that there is consistency. Note that `income` was the first non-zero coefficient, and it has the lowest p-value in the linear model. Also note that the maximum coefficients in the lasso regression plot are also consistent with the linear model coefficients.\n\nOur task now is to find the model that has good predictive power while including only the most significant predictors. In other words, we need a method to find the right $\\lambda$ value. Before we get to how we identify that $\\lambda$, let's look at some other useful information from `tg.lasso`. If we print our glmnet object, we see (going by columns from left to right) the number of predictors included in the model (Df, not to be confused with the degrees of freedom in a linear model summary), the percent of null deviance explained, and the associated $\\lambda$ value.\n\n```{r}\nprint(tg.lasso)\n```\n\nWe can also see the coefficient values for any given $\\lambda$ with `coef`. We can see that small values of $\\lambda$ include more predictors and so correspond with the right side of the plot above. We can get the coefficients for any given $\\lambda$ value with `coef()`. If we choose the smallest values of $\\lambda$ from the above data, we get:\n\n```{r}\n# Note that we specify lambda with s\ncoef(tg.lasso, s=0.0626)\n```\n\nNow we can more directly compare these coefficients to the full linear model coefficients. Recall that we withheld a test set prior to performing lasso regression, so the coefficients are close, but not equal to the linear model coefficients.\n\n```{r}\nsumary(lm(gamble~., data=teengamb))\n```\n\nIf we choose a $\\lambda$ associated with 2 Df, we see that only two predictors have non-zero coefficients.\n\n```{r}\ncoef(tg.lasso, s=5.9770)\n```\n\nTo find the optimal value for $\\lambda$, we use cross validation again. We can include cross validation in the `glmnet()` function by prepending `cv.` as shown below. The default number of folds in the `cv.glmnet` function is 10, which is fine for this example. There's a built-in method for plotting the results as we did manually above.\n\n```{r}\ntg.cv = cv.glmnet(x, y)\nplot(tg.cv)\n```\n\nWhat we get is the cross validation curve (red dots) and two values for $\\lambda$ (vertical dashed lines). The left dashed line is the value of lambda that gives the minimum mean cross-validated error. The right dashed line is the value of $\\lambda$ whose error is within one standard deviation of the minimum. This is the $\\lambda$ we've been after. We can get the coefficients associated with this $\\lambda$ by specifying `s = \"lambda.1se\"`. Our cross validated best fit lasso regression model is shown below.\n\n```{r}\ncoef(tg.cv, s = \"lambda.1se\")\n```\n\nFor a more thorough discussion of the `glmnet` package, including its use with non-Gaussian data, refer to the [vignette](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html) written by Trevor Hastie and Junyang Qian.\n\n## Parting Thought\n\nIn this chapter, we have seen that different methods for model selection can produce different \"best\" models, which might make you leery about the whole thing. Remember the George Box quote:\n\n> All models are wrong...\n\nWe're just trying to find one that's useful.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"paged","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"model_selection.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.280","editor":"visual","theme":{"dark":"darkly","light":"flatly"},"title":"Model Selection","author":"John King","date":"5/24/2020","bibliography":["references.bib"],"link-citations":true,"code-copy":true},"extensions":{"book":{"multiFile":true}}}}}