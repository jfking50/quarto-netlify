{"title":"Multiple Linear Regression","markdown":{"yaml":{"title":"Multiple Linear Regression","author":"John King","date":"5/23/2020","format":{"html":{"toc":true,"code-copy":true,"df-print":"paged"}},"execute":{"warning":false,"echo":true}},"headingText":"| echo: false","containsRefs":false,"markdown":"\n\nIn the previous section we considered just one predictor and one response. The linear model can be expanded to include multiple predictors by simply adding terms to the equation:\n\n$$y = \\beta_{0} + \\beta_{1}x_{1}+ \\beta_{2}x_{2} + ... + \\beta_{(p-1)}x_{(p-1)} + \\varepsilon$$\n\nWith one predictor, least squares regression produces a regression line. With two predictors, we get a regression plane (shown below), and so on up to a $(p-1)$ dimensional hyperplane.\n\n```{r}\n#| message: false\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(plotly)\n\nset.seed(42)\nmlr = tibble(x1 = runif(10, 0, 10),\n             x2 = runif(10, 0, 10),\n             y = 4 + 0.05*x1 + 0.05*x2 + rnorm(10))\n\nbetas = coef(lm(y~., data=mlr))\n\nplane = tibble(x1=c(0, 10, 0, 10),\n               x2=c(0, 0, 10, 10),\n               y=c(betas[1], betas[1]+10*betas[2], betas[1]+10*betas[3], betas[1]+sum(10*betas[2:3])))\n\nplot_ly() %>%\n  add_trace(data = mlr, x=~x1, y = ~x2, z = ~y, type='scatter3d', mode='markers', \n            marker=list(color='black', size=7), showlegend=FALSE) %>%\n  add_trace(data = plane, x=~x1, y = ~x2, z = ~y, type='mesh3d', \n            facecolor=c('blue', 'blue'), opacity = 0.75, showlegend=FALSE) %>%\n  layout(title = 'Best Fit Plane', showlegend = FALSE,\n         scene = list(xaxis = list(range=c(0,10)),\n                      yaxis = list(range=c(0,10)),\n                      camera = list(eye = list(x = 0, y = -2, z = 0.3))))\n```\n\n## Linear Algebra Solution\n\nWith two or more predictors, we can't solve for the linear model coefficients the same way we did for the one predictor case. Solving for the coefficients (the $\\beta$s) requires some linear algebra. Given a data set with $n$ observations and one predictor, the $y$ (response), $X$ (predictor), and $\\beta$ (coefficient) matrices are written as:\n\n$$y= \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{pmatrix} , X= \\begin{pmatrix} 1 & x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{pmatrix}, \\beta= \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}$$\n\nIncorporating the error term, we have:\n\n$$\\varepsilon= \\begin{pmatrix} \\varepsilon_1 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix} = \\begin{pmatrix} y_1-\\beta_0-\\beta_1x_1 \\\\ \\vdots \\\\ y_n-\\beta_0-\\beta_1x_n \\end{pmatrix} = y-X\\beta$$\n\nOnce we solve for the coefficients, multiplying them by the predictors gives the estimated response, $\\hat{y}$.\n\n$$X\\beta \\equiv \\hat{y}$$\n\nWith multiple linear regression, we expand the $X$ and $\\beta$ matrices accordingly.\n\n$$y= \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{pmatrix} , X= \\begin{pmatrix} 1 & x_{11} & \\ldots & x_{1p-1} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & \\ldots & x_{np-1} \\end{pmatrix}, \\beta= \\begin{pmatrix} \\beta_0 \\\\ \\vdots \\\\ \\beta_{p-1} \\end{pmatrix}$$\n\nIncorporating error:\n\n$$\\varepsilon= \\begin{pmatrix} \\varepsilon_1 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix} = \\begin{pmatrix} y_1-(\\beta_0-\\beta_1x_{11} + \\ldots + \\beta_{p-1}x_{1p-1}) \\\\ \\vdots \\\\ y_n-(\\beta_0-\\beta_1x_{n1} + \\ldots + \\beta_{p-1}x_{np-1}) \\end{pmatrix} = y-X\\beta$$\n\nHowever, notice that the final equation remains unchanged.\n\n$$X\\beta \\equiv \\hat{y}$$\n\nThe residual sum of squares (RSS) also remains unchanged, and so do the other equations that have RSS as a term, such as residual standard error and $R^2$. The following is an example of solving the system of equations for a case with two predictors and no error. Given $n=4$ observations, we have the following system of equations:\n\n$$x_0 = 10$$ $$x_0 + x_2 = 17$$ $$x_0 + x_1 = 15$$ $$x_0 + x_1 + x_2 = 22$$\n\nIn this example, we technically have all of the information we need to solve this system of equations without linear algebra, but we'll apply it anyway to demonstrate the method. Rewriting the above system of equations into matrix form gives:\n\n$$X= \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 0 \\\\ 1 & 1 & 1 \\end{pmatrix}, y= \\begin{pmatrix} 10 \\\\ 17 \\\\ 15 \\\\ 22 \\end{pmatrix}$$\n\nOne way to solve for the $\\beta$ vector is to transpose the $X$ matrix and multiply it by the $X|y$ augmented matrix.\n\n$$X^TX|y = \\begin{pmatrix} 1&1&1&1 \\\\ 0&0&1&1 \\\\ 0&1&0&1 \\end{pmatrix} \\begin{pmatrix} 1&0&0&|&10 \\\\ 1&0&1&|&17 \\\\ 1&1&0&|&15 \\\\ 1&1&1&|&22 \\end{pmatrix} = \\begin{pmatrix} 4&2&2&|&64 \\\\ 2&2&1&|&37 \\\\ 2&1&2&|&39 \\end{pmatrix}$$\n\nUse Gaussian elimination to reduce the resulting matrix by first multiplying the top row by $-\\frac{1}{2}$ and adding those values to the second row.\n\n$$\\begin{pmatrix} 4&2&2&|&64 \\\\ 0&1&0&|&5 \\\\ 2&1&2&|&39 \\end{pmatrix}$$\n\nReduce further using the same process on the third row.\n\n$$\\begin{pmatrix} 4&2&2&|&64 \\\\ 0&1&0&|&5 \\\\ 0&0&1&|&7 \\end{pmatrix}$$\n\nWe find that\n\n$$\\beta_2 = 7$$ and\n\n$$\\beta_1 = 5$$\n\nand using back substitution we get\n\n$$4\\beta_0 + 2(5) + 2(7) = 64, \\enspace so \\enspace \\beta_0 = 10$$\n\nThe resulting equation:\n\n$$y=10+5x_1+7x_2$$\n\ndefines the best fit plane for this data, which is visualized below.\n\n```{r}\n#| echo: false\n#| message: false\n#| warning: false\n\nla = tibble(x1=c(0,0,1,1),x2=c(0,1,0,1),y=c(10,17,15,22))\n\nplot_ly() %>% \n  add_trace(data = la, x=~x1, y = ~x2, z = ~y, type='scatter3d', mode='markers', \n            marker=list(color='black', size=7), showlegend=FALSE) %>%\n  add_trace(data = la, x=~x1, y = ~x2, z = ~y, type='mesh3d', \n            facecolor=c('blue', 'blue'), showlegend=FALSE) %>%\n  layout(title = 'Best Fit Plane', showlegend = FALSE,\n         scene = list(camera = list(eye = list(x = -1.5, y = -1.5, z = 0.3))))\n```\n\nOf course, *R* has linear algebra functions, so we don't have to do all of that by hand. For example, we can solve for the $\\beta$ vector by multiplying both sides of the equation $X\\beta \\equiv \\hat{y}$ by $X^T$.\n\n$$X^TX\\beta = X^Ty$$\n\nSolving for $\\beta$, we get:\n\n$$\\beta=(X^TX)^{-1}X^Ty$$\n\nNow use `solve()` function to calculate the $\\beta$ vector (note that `solve()` inverts $X^TX$ automatically).\n\n```{r}\nX = matrix(c(1,0,0,\n             1,0,1,\n             1,1,0,\n             1,1,1), byrow = TRUE, ncol=3)\n\ny = 10 + 5*X[, 2] + 7*X[, 3]\n\nsolve(t(X) %*% X) %*% (t(X) %*% y)\n```\n\nFitting a linear model in *R* using the `lm()` function produces coefficients identical to the above results.\n\n```{r}\ncoef(lm(y ~ X[, 2] + X[, 3]))\n```\n\nTechnically, neither the `solve()` nor the `lm()` functions use Gaussian elimination when solving the system of equations. According to [this site](https://software.intel.com/content/www/us/en/develop/documentation/mkl-developer-reference-c/top/lapack-routines/lapack-least-squares-and-eigenvalue-problem-routines.html), for overdetermined systems (where there are more equations than unknowns) like the example we're working with, they use QR factorization instead. The details of QR factorization are beyond the scope of this course, but are explained well on [these slides](http://www.seas.ucla.edu/~vandenbe/133A/lectures/ls.pdf) from a course at UCLA's School of Engineering and Applied Sciences. In essence, the $X$ matrix is decomposed into $Q$ and $R$ matrices that are substituted for $X$ in the equation.\n\n$$X^TX\\beta = X^Ty$$\n\n$$(QR)^T(QR)\\beta = (QR)^Ty$$\n\nSkipping a lot of math, we end up with:\n\n$$R\\beta=Q^Ty$$\n\nIn *R*, use `qr(X)` to decompose $X$, and then use `solve.qr()` to calculate the $\\beta$ vector.\n\n```{r}\nQR = qr(X)\nsolve.qr(QR, y)\n```\n\nNow we'll make it a little more complicated by returning to the data set plotted at the beginning of this section. It consists of $n=10$ observations with random error.\n\n```{r}\nmlr # multiple linear regression data set\n```\n\nUsing QR decomposition, we get the following coefficients:\n\n```{r}\n# we need to add a column of 1's to get beta_0 for the intercept\nintercept = rep(1, 10)\nQR = qr(cbind(intercept, mlr[, 1:2])) \nbetas = solve.qr(QR, mlr$y)\nbetas\n```\n\nAnd we get the following coefficients in the linear model:\n\n```{r}\ncoef(lm(y ~ ., data=mlr))\n```\n\nThe following code chunk shows how the earlier interactive plot was generated. Note the following:\n\n-   The value of `y` defined by the plane at (`x1`, `x2`) = (0,0) is $\\beta_0$ (shown by the red dot).\n\n-   The slope of the line at the intersection of the plane with the `x1` axis is $\\beta_1$.\n\n-   The slope of the line at the intersection of the plane with the `x2` axis is $\\beta_2$,\n\n```{r}\n# define the bast fit plane using the betas from QR decomposition\nplane = tibble(x1=c(0, 10, 0, 10),\n               x2=c(0, 0, 10, 10),\n               y=c(betas[1], betas[1]+10*betas[2], betas[1]+10*betas[3], betas[1]+sum(10*betas[2:3])))\n\n# use plotly for interactive 3D graphs\nplot_ly() %>%\n  # add the points to the graph\n  add_trace(data = mlr, x=~x1, y = ~x2, z = ~y, type='scatter3d', mode='markers', \n            marker=list(color='black', size=7), showlegend=FALSE) %>%\n  # add the plane\n  add_trace(data = plane, x=~x1, y = ~x2, z = ~y, type='mesh3d', \n            facecolor=c('blue', 'blue'), opacity = 0.75, showlegend=FALSE) %>%\n  # add the red dot\n  add_trace(x=0, y=0, z=betas[1], type='scatter3d', mode='markers',\n            marker=list(color='red', size=7), showlegend=FALSE) %>%\n  # adjust the layout\n  layout(title = 'Best Fit Plane', showlegend = FALSE,\n         scene = list(xaxis = list(range=c(0,10)),\n                      yaxis = list(range=c(0,10)),\n                      camera = list(eye = list(x = 0, y = -2, z = 0.3))))\n```\n\n## Multiple Linear Regression in *R*\n\n::: callout-note\nThis section is the result of a collaborative effort with Dr. Stephen E. Gillespie.\n:::\n\nBelow is a short example on doing multiple linear regression in *R*. This example uses a data set on patient satisfaction as a function of their age, illness severity, anxiety level, and a surgery variable (this is a binary variable, we will ignore for this exercise). We will attempt to model patient satisfaction as a function of age, illness severity, and anxiety level. First, read the data and build the linear model.\n\n```{r}\n# Read the data\npt <- read.csv('data/PatientSatData.csv', sep = ',', header = T)\n\n# let's drop SurgMed as we're not going to use it\npt <- pt %>% select(-SurgMed)\n\n# View the data\npt\n```\n\nNote that our data is formatted in numeric format, which is what we need for this sort of modeling.\n\nWe can look at our data. In multiple regression, `pairs` is useful.\n\n```{r}\npairs(pt)\n```\n\nWe can see some useful things:\n\n1.  Age and satisfaction appear to have a linear relationship (the bottom left corner)\n\n2.  Illness severity and satisfaction appear to have a linear relationship, though not as strongly\n\n3.  It's less clear for anxiety and satisfaction.\n\n4.  Age and illness severity do not appear to have a relationship.\n\n5.  Age and anxiety might have a relationship, but its not fully clear.\n\n6.  Illness severity and anxiety do not appear to have a relationship\n\nModel the data. Note how this format is analogous to ANOVA with multiple factors and simple linear regression.\n\n```{r}\nptLM <- lm(Satisfaction ~ Age + illSeverity + Anxiety, data = pt)\n```\n\nWe can now view our model results. We will use $\\alpha = .05$ as our appropriate significance level.\n\n```{r}\n# we view the summary results\nsummary(ptLM)\n```\n\nWe see several things. First, we can say that the intercept, age, and illness severity are all statistically significant. Anxiety does not appear to be significant. As expected given our individual results, our F-statistic (sometimes called a model utility test) shows us that there is at least one predictor that is significant. Further we can see that our $R^2$ and $R_{adj}^2$ are both relatively high, which shows that these predictors explain much of the variability in the data. We can see our RSE is about 7, which is not too extreme given our the range on our outputs.\n\nAs we do not find anxiety significant, we can drop it as an independent variable (we discuss model selection in the next chapter). Our new model is then:\n\n```{r}\nptLM2 <- lm(Satisfaction ~ Age + illSeverity, data = pt)\nsummary(ptLM2)\n```\n\nWe get similar results. We can then build our linear model:\n\n$$[Patient Satisfaction] = 143 + -1.03[Age] + -0.556[Illness Severity] + \\epsilon$$\n\n### Interpretation\n\nWe can interpret this as saying that for every additional year of age, a patient's satisfaction drops about a point and for every additional point of illness severity, a patient loses about half a point of satisfaction. That is, the older and sicker you are, the less likely you are to be satisfied. This generally seems to make sense.\n\n### Confidence Intervals\n\nMoreover, we can use the model to show our confidence intervals on our coefficients using `confint`.\n\n```{r}\nconfint(ptLM2, level = .95)\n\n# We can then say, with 95% confidence that our intercept is in the interval ~ (131, 156)\n```\n\n::: callout-note\n`confint` requires:\n\n1.  A model, called `ptLM2` in this case.\n\n2.  You can also pass it your 1-alpha level (the default is alpha = .05, or .95 confidence).\n\n3.  You can also pass it specific parameters to check (useful if working with amodel with many parameters).\n:::\n\n### Predictions\n\nWe can use our model to predict a patient's satisfaction given their age and illness severity using `predict` in the same manner as simple linear regression. For a point estimate:\n\n```{r}\npredict(ptLM2, list(Age = 35, illSeverity=50))\n```\n\nAn individual response will be in this interval:\n\n```{r}\npredict(ptLM2,list(Age=35, illSeverity=50),interval=\"prediction\")\n```\n\nThe mean response for someone with these inputs will be:\n\n```{r}\npredict(ptLM2,list(Age=35, illSeverity=50),interval=\"confidence\")\n```\n\nWe can also plot these values. Note that our result with two predictors is a plane in this case. With 3+ predictors, it is a hyperplane. Often, we will plot either a contour map where each line corresponds to a fixed level of a predictor or just choose a single predictor.\n\nWe can produce a contour plot using `geom_contour` (one can also use `stat_contour`). Of course, this works for two predictors. As the number of independent variables increases, visualizing the data becomes somewhat more challenging and requires visualizing the solution only a few dimensions at a time.\n\n```{r}\n# Requires a set of points with predictions:\n# Produce a data frame that is every combination of Age and issEverity\nmySurface <- expand_grid( \n  Age = seq(min(pt$Age), max(pt$Age), by = 1), \n  illSeverity = seq(min(pt$illSeverity), max(pt$illSeverity), by = 1)) \n\n# look at our data\nhead(mySurface)\n```\n\nNow add in our predictions. Recall `predict` takes a data frame with columns that have the same name as the variables in the model.\n\n```{r}\nmySurface$Satisfaction <- predict(ptLM2, mySurface) \nhead(mySurface)\n```\n\nPlot the contours for our response surface.\n\n```{r}\nggplot(data = mySurface,\n       aes(x = Age, y = illSeverity, z = Satisfaction)) + \n  # you can use a number of ways to do this.  geom_contour works\n  geom_contour(aes(color = after_stat(level))) + \n  # This color argument varies the color of your contours by their level\n  scale_color_distiller(palette = 'Spectral', direction = -1) + \n  # clean up the plot\n  theme_minimal() + \n  xlab('Age') + ylab('Illness Severity') + \n  ggtitle('Patient Satisfaction Response Surface') \n```\n\nWith this plot, we can clearly see at least two things:\n\n-   Our mathematical interpretation holds true. The younger and less severely ill the patient, the more satisfied they are (in general, as based on our model).\n-   Our model is a plane. We see this with the evenly spaced, linear contour lines.\n\nIt is also useful to overlay the actual observations on the plot. We can do this as follows:\n\n```{r}\n# This is our original contour plot as produced above, with one exception.  \n# We move the data for the contour to the geom_contour so we can also plot the observations\nggplot() + \n  geom_contour(data = mySurface, aes(x = Age, y = illSeverity, z = Satisfaction, color = after_stat(level))) + \n  # This color argument varies the color of your contours by their level\n  scale_color_distiller(palette = 'Spectral', direction = -1) + \n  theme_minimal() + xlab('Age') + ylab('Illness Severity') + \n  ggtitle('Patient Satisfaction Response Surface')  + \n  geom_point(data = pt, aes(x = Age, y = illSeverity, color = Satisfaction))\n\n\n```\n\n### Outliers\n\nBy plotting these points, we can compare our results (the contour lines) to the observations. The first thing this allows us to do is look for outliers. For example, there are two around the age 30 and a severity of illness; note how their colors are disjoint from what the contour colors predict. This, of course, is harder to interpret than a simple linear regression as it involves comparing colors. In general, it is easier to use the numbers for higher dimensional models. Second, we can get an idea of leverage or areas of our model that are not informed by data. For example, there are no observations in this region:\n\n```{r}\n#| echo: false\n#| \nggplot() + \n  geom_contour(data = mySurface, aes(x = Age, y = illSeverity, z = Satisfaction, color = after_stat(level))) + \n  scale_color_distiller(palette = 'Spectral', direction = -1) +\n  theme_minimal() + xlab('Age') + ylab('Illness Severity') + \n  ggtitle('Patient Satisfaction Response Surface')  + \n  geom_point(data = pt, aes(x = Age, y = illSeverity, color = Satisfaction)) + \n  geom_segment(aes(x=50, xend=50, y=50, yend=70), color = 'black', size = 2) + \n  geom_segment(aes(x=20, xend=50, y=50, yend=50), color = 'black', size = 2) + \n  geom_segment(aes(x=20, xend=50, y=70, yend=70), color = 'black', size = 2) + \n  geom_segment(aes(x=20, xend=20, y=50, yend=70), color = 'black', size = 2) + \n  geom_label(aes(x = 35, y = 60, label = 'No Observations'))\n\n```\n\nThat means that any predictions in this region are ill-informed and extrapolations beyond the data. In an advanced design section, we will discuss how ensuring we get \"coverage\" or \"space-filling\" is an important property for good experimental designs so we can avoid this problem.\n\n### Assumptions\n\nFinally, we can check our model to ensure that it is legitimate. Check assumptions:\n\n```{r}\n# Plot our standard diagnostic plots\npar(mfrow = c(2,2))\nplot(ptLM2)\n```\n\nIt appears that we meet our linearity, independence, normality and homoscedasticity assumptions. There are no significant patterns, though we may have a few unusual observations.\n\n```{r}\n# Check normality\nshapiro.test(ptLM2$residuals)\n```\n\nCheck homoscedasticity.\n\n```{r}\ncar::ncvTest(ptLM2)\n```\n\nWe meet our assumptions, so also check for unusual observations.\n\n```{r}\n# We can identify points that have residuals greater than 2 standard deviations away from our model's prediction\nptLM2$residuals[abs(ptLM2$residuals) >= 2*sd(ptLM2$residuals)]\n# Point 9 is an outlier\n```\n\nWe can check for leverage points with a number of ways. We'll check using Cook's distance.\n\n```{r}\nplot(ptLM2, which = 4)\n# Again point 9 is a point of significant leverage. \n```\n\nBased on these results, we may consider dropping point nine. Before doing so, we should check for data entry errors or anything unusual about that data point. If we do drop it, we should note that we did so in our analysis.\n\nIf we do conclude that point nine should be dropped, we can build a new linear model:\n\n```{r}\n# Just check the summary of the model with Point 9 dropped\nsummary(lm(Satisfaction ~ Age + illSeverity, data = pt[-9,]))\n```\n\nNote that our standard errors decrease somewhat and our $R^2$ increases, indicating this is a better model (although on a smaller subset of the data).\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"paged","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"mlr.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.280","editor":"visual","theme":{"dark":"darkly","light":"flatly"},"title":"Multiple Linear Regression","author":"John King","date":"5/23/2020","code-copy":true},"extensions":{"book":{"multiFile":true}}}}}