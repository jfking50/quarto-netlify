{"title":"Nonparametric ANOVA","markdown":{"yaml":{"title":"Nonparametric ANOVA","author":"John King","date":"5/28/2020","format":{"html":{"toc":true,"code-copy":true,"df-print":"paged"}},"execute":{"warning":false,"echo":true},"bibliography":"references.bib","link-citations":true},"headingText":"Non-Parametric ANOVA","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nlibrary(Cairo)\n\nknitr::opts_chunk$set(\n  echo = TRUE,\n  message=FALSE, \n  warning=FALSE,\n  dev.args = list(png = list(type = \"cairo\"))\n  )\n```\n\nAs we've seen in the last few posts, linear models can be successfully applied to many data sets. However, there may be times when even after transforming variables, your model clearly violates the assumptions of linear models. Alternatively, you may have evidence that there is a complex relationship between predictor and response that is difficult to capture through transformation. In these cases, non-parametric regression techniques offer alternative methods for modeling your data.\n\n\nOne of the assumptions of parametric ANOVA is that the underlying data are normally distributed. If we have a data set that violates that assumption, as is often the case with counts (especially of relatively rare events), then we'll need to use a non-parametric method such as the Kruskal-Wallis (KW) test.\n\nThe setup for the KW test is as follows:\n\n-   Level 1: $X_{11}, X_{12}, ...,X_{1J_{1}} \\sim F_{1}$\n-   Level 2: $X_{21}, X_{22}, ...,X_{2J_{2}} \\sim F_{2}$\n-   ...\n-   Level I: $X_{I1}, X_{I2}, ...,X_{IJ_{I}} \\sim F_{I}$\n\n**Null hypothesis** $H_{o}: F_{1} = F_{2} = ... = F_{I}$\n\n**Alternative hypothesis** $H_{a}:$ not $H{o}$ (i.e., at least two of the distributions are different).\n\nThe idea behind the KW test is to sort the data and use an observation's rank instead of the value itself. If we have a sample size $N = J_{1}, J_{2}, ... J_{I}$, we first rank the observations (the lowest value gets a 1, second lowest a 2, etc.). If there are ties, then use the mid-rank (the mean of the two ranks). Then, separate the samples into their respective levels and sum the ranks for each level. For example, if we have three levels:\n\n-   Level 1: $R_{11} + R_{12} + ... + R_{1J_{1}}=$ sum of ranks\n-   Level 2: $R_{21} + R_{22} + ... + R_{2J_{1}}=$ sum of ranks\n-   Level 3: $R_{31} + R_{32} + ... + R_{3J_{1}}=$ sum of ranks\n\nNow we do the non-parametric equivalent of a sum of squares for treatment (SSTr) using these ranks. The KW test statistic takes the form:\n\n$$K=\\frac{12}{N(N+1)}\\sum\\limits_{i=1}^{I}{\\frac{R^2_{i}}{J_{i}}-3(N+1)}$$\n\nFor hypothesis testing, we calculate a p-value where large values of K signal rejection. We do this by approximating K with a chi-square distribution having $I-1$ degrees of freedom. According to @devore2015, chi-square is a good approximation for K if:\n\n1.  If there are I = 3 treatments and each sample size is \\>= 6, or\n2.  If there are I \\> 3 treatments and each sample size is \\>= 5\n\nLet's look at an example. Say we are a weapon manufacturer that produces rifles on three different assembly lines, and we want to know if there's a difference in the number of times a weapon jams. We select 10 random weapons from each assembly line and perform our weapon jamming test identically on all 30 weapons. I'll make up some dummy data for this situation by drawing random numbers from Poisson distributions with two different $\\lambda$s.\n\n```{r message=FALSE, warning=FALSE}\nlibrary(tidyverse)\nset.seed(42)\n\nj = c(rpois(10, 10), rpois(10, 10), rpois(10, 15))\nr = rank(j, ties.method=\"average\")\nl = paste(\"line\", rep(1:3, each = 10), sep=\"\")\n\ntibble(\n  jams1 = j[1:10],\n  rank1 = r[1:10],\n  jams2 = j[11:20],\n  rank2 = r[11:20],\n  jams3 = j[21:30],\n  rank3 = r[21:30])\n\n```\n\nFor this data,\n\n-   $I = 3$\n-   $J_{1} = J_{2} = J_{3} = 10$\n-   $N = 10 + 10 + 10 = 30$\n-   $R_{1} =$ `r sum(r[1:10])`\n-   $R_{2} =$ `r sum(r[11:20])`\n-   $R_{3} =$ `r sum(r[21:30])`\n\nWhich gives the K statistic:\n\n$$K=\\frac{12}{30(31)} \\left[\\frac{120^2}{10} + \\frac{131.5^2}{10} + \\frac{213.5^2}{10}\\right] - 3(31)$$\n\n```{r}\nK = 12/(30*31) * (sum(r[1:10])^2/10 + sum(r[11:20])^2/10 + sum(r[21:30])^2/10) - 3*31\nprint(paste(\"K =\", K), quote=FALSE)\n```\n\nThen, compute an approximate p-value using the chi-square test with two degrees of freedom.\n\n```{r}\n1 - pchisq(K, df=2)\n```\n\nWe therefore reject the null hypothesis at the $\\alpha = 0.5$ test level. Of course, there's an *R* function `kruskall.test()` so we don't have to do all that by hand.\n\n```{r}\nrifles = tibble(jams = j, line = l)\n\nkruskal.test(jams ~ line, data = rifles)\n```\n\n### Multiple Comparisons\n\nThe KW test can also be used for multiple comparisons. Recall that we applied the Tukey Test to the parametric case, and it took into account that there is an increase in the probability of a Type I error when conducting multiple comparisons. The non-parametric equivalent is to combine the Bonferroni Method with the KW test. Consider the case where we conduct $m$ tests of the null hypothesis. We calculate the probability that at least one of the null hypotheses is rejected ($P(\\bar{A})$) as follows:\n\n$$P(\\bar{A}) = 1-P(A) = 1-P(A_{1} \\cap A_{2} \\cap...\\cap A_{m})$$\n\n$$=1-P(A_{1})P(A_{2})\\cdot\\cdot\\cdot P(A_{m})$$\n\n$$=1-(1-\\alpha)^{m}$$\n\nSo if our individual test level target is $\\alpha=0.05$ and we conduct $m=10$ tests, then $P(\\bar{A})=1-(1-0.05)^{10}=$ `r 1-(1-0.05)^10`. If we want to establish a **family-wide Type I error rate**, $\\Psi=P(\\bar{A})=0.05$, then the individual test levels should be:\n\n$$\\alpha=P(\\bar{A}_{i})=1-(1-\\Psi)^{1/m}$$\n\nFor example, if $\\Psi=0.05$ and we again conduct $m=10$ tests, then $\\alpha=1-(1-0.05)^{1/10}=$ `r 1-(1-0.05)^(1/10)`. The downfall of this approach is that the same data are used to test the collection of hypotheses, which violates the independence assumption. The Bonferroni Inequality saves us from this situation because it doesn't rely on the assumption of independence. Using the Bonferroni method, we simply calculate $\\alpha=\\Psi/m = 0.005$. Note that this is almost identical to the result when we assumed independence. Unfortunately, the method isn't perfect. As noted in Section 2 of [this paper](https://www.stat.berkeley.edu/~mgoldman/Section0402.pdf), the Bonferroni method tends to be overly conservative, which increases the chance of a false negative.\n\nUsing the `rifles` data from earlier, if we wanted to conduct all three pair-wise hypothesis tests, then $\\alpha=0.05/3=0.01667$ for the individual KW tests.\n\nThe `dunn.test()` function from the aptly-named `dunn.test` package performs the multiple pair-wise tests and offers several methods for accounting for performing multiple tests, including Bonferroni. For example (and notice we get the KW test results also):\n\n```{r}\ndunn.test::dunn.test(rifles$jams, rifles$line, method=\"bonferroni\")\n```\n\nAccording to the `dunn.test` documentation, the null hypothesis for the Dunn test is:\n\n> The null hypothesis for each pairwise comparison is that the probability of observing a randomly selected value from the first group that is larger than a randomly selected value from the second group equals one half; this null hypothesis corresponds to that of the Wilcoxon-Mann-Whitney rank-sum test. Like the ranksum test, if the data can be assumed to be continuous, and the distributions are assumed identical except for a difference in location, Dunn's test may be understood as a test for median difference. 'dunn.test' accounts for tied ranks.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"paged","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"np_anova.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.280","editor":"visual","theme":{"dark":"darkly","light":"flatly"},"title":"Nonparametric ANOVA","author":"John King","date":"5/28/2020","bibliography":["references.bib"],"link-citations":true,"code-copy":true},"extensions":{"book":{"multiFile":true}}}}}