{"title":"Simple Linear Regression","markdown":{"yaml":{"title":"Simple Linear Regression","author":"John King","date":"5/20/2020","format":{"html":{"toc":true,"code-copy":true,"df-print":"paged"}},"execute":{"warning":false,"echo":true}},"headingText":"Least Squares Method Manually","containsRefs":false,"markdown":"\n\nThe purpose of regression is to describe a relationship that explains one variable (the *response*, or the \"y\" variable) based on one or more other variables (the *predictors*, or the \"x\" variables). The simplest deterministic mathematical relationship between two variables $x$ and $y$ is a linear relationship, which we define as:\n\n$$y = \\beta_{0} + \\beta_{1}x + \\varepsilon$$\n\nwhere,\n\n-   $\\beta_{0}$ is the y-intercept\n-   $\\beta_{1}$ is the slope\n-   $\\varepsilon$ is the error in $y$ not explained by $\\beta_{0}$ and $\\beta_{1}$.\n\nIf there is no error in the model and a linear relationship exists, we could predict the true value of y given any value of x. With error, however, we can only estimate y, which we annotate by $\\hat{y}$. The regression line itself is determined using the *least squares* method, which involves drawing a line through the centroid of the data, and adjusting the slope until the squared distance between the straight line and each observed value (the *residual*) is minimized. For example, assume we have the following observations of height in inches (predictor) and weight in pounds (response).\n\n```{r message=FALSE, warning=FALSE}\nlibrary(tidyverse)\nlibrary(GGally)\n\ndf = tibble(\n  height = c(66, 54, 50, 74, 59, 53),\n  weight = c(141, 128, 123, 160, 136, 139)\n)\n```\n\n\nThe centroid coordinates $(\\bar{x},\\bar{y})$ are calculated simply by $\\bar{x}$ = `mean(df$height)` = `r round(mean(df$height), 2)`, and $\\bar{y}$ = `mean(df$weight)` = `r round(mean(df$weight), 2)`. Plotting the data with the centroid, we get:\n\n```{r}\n#| echo: false\n\ncentroid = tibble(\n    height = mean(df$height),\n    weight = mean(df$weight))\n\nggplot() +\n  geom_point(data=df, aes(x=height, y=weight)) +\n  geom_point(data = centroid, aes(x=height, y=weight), color='red', size=5) +\n  annotate(\"text\", x=61, y=138, label=\"centroid\", color='red') +\n  theme_bw()\n```\n\nTo find the slope, $\\beta_{1}$, we calculate how much each height and weight observation deviate from the centroid, multiply those paired deviations, sum them, and divide that by the sums of the squared height deviations. With the height and weight data, we find:\n\n```{r}\ndf = df %>%\n  mutate(\n    h_dev = height - mean(height),    # height deviation from centroid\n    w_dev = weight - mean(weight),    # weight deviation from centroid\n    dev_prod = h_dev * w_dev,         # the product of the deviations\n    h_dev_squared = h_dev^2           # the squared products\n  )\n```\n\n| height            | weight             | height deviance | weight deviance | deviation products               | height deviance squared |\n|------------|------------|------------|------------|------------|------------|\n| x                 | y                  | $x_{i}-\\bar{x}$ | $y_{i}-\\bar{y}$ | $(x_{i}-\\bar{x})(y_{i}-\\bar{y})$ | $(x_{i}-\\bar{x})^{2}$   |\n| 66                | 141                | 6.67            | 3.17            | 21.14                            | 44.49                   |\n| 54                | 128                | -5.33           | -9.83           | 52.39                            | 28.41                   |\n| 50                | 123                | -9.33           | -14.83          | 138.36                           | 87.05                   |\n| 74                | 160                | 14.67           | 22.17           | 325.23                           | 215.21                  |\n| 59                | 136                | -0.33           | -1.83           | 0.60                             | 0.11                    |\n| 53                | 139                | -6.33           | 1.17            | -7.41                            | 40.07                   |\n| $\\bar{x} = 59.33$ | $\\bar{y} = 137.83$ | \\-              | \\-              | $\\Sigma = 530.33$                | $\\Sigma = 415.33$       |\n\nThe slope is found by $\\beta_{1} = \\frac{\\Sigma (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{\\Sigma (x_{i}-\\bar{x})^{2}} = \\frac{530.33}{415.33} = 1.28$. For our dataset in *R*, that translates to:\n\n```{r}\nbeta1 = sum(df$dev_prod) / sum(df$h_dev_squared)\nbeta1\n```\n\nWe have now have what we need to calculate the y-intercept, $\\beta_{0} =\\bar{y}-\\beta{_1}\\bar{x}$. Equivalently in *R*:\n\n```{r}\nbeta0 = mean(df$weight) - beta1 * mean(df$height)\nbeta0\n```\n\nWhen we plot the line defined by our beta values, we find that it does, in fact, pass through the centroid and visually appears to fit the data.\n\n```{r}\nggplot(data=df, aes(x=height, y=weight)) +\n  geom_point() +\n  geom_abline(intercept=beta0, slope=beta1, color='blue') + # note the explicit use of our betas\n  #geom_smooth(formula=y~x, method=\"lm\", se=FALSE) + # how it's normally done in practice\n  geom_point(data = centroid, aes(x=height, y=weight), color='red', size=5) +\n  annotate(\"text\", x=61, y=138, label=\"centroid\", color='red') +\n  annotate(\"text\", x=67.5, y=143.5, label=\"residual\", color='blue') +\n  annotate(\"segment\", x=50, xend=50, y=123, yend=125.5, color='blue', linetype=2, size=0.5) +\n  annotate(\"segment\", x=53, xend=53, y=139, yend=129.5, color='blue', linetype=2, size=0.5) +\n  annotate(\"segment\", x=54, xend=54, y=128, yend=131.5, color='blue', linetype=2, size=0.5) +\n  annotate(\"segment\", x=59, xend=59, y=136, yend=137.5, color='blue', linetype=2, size=0.5) +\n  annotate(\"segment\", x=66, xend=66, y=141, yend=146.5, color='blue', linetype=2, size=0.5) +\n  annotate(\"segment\", x=74, xend=74, y=160, yend=156.5, color='blue', linetype=2, size=0.5) +\n  theme_bw()\n```\n\nThis method minimizes the residual sum of squares (RSS), which is represented mathematically by:\n\n$$RSS = \\sum\\limits_{i=1}^{n}{(y_{i} - \\hat{y}_{i})^2} = \\sum\\limits_{i=1}^{n}{\\hat\\varepsilon_{i}^{2}}$$\n\nand is calculated as follows.\n\n```{r}\ndf = df %>% mutate(\n  y_hat = beta0 + beta1 * height,\n  error = weight - y_hat,\n  error_squared = error^2\n)\nprint(paste(\"RSS =\", round(sum(df$error_squared), 2)))\n```\n\n| height            | weight             | predicted weight | error           | squared error          |\n|---------------|---------------|---------------|---------------|---------------|\n| x                 | y                  | $\\hat{y}_{i}$    | $y-\\hat{y}_{i}$ | $(y-\\hat{y}_{i})^2$    |\n| 66                | 141                | 146.35           | -5.35           | 28.58                  |\n| 54                | 128                | 131.02           | -3.02           | 9.14                   |\n| 50                | 123                | 125.92           | -2.92           | 8.50                   |\n| 74                | 160                | 156.56           | 3.44            | 11.83                  |\n| 59                | 136                | 137.41           | -1.41           | 1.98                   |\n| 53                | 139                | 129.75           | 9.25            | 85.63                  |\n| $\\bar{x} = 59.33$ | $\\bar{y} = 137.83$ | \\-               | \\-              | $RSS: \\Sigma = 145.66$ |\n\n### Goodness of Fit\n\nWhile RSS gives us an idea of how well the regression prediction ($\\hat{y}$) can approximate the response ($y$), it does not tell us how well the model fits the data because it has the same units as $y$. To obtain a unitless measure of fit, $R^2$ (also called the coefficient of determination), RSS is divided by the total sum of squares (TSS), and that ratio is subtracted from 1.\n\n$$R^2 = 1- \\frac{RSS}{TSS} = 1 - \\frac{\\Sigma(y_{i} - \\hat{y}_{i})^2}{\\Sigma(y_{i} - \\bar{y}_{i})^2}$$\n\nWe calculate $R^2$ for the height/weight data as follows:\n\n```{r}\n1 - sum(df$error_squared) / sum(df$w_dev^2)\n```\n\nWe interpret $R^2$ as the proportion of weight variation explained by the linear model. As a proportion, $R^2$ varies from 0 to 1, and ideally we seek models with a high $R^2$. A graphical depiction of RSS and TSS for one of the residuals illustrates their relationship.\n\n```{r}\n#| echo: false\nggplot() +\n  geom_point(data=df, aes(x=height, y=weight)) +\n  geom_abline(intercept=beta0, slope=beta1, color='blue') +\n  geom_hline(yintercept=mean(df$weight), color='black', linetype=3, size=0.5) +\n  annotate(\"text\", x=51.5, y=135, label=\"RSS\", color='red') +\n  annotate(\"text\", x=54.7, y=138.8, label=\"} TSS\", color='firebrick') +\n  annotate(\"text\", x=70, y=139, label=\"Mean Weight (y hat)\", color='black') +\n  annotate(\"segment\", x=53, xend=53, y=139, yend=129.5, color='blue', linetype=2, size=0.5) +\n  annotate(\"segment\", x=52.5, xend=52.5, y=139, yend=129.5, color='red', size=1) +\n  annotate(\"segment\", x=53.5, xend=53.5, y=139, yend=mean(df$weight), color='firebrick', size=1) +\n  theme_bw()\n```\n\n### Least Squares Method In *R*\n\nThat was a fair amount of work, and of course *R* simplifies the process. Fortunately, the syntax for creating a linear model is very similar to ANOVA.\n\n```{r}\ndf.lm = lm(weight ~ height, data=df)\nsummary(df.lm)\n```\n\nGoing through each section of the output from top to bottom:\n\n**Call:** This is simply the formula we gave the `lm()` function.\n\n**Residuals:** The residuals in order of observation.\n\n**Coefficients:**\n\n-   `Estimate` These are the $\\beta$s, and we see that they match our values calculated above.\n-   `Std. Error` is the standard deviation of each `Estimate` (or $\\beta$) and can be used to determine the 95% confidence interval (CI). For example, the 95% CI for `height` is $1.28 \\pm 1.96(0.296)$.\n-   `t value` is `Estimate` / `Std. Error`.\n-   `Pr(>|t|)` is the probability of observing a value at least as extreme as $\\beta$ using a t-distribution and $(n-p-1)$ degrees of freedom. The null hypothesis is $H_{o}: \\beta_{i}=0$. Notice that this is a test of each individual predictor.\n\n<!-- Steve comment:  If I'm not mistaken, R doesn't use a z test if you calculate CIs on your estimates.  I think it uses a t test (which, with enough observations is about the same).  It might be worth noting that this CI estimate is similar to what we did in chapter 2.  see confint section below-->\n\n**Residual standard error:** The square root of RSS divided by the difference of number of observations and the number of predictors. Or, $RSE = \\sqrt{\\frac{RSS}{n-p}}$. Degrees of freedom is $(n-p-1)$.\n\n**Multiple R-squared:** The $R^2$ we calculated above.\n\n**Adjusted R-squared:**Normalizes $R^2$ by accounting for the number of observations and predictors in the model. When conducting multiple linear regression, this is the appropriate method of measuring goodness of fit. Adjusted r-squared is calculated by: $\\bar{R}^{2} = 1 - (1 - R^{2}) \\frac{n-1}{n-p-1}$.\n\n**F-statistic:** The global test of significance where we wish to determine if *at least one* predictor is sginificant. The null hypothesis is $H_{o}: \\beta_{1}=...=\\beta_{p-1}=0$ under the F-distribution with $p-1$ and $n-p-1$ degrees of freedom.\n\nWe interpret the linear model in the following manner: **for every inch increase in height, we predict a person's weight increases by 1.28 pounds**.\n\n*R* allows us to do several things with a model. We can use *R* to give us a confidence interval on our coefficients using `confint`:\n\n```{r}\nconfint(df.lm)\n```\n\n::: callout-note\nThis is more precise than the Z estimation shown above as it accounts for our sample size and uses a t test.\n:::\n\nWe can also predict results using `predict` Predict can either give you a point estimate, or an interval based on either the mean predictions (using `'confidence'`) or a single point (using `'prediction'`). You can read more about these options \\[here\\](http://www.sthda.com/english/articles/40-regression-analysis/166-predict-in-r-model-predictions-and-confidence-intervals/).\n\n```{r}\npredict(df.lm, list(height = 66))\npredict(df.lm, list(height = 66), interval = 'confidence')\npredict(df.lm, list(height = 66), interval = 'prediction')\n```\n\nNote, *R* will not prevent one from extrapolating beyond the data. Predicting a result on values outside the observed data is bad practice and should generally be avoided.\n\nFinally, one can plot the results and a regression quite simply with `ggplot()` using `stat_smooth()`:\n\n```{r}\nggplot(data = df, aes(x = height, y = weight)) + # Provide your data and aesthetics as usual\n  geom_point() +  # plot the observations\n  # geom_smooth creates a linear regression based on your given x and y values (i.e. lm(y~x))\n  # you can also plot the standard error\n  geom_smooth(method = 'lm', se = T, formula = \"y ~ x\") \n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"paged","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"slr.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.280","editor":"visual","theme":{"dark":"darkly","light":"flatly"},"title":"Simple Linear Regression","author":"John King","date":"5/20/2020","code-copy":true},"extensions":{"book":{"multiFile":true}}}}}