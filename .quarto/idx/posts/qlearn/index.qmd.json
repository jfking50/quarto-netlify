{"title":"Q Learning","markdown":{"yaml":{"title":"Q Learning","description":"Train an agent to play tic-tac-toe without using a neural network.","author":"John King","date":"4/5/2020","format":{"html":{"toc":true,"code-fold":false,"code-tools":true,"code-copy":true,"df-print":"paged"}},"execute":{"warning":false},"categories":["python","reinforcement learning"],"image":"ttt.png"},"headingText":"States and Actions","containsRefs":false,"markdown":"\n\nI've been reading some books on machine learning, and recently started going through [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646) by Aurélien Géron. His chapter on reinforcement learning is great, and it inspired me to apply some of the methods on my own. I decided to start with the game of tic-tac-toe since it's a little more complicated than a trivial simple example but not as complicated as chess or go.\n\n\nQ-Learning is a type of reinforcement learning that can be applied to situations where there are a discrete number of states and actions, but the transition probabilities between states are unknown. In the game of tic-tac-toe, the state is the game board, which consists of nine possible locations to play that are either empty, contain an X, or contain a O. I will represent the state as an array of length nine where 0 represents an empty space, 1 represents an X, and -1 represents an O. I'm defining things this way because eventually I want to apply deep Q learning to the same problem, and these will become inputs to a neural network. Anyway, at the beginning of the game, the board is empty, so it will be represented as:\n\n```{python}\nstate = [0 for i in range(9)]\nstate\n```\n\nAt the beginning of the game, there are nine possible actions for the first player (in this case, player X). An action of 0 represents X playing in the upper left board position, an action of 1 represents the upper middle position, etc. If X plays action 0, and O plays action 3, the state becomes:\n\n```{python}\nstate[0] = 1   # X plays action 0\nstate[3] = -1  # O plays action 3\nstate\n```\n\n## Rewards\n\nIn all reinforcement learning techniques, a reward system is used. To develop a game playing policy for X, if an action results in X winning the game, the reward will be 1. If O wins, the reward will be -1. All other actions will have a reward of 0.\n\n## Q-Values\n\nIn Q-Learning, state-action pairs *(s, a)* have an associated quality value $Q$ that are used to determine what the best move is for that state (the higher the Q-value, the better the move). When the algorithm begins and the first game is played, all Q-values are zero, so the AI (player X) is essentially playing randomly. Once the first game is played that is either a win or a loss, the first non-zero reward is given, and so the first non-zero Q-value is produced for the state-action pair that resulted in that reward. As the algorithm explores the state space, non-zero Q-values propagate back from the state-action pairs that won or lost the game to the state-action pairs that led up to the winning or losing move. In other words, the Q-values are slowly updated based on a running average of the reward $r$ received when leaving a state $s$ with an action $a$ plus the sum of discounted future rewards it expects to get assuming optimal play from that point on and adjusted by a learning rate $\\alpha$. To estimate the sum of the discounted future rewards, I take the maximum of the Q-value estimates for the next state $s′$. That was a mouthful. Maybe in this case, math might be clearer than English:\n\n$$\nQ\\left(s, a\\right) \\xleftarrow[\\alpha ]{} r \\gamma\\cdot \\underset{a'}{max}Q\\left(s', a'\\right)\n$$\n\nThat's it. It ends up just being a big book-keeping exercise to keep track of a whole bunch of state-action pairs and their associated Q-values. To do that, I use a dictionary data structure with the state-action pair as the key and the Q-value as the value.\n\n## Epsilon-Greedy Policy\n\nI added an epsilon-greedy policy to encourage the algorithm to continue to explore the state-space throughout the training process. I implemented it in the following way. Before each action by X, I draw a random number and compare it to a variable represented by epsilon. If the random number is less than epsilon, then player X's next action is random. Otherwise, player X's next action is determined by the Q-values. When the algorithm starts, epsilon is 1.0. As training progresses, epsilon gradually decreases to 0.01.\n\n## The Tic-Tac-Toe Environment\n\nI set this all up in a{python} class called `env` with the following functions:\n\n-   `reset` sets the state to represent an empty board (the start of a new game)\n-   `game_over` checks to see if the game is over, and if so, gives a reward for a win or loss\n-   `step` represents a single action either by player X or player O\n-   `train_q` implements the Q-learning algorithm\n\nAfter importing the usual suspects, I define the `env` class.\n\n```{python}\nimport copy\nimport random\nimport plotly.express as px\n```\n\n```{python}\nclass env:\n\n    def __init__(self):\n        self.state = self.reset()\n        self.alpha0 = 0.05           # learning rate\n        self.decay = 0.005           # learning rate decay\n        self.gamma = 0.95            # discount factor\n        self.Qvalues = {}\n\n    def reset(self):\n        return [0 for i in range(9)]\n\n    def game_over(self, s):\n        done = False\n        reward = 0\n        if (s[0] + s[1] + s[3]  == 3 or s[3] + s[4] + s[5]  == 3 or s[6] + s[7] + s[8]  == 3 or\n            s[0] + s[3] + s[6]  == 3 or s[1] + s[4] + s[7]  == 3 or s[2] + s[5] + s[8]  == 3 or\n            s[0] + s[4] + s[8]  == 3 or s[2] + s[4] + s[6]  == 3):\n            done = True\n            reward = 1\n        if (s[0] + s[1] + s[3]  == -3 or s[3] + s[4] + s[5]  == -3 or s[6] + s[7] + s[8]  == -3 or\n            s[0] + s[3] + s[6]  == -3 or s[1] + s[4] + s[7]  == -3 or s[2] + s[5] + s[8]  == -3 or\n            s[0] + s[4] + s[8]  == -3 or s[2] + s[4] + s[6]  == -3):\n            done = True\n            reward = -1\n        if sum(1 for i in s if i != 0)==9 and not done:\n            done = True\n        return done, reward\n\n    def step(self, state, action, player):\n        next_state = state.copy()\n        if player == 0: next_state[action] = 1\n        else: next_state[action] = -1\n        done, reward = self.game_over(next_state)\n        return next_state, done, reward\n\n    def train_q(self, iterations):\n        for iteration in range(iterations): # loop to play a bunch of games\n            state = self.reset()\n            next_state = state.copy()\n            done = False\n            epsilon = max(1 - iteration/(iterations*0.8), 0.01)\n            while not done: # loop to play one game\n                if random.random() < epsilon:  # epsilon greedy policy for player X\n                    action = random.choice([i for i in range(len(state)) if state[i] == 0])\n                else:\n                    xq = [self.Qvalues.get((tuple(state), i)) for i in range(9) if self.Qvalues.get((tuple(state), i)) is not None]\n                    if len(xq) == 0: action = random.choice([i for i in range(len(state)) if state[i] == 0])\n                    else:\n                        idx = [i for i in range(9) if self.Qvalues.get((tuple(state), i)) is not None]\n                        action = idx[xq.index(max(xq))]\n                next_state, done, reward = self.step(state, action, 0)\n                if not done: # random policy for player O\n                    omove = random.choice([i for i in range(len(next_state)) if next_state[i] == 0])\n                    next_state, done, reward = self.step(next_state, omove, 1)\n                if not done:\n                    key = (tuple(state), action)\n                    if key not in self.Qvalues:\n                        self.Qvalues[key] = reward\n                    next_idx = [i for i in range(9) if self.Qvalues.get((tuple(next_state), i)) is not None]\n                    if len(next_idx) > 0:\n                        next_value = max([self.Qvalues.get((tuple(next_state), i)) for i in next_idx])\n                    else: next_value = 0\n                else: next_value = reward\n                # now update the Q-value for the state-action pair\n                alpha = self.alpha0 / (1 + iteration * self.decay)\n                self.Qvalues[key] *= 1 - alpha\n                self.Qvalues[key] += alpha * (reward + self.gamma * next_value)\n                state = next_state.copy()\n        return self.Qvalues\n```\n\n## Results\n\nThe `play_v_random` function pits the AI-enabled player against an opponent that plays randomly. It takes the number of games to be played as an argument and returns the number of X wins, O wins, and ties.\n\n```{python}\ndef play_v_random (games):\n    results = [0 for i in range(games)]\n    for i in range(games):\n        state = ttt.reset()\n        next_state = state.copy()\n        done = False\n        while not done:\n            xq = [Q_values.get((tuple(state), i)) for i in range(9) if Q_values.get((tuple(state), i)) is not None]\n            if len(xq) == 0:\n                action = random.choice([i for i in range(len(state)) if state[i] == 0])\n            else:\n                idx = [i for i in range(9) if Q_values.get((tuple(state), i)) is not None]\n                action = idx[xq.index(max(xq))]\n            next_state, done, reward = ttt.step(state, action, 0)\n            if not done:\n                omove = random.choice([i for i in range(len(next_state)) if next_state[i] == 0])\n                next_state, done, reward = ttt.step(next_state, omove, 1)\n            state = next_state.copy()\n        results[i] = reward\n    return results\n```\n\nSo here we go. I'll have X play O before any Q-learning, update the Q-values based on 20,000 games, have X play O again, update Q-values some more, and so on for 10 iterations. By the end, the algorithm will have seen 200,000 games. Recall that the output is X wins, O wins, and ties and that X plays O 1,000 times, so move the decimal to the left once for the percent of wins.\n\n```{python}\nttt = env()\nfor i in range(101):\n    Q_values = ttt.train_q(2000)\n    if i % 10 == 0:\n        results = play_v_random(1000)\n        print(sum(1 for i in results if i == 1), sum(1 for i in results if i == -1), sum(1 for i in results if i == 0))\n```\n\nSeems to be working nicely! X, the AI-enabled player, definitely improves over time. This shows all of the non-zero Q-values at the end of training. There are a little over 6,500 of them and most are positive, which suggests to me that the algorithm capitalizing on what it's learned and back propagating Q-values. Since there are more positive Q-values than negative, it's seeking out those positive rewards like it's supposed to.\n\n```{python}\n# fig.height: 3\nx = [i for i in range(len(Q_values))]\nq = list(Q_values.values())\nq.sort()\n\nfig = px.scatter(x=x, y=q)\nfig.update_layout(xaxis_title='X', yaxis_title='Q Value')\nfig.show()\n```\n\nAnyone who has played tic-tac-toe more than a few times learns that the best opening move is to play in the center of the board. I was curious of the Q-leaning algorithm picked this up, so I looked up the Q-values for an empty board. The fifth value listed represents the center of the board, and it is in fact the greatest Q-value. Pretty cool!\n\n```{python}\n[Q_values.get(((0,0,0,0,0,0,0,0,0), i)) for i in range(9)]\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"paged","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.280","editor":"visual","theme":{"dark":"darkly","light":"flatly"},"title":"Q Learning","description":"Train an agent to play tic-tac-toe without using a neural network.","author":"John King","date":"4/5/2020","categories":["python","reinforcement learning"],"image":"ttt.png","code-copy":true},"extensions":{"book":{"multiFile":true}}}}}