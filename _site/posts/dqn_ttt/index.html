<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="John King">
<meta name="dcterms.date" content="2020-04-10">
<meta name="description" content="Train an AI agent to play tic-tac-toe.">

<title>A Random Walk - Deep Q-Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">A Random Walk</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-tutorials" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Tutorials</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-tutorials">    
        <li>
    <a class="dropdown-item" href="../../tutorial/slr.html">
 <span class="dropdown-text">Simple Linear Regression</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../tutorial/lm_assumptions.html">
 <span class="dropdown-text">Linear Model Assumptions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../tutorial/mlr.html">
 <span class="dropdown-text">Multiple Linear Regression</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../tutorial/model_selection.html">
 <span class="dropdown-text">Model Selection</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../tutorial/transform.html">
 <span class="dropdown-text">Variable Transformation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../tutorial/logistic.html">
 <span class="dropdown-text">Logistic Regression</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../tutorial/advanced.html">
 <span class="dropdown-text">Advanced Designs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../tutorial/np_anova.html">
 <span class="dropdown-text">Nonparametric ANOVA</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../tutorial/gam.html">
 <span class="dropdown-text">Generalized Additive Models</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../tutorial/svm.html">
 <span class="dropdown-text">Support Vector Machines</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../tutorial/random_forest.html">
 <span class="dropdown-text">Random Forests</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../tutorial/nn_regression.html">
 <span class="dropdown-text">Neural Network Regression</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-presentations" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Presentations</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-presentations">    
        <li>
    <a class="dropdown-item" href="../../presentations/doe.qmd">
 <span class="dropdown-text">Design of Experiments</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/jfking50"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#tic-tac-toe-environment" id="toc-tic-tac-toe-environment" class="nav-link active" data-scroll-target="#tic-tac-toe-environment">Tic-Tac-Toe Environment</a></li>
  <li><a href="#deep-q-learning" id="toc-deep-q-learning" class="nav-link" data-scroll-target="#deep-q-learning">Deep Q-Learning</a></li>
  <li><a href="#training" id="toc-training" class="nav-link" data-scroll-target="#training">Training</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Deep Q-Networks</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
  <div class="quarto-categories">
    <div class="quarto-category">python</div>
    <div class="quarto-category">reinforcement learning</div>
    <div class="quarto-category">AI</div>
    <div class="quarto-category">tensorflow</div>
    <div class="quarto-category">neural network</div>
  </div>
  </div>

<div>
  <div class="description">
    Train an AI agent to play tic-tac-toe.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>John King </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 10, 2020</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>In a <a href="https://jfking.netlify.app/blog/qlearn">previous post</a> on Q-learning, I demonstrated how to train an agent to play tic-tac-toe. One of the challenges with Q-learning is that it doesn’t scale well to environments with a large state space. Even after experiencing 200,000 tic-tac-toe games, the agent had visited only a fraction of the total state space. After training and when playing against an opponent, if the agent encountered a board that it didn’t experience in training, the Q-values for all available moves were 0, and so the agent played randomly. This becomes even more problematic if we try to apply Q-learning to a game like chess or Go. We need a method that doesn’t rely on a training agent visiting every state. Neural networks have been very successful in solving this problem. <a href="https://deepmind.com/about#our_story">DeepMind</a>, for example, has not only mastered 49 different Atari games, but has also defeated the raining Go champion. I’m not that ambitious. I’ll just stick with tic-tac-toe. The most basic application of neural networks to this problem is a <strong>deep Q-Network</strong>, which I’ll demonstrate here.</p>
<section id="tic-tac-toe-environment" class="level2">
<h2 class="anchored" data-anchor-id="tic-tac-toe-environment">Tic-Tac-Toe Environment</h2>
<p>As with Q-learning, I needed to define an environment that contains the game fundamentals. I started with the same environment I set up for Q-learning and then stripped out the Q-learning algorithm so I just have the basics of the game. The <code>env</code> class below does just three things: 1. It resets the game board to begin a new game. I define the board state as a list with nine elements to represent each of the nine game board positions. A 0 represents an empty position, 1 is an X, and -1 is an O. 2. In the <code>step</code> function, it makes a move for either player X or O. 3. In the <code>game_over</code> function, it checks the state of the board and returns a reward. The reward system is the same as before. An X win gets a reward of 1, an O win gets a reward of -1, and everything else gets a reward of 0.</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> copy</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> env:</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.state <span class="op">=</span> <span class="va">self</span>.reset()</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset(<span class="va">self</span>):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [<span class="dv">0</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">9</span>)]</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> game_over(<span class="va">self</span>, s):</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (s[<span class="dv">0</span>] <span class="op">+</span> s[<span class="dv">1</span>] <span class="op">+</span> s[<span class="dv">3</span>]  <span class="op">==</span> <span class="dv">3</span> <span class="kw">or</span> s[<span class="dv">3</span>] <span class="op">+</span> s[<span class="dv">4</span>] <span class="op">+</span> s[<span class="dv">5</span>]  <span class="op">==</span> <span class="dv">3</span> <span class="kw">or</span> s[<span class="dv">6</span>] <span class="op">+</span> s[<span class="dv">7</span>] <span class="op">+</span> s[<span class="dv">8</span>]  <span class="op">==</span> <span class="dv">3</span> <span class="kw">or</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>            s[<span class="dv">0</span>] <span class="op">+</span> s[<span class="dv">3</span>] <span class="op">+</span> s[<span class="dv">6</span>]  <span class="op">==</span> <span class="dv">3</span> <span class="kw">or</span> s[<span class="dv">1</span>] <span class="op">+</span> s[<span class="dv">4</span>] <span class="op">+</span> s[<span class="dv">7</span>]  <span class="op">==</span> <span class="dv">3</span> <span class="kw">or</span> s[<span class="dv">2</span>] <span class="op">+</span> s[<span class="dv">5</span>] <span class="op">+</span> s[<span class="dv">8</span>]  <span class="op">==</span> <span class="dv">3</span> <span class="kw">or</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>            s[<span class="dv">0</span>] <span class="op">+</span> s[<span class="dv">4</span>] <span class="op">+</span> s[<span class="dv">8</span>]  <span class="op">==</span> <span class="dv">3</span> <span class="kw">or</span> s[<span class="dv">2</span>] <span class="op">+</span> s[<span class="dv">4</span>] <span class="op">+</span> s[<span class="dv">6</span>]  <span class="op">==</span> <span class="dv">3</span>):</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>            done <span class="op">=</span> <span class="va">True</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>            reward <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (s[<span class="dv">0</span>] <span class="op">+</span> s[<span class="dv">1</span>] <span class="op">+</span> s[<span class="dv">3</span>]  <span class="op">==</span> <span class="op">-</span><span class="dv">3</span> <span class="kw">or</span> s[<span class="dv">3</span>] <span class="op">+</span> s[<span class="dv">4</span>] <span class="op">+</span> s[<span class="dv">5</span>]  <span class="op">==</span> <span class="op">-</span><span class="dv">3</span> <span class="kw">or</span> s[<span class="dv">6</span>] <span class="op">+</span> s[<span class="dv">7</span>] <span class="op">+</span> s[<span class="dv">8</span>]  <span class="op">==</span> <span class="op">-</span><span class="dv">3</span> <span class="kw">or</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>            s[<span class="dv">0</span>] <span class="op">+</span> s[<span class="dv">3</span>] <span class="op">+</span> s[<span class="dv">6</span>]  <span class="op">==</span> <span class="op">-</span><span class="dv">3</span> <span class="kw">or</span> s[<span class="dv">1</span>] <span class="op">+</span> s[<span class="dv">4</span>] <span class="op">+</span> s[<span class="dv">7</span>]  <span class="op">==</span> <span class="op">-</span><span class="dv">3</span> <span class="kw">or</span> s[<span class="dv">2</span>] <span class="op">+</span> s[<span class="dv">5</span>] <span class="op">+</span> s[<span class="dv">8</span>]  <span class="op">==</span> <span class="op">-</span><span class="dv">3</span> <span class="kw">or</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>            s[<span class="dv">0</span>] <span class="op">+</span> s[<span class="dv">4</span>] <span class="op">+</span> s[<span class="dv">8</span>]  <span class="op">==</span> <span class="op">-</span><span class="dv">3</span> <span class="kw">or</span> s[<span class="dv">2</span>] <span class="op">+</span> s[<span class="dv">4</span>] <span class="op">+</span> s[<span class="dv">6</span>]  <span class="op">==</span> <span class="op">-</span><span class="dv">3</span>):</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>            done <span class="op">=</span> <span class="va">True</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>            reward <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">sum</span>(<span class="dv">1</span> <span class="cf">for</span> i <span class="kw">in</span> s <span class="cf">if</span> i <span class="op">!=</span> <span class="dv">0</span>)<span class="op">==</span><span class="dv">9</span> <span class="kw">and</span> <span class="kw">not</span> done:</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>            done <span class="op">=</span> <span class="va">True</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> done, reward</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>, state, action, player):</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        next_state <span class="op">=</span> state.copy()</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> player <span class="op">==</span> <span class="dv">0</span>: next_state[action] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>: next_state[action] <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        done, reward <span class="op">=</span> <span class="va">self</span>.game_over(next_state)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> next_state, done, reward</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="deep-q-learning" class="level2">
<h2 class="anchored" data-anchor-id="deep-q-learning">Deep Q-Learning</h2>
<p>Next, I create a class <code>DQNagent</code> where the real work is done. I’ll go through the three main functions in detail.</p>
<p><strong><code>build_model</code> Function.</strong> This is where I create the neural network itself. It’s a simple sequential model with two hidden layers that I built some flexibility into. The input shape, the number of nodes, and the output size are all variables because I intend to apply this basic set-up to other problems. In the tic-tac-toe problem, the neural network takes the board state as input, so in this case the <code>input_shape</code> is nine. The two hidden layers have 81 nodes each with the <code>relu</code> activation function. The output also has a length of nine that are the Q-values for each of the nine actions (i.e., moves on the game board). In other words, the network takes the game board as input, and provides the best move as output.</p>
<p><strong><code>play_ttt</code> Function.</strong> It might make more sense to explain this function next. Basically, there’s an inner loop to play one game of tic-tac-toe, and an outer loop to play the game multiple times. Most of the function just controls the mechanics of the game, but there are two aspects I want to point out. 1. The function uses an <em>epsilon-greedy policy</em> to determine whether to make a move based on the neural network Q-values or make a random move. The probability of playing randomly is initially high during training to encourage the algorithm to explore the state space. As training progresses, player X moves are more and more likely to be based on the neural network. Note that the neural network is set up to learn the game from player X’s perspective. Player O always plays randomly. 2. During the course of playing one game, the game board states, player moves (actions), and rewards are recorded. These are passed to the <code>train_model</code> function at the end of a game.</p>
<p><strong><code>train_model</code> Function.</strong> This is where the neural network gets trained and things get complicated. I’m going to let Aurélien Géron do the ’splainin from his book <a href="https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646">Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</a>.</p>
<blockquote class="blockquote">
<p>Consider the approximate Q-Value computed by the DQN for a given state-action pair (s, a). Thanks to Bellman, we know we want this approximate Q-Value to be as close as possible to the reward r that we actually observe after playing action a in state s, plus the discounted value of playing optimally from then on. To estimate this sum of future discounted rewards, we can simply execute the DQN on the next state s′ and for all possible actions a′. We get an approximate future Q-Value for each possible action. We then pick the highest (since we assume we will be playing optimally) and discount it, and this gives us an estimate of the sum of future discounted rewards. By summing the reward r and the future discounted value estimate, we get a target Q-Value y(s, a) for the state-action pair (s, a), as shown in the equation:</p>
</blockquote>
<p><span class="math display">\[
Q_{target}(s,a)=r\gamma\cdot \underset{a'}{max}Q_{\theta}\left(s', a'\right)
\]</span></p>
<blockquote class="blockquote">
<p>With this target Q-Value, we can run a training step using any Gradient Descent algorithm. Specifically, we generally try to minimize the squared error between the estimated Q-Value Q(s, a) and the target Q-Value (or the Huber loss to reduce the algorithm’s sensitivity to large errors). And that’s all for the basic Deep Q-Learning algorithm!</p>
</blockquote>
<p>Right. So that’s what happening in the <code>train_model</code> function.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DQNagent:</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, state_size, action_size, iterations):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha0 <span class="op">=</span> <span class="fl">0.05</span>           <span class="co"># learning rate</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decay <span class="op">=</span> <span class="fl">0.005</span>           <span class="co"># learning rate decay</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gamma <span class="op">=</span> <span class="fl">0.95</span>            <span class="co"># discount factor</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.state_size <span class="op">=</span> state_size</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.action_size <span class="op">=</span> action_size</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.iterations <span class="op">=</span> iterations</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> <span class="va">self</span>.build_model()</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.optimizer <span class="op">=</span> tf.keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">0.02</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.loss_fn <span class="op">=</span> tf.keras.losses.mean_squared_error</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build_model(<span class="va">self</span>):</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> tf.keras.models.Sequential([</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>            tf.keras.layers.Dense(<span class="va">self</span>.state_size<span class="op">**</span><span class="dv">2</span>, activation<span class="op">=</span><span class="st">"relu"</span>, input_shape<span class="op">=</span>[<span class="va">self</span>.state_size]),</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>            tf.keras.layers.Dense(<span class="va">self</span>.state_size<span class="op">**</span><span class="dv">2</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>            tf.keras.layers.Dense(<span class="va">self</span>.action_size)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> model</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train_model(<span class="va">self</span>, state_history, action_history, next_state_history, rewards, dones):</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        next_Q_values <span class="op">=</span> <span class="va">self</span>.model.predict(np.array(next_state_history), verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        max_next_Q_values <span class="op">=</span> np.<span class="bu">max</span>(next_Q_values, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        target_Q_values <span class="op">=</span> rewards <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> <span class="dv">1</span><span class="op">*</span>np.array(dones)) <span class="op">*</span> <span class="va">self</span>.gamma <span class="op">*</span> max_next_Q_values</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        target_Q_values <span class="op">=</span> tf.reshape(target_Q_values, [<span class="bu">len</span>(rewards), <span class="dv">1</span>])</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> tf.one_hot(action_history, <span class="dv">9</span>)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> tf.GradientTape() <span class="im">as</span> tape:</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>            all_Q_values <span class="op">=</span> <span class="va">self</span>.model(np.array(state_history))</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>            Q_values <span class="op">=</span> tf.reduce_sum(all_Q_values <span class="op">*</span> mask, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> tf.reduce_mean(<span class="va">self</span>.loss_fn(target_Q_values, Q_values))</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        grads <span class="op">=</span> tape.gradient(loss, <span class="va">self</span>.model.trainable_variables)</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.optimizer.apply_gradients(<span class="bu">zip</span>(grads, <span class="va">self</span>.model.trainable_variables))</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> play_ttt(<span class="va">self</span>):</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.iterations):    <span class="co"># outer loop to play the game a bunch of times</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>            state <span class="op">=</span> env().reset()</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>            next_state <span class="op">=</span> state.copy()</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>            done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>            dones <span class="op">=</span> []</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>            state_history <span class="op">=</span> []</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>            state_history.append(state)</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>            action_history <span class="op">=</span> []</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>            rewards <span class="op">=</span> []</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>            epsilon <span class="op">=</span> <span class="bu">max</span>(<span class="dv">1</span> <span class="op">-</span> iteration<span class="op">/</span>(<span class="va">self</span>.iterations<span class="op">*</span><span class="fl">0.8</span>), <span class="fl">0.01</span>)</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>            <span class="cf">while</span> <span class="kw">not</span> done:                          <span class="co"># inner loop to play one game</span></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> random.random() <span class="op">&lt;</span> epsilon:        <span class="co"># epsilon-greedy policy</span></span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>                    action <span class="op">=</span> random.choice([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(state)) <span class="cf">if</span> state[i] <span class="op">==</span> <span class="dv">0</span>])</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>                    action <span class="op">=</span> np.argmax(<span class="va">self</span>.model.predict(np.array(state)[np.newaxis], verbose<span class="op">=</span><span class="dv">0</span>)[<span class="dv">0</span>])</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>                action_history.append(action)</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>                next_state, done, reward <span class="op">=</span> env().step(state, action, <span class="dv">0</span>)</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> done:</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>                    state_history.append(next_state)</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>                    dones.append(done)</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>                    rewards.append(reward)</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="kw">not</span> done:</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>                    omove <span class="op">=</span> random.choice([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(next_state)) <span class="cf">if</span> next_state[i] <span class="op">==</span> <span class="dv">0</span>])</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>                    next_state, done, reward <span class="op">=</span> env().step(next_state, omove, <span class="dv">1</span>)</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>                    state <span class="op">=</span> next_state.copy()</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>                    state_history.append(next_state)</span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>                    dones.append(done)</span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>                    rewards.append(reward)</span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a>            next_state_history <span class="op">=</span> state_history[<span class="dv">1</span>:<span class="bu">len</span>(state_history)]</span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>            state_history <span class="op">=</span> state_history[<span class="dv">0</span>:<span class="bu">len</span>(action_history)]</span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.train_model(state_history, action_history, next_state_history, rewards, dones)</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="training" class="level2">
<h2 class="anchored" data-anchor-id="training">Training</h2>
<p>Here I just provide the <code>DQNagent</code> with the number of input and output nodes for the neural network (9 each) and how many games to play (1000). Compare the number of games here with the 200,000 games I used for the Q-learning method - a huge difference!</p>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<p>I want to see real quick if the neural net has learned to play in the center of the board on the opening move, so I have the model give me the Q-values for an empty board. The fifth number returned represents the center of the board, and it is the highest Q-value, so the neural net has already learned it!</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>m.predict(np.zeros(<span class="dv">9</span>)[np.newaxis], verbose<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>array([[0.5399946 , 0.45416299, 0.60292625, 0.87839925, 0.7457614 ,
        0.5695118 , 0.6507269 , 0.49802697, 0.5923823 ]], dtype=float32)</code></pre>
</div>
</div>
<p>Next, the <code>play_v_random</code> function pits the AI-enabled player against an opponent that plays randomly.</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> play_v_random (games):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> [<span class="dv">0</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(games)]</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(games):</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        board <span class="op">=</span> env().reset()</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print(board[0:3])</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print(board[3:6])</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print(board[6:9])</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>            xmoves <span class="op">=</span> m.predict(np.array(board)[np.newaxis], verbose<span class="op">=</span><span class="dv">0</span>)[<span class="dv">0</span>]</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>            xmoves[np.where(np.array(board)<span class="op">!=</span><span class="dv">0</span>)[<span class="dv">0</span>]] <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>            xmove <span class="op">=</span> np.argmax(xmoves)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print("move", xmove)</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>            board[xmove] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>            done, reward <span class="op">=</span> env().game_over(board)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> done:</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>                omove <span class="op">=</span> random.choice([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(board)) <span class="cf">if</span> board[i] <span class="op">==</span> <span class="dv">0</span>])</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>                board[omove] <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>                done, reward <span class="op">=</span> env().game_over(board)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print(board[0:3])</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print(board[3:6])</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print(board[6:9])</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>        results[i] <span class="op">=</span> reward</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>So here we have it. The AI-enabled player won 90% of the games, lost 8%, and 2% were ties. For comparison, the best performance using Q-learning was 81.2% AI wins, 2.3% losses, and 16.5% ties. The deep Q-network performed significantly better after being trained on 1,000 games than Q-learning did after being trained on 200,000 games. Remarkable!</p>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="bu">sum</span>(<span class="dv">1</span> <span class="cf">for</span> i <span class="kw">in</span> results <span class="cf">if</span> i <span class="op">==</span> <span class="dv">1</span>), <span class="bu">sum</span>(<span class="dv">1</span> <span class="cf">for</span> i <span class="kw">in</span> results <span class="cf">if</span> i <span class="op">==</span> <span class="op">-</span><span class="dv">1</span>), <span class="bu">sum</span>(<span class="dv">1</span> <span class="cf">for</span> i <span class="kw">in</span> results <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>(903, 44, 53)</code></pre>
</div>
</div>
<p>As I mentioned at the beginning of the post, a deep Q-network is the most basic application of neural networks to reinforcement learning. More advanced applications include double deep Q-networks and dueling deep Q networks, and each of these can be enhanced with techniques like actor-critic algorithms, curiosity-based exploration, proximal policy optimization, and so on. There’s a lot left to explore.</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'alternate';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb9" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Deep Q-Networks"</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "Train an AI agent to play tic-tac-toe."</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "John King"</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "4/10/2020"</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co">    code-copy: true</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co">    df-print: paged</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span><span class="co"> </span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co">  warning: false</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co">  - python</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co">  - reinforcement learning</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="co">  - AI</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="co">  - tensorflow</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="co">  - neural network</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "ttt.png"</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>In a <span class="co">[</span><span class="ot">previous post</span><span class="co">](https://jfking.netlify.app/blog/qlearn)</span> on Q-learning, I demonstrated how to train an agent to play tic-tac-toe. One of the challenges with Q-learning is that it doesn't scale well to environments with a large state space. Even after experiencing 200,000 tic-tac-toe games, the agent had visited only a fraction of the total state space. After training and when playing against an opponent, if the agent encountered a board that it didn't experience in training, the Q-values for all available moves were 0, and so the agent played randomly. This becomes even more problematic if we try to apply Q-learning to a game like chess or Go. We need a method that doesn't rely on a training agent visiting every state. Neural networks have been very successful in solving this problem. <span class="co">[</span><span class="ot">DeepMind</span><span class="co">](https://deepmind.com/about#our_story)</span>, for example, has not only mastered 49 different Atari games, but has also defeated the raining Go champion. I'm not that ambitious. I'll just stick with tic-tac-toe. The most basic application of neural networks to this problem is a **deep Q-Network**, which I'll demonstrate here.</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## Tic-Tac-Toe Environment</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>As with Q-learning, I needed to define an environment that contains the game fundamentals. I started with the same environment I set up for Q-learning and then stripped out the Q-learning algorithm so I just have the basics of the game. The <span class="in">`env`</span> class below does just three things: 1. It resets the game board to begin a new game. I define the board state as a list with nine elements to represent each of the nine game board positions. A 0 represents an empty position, 1 is an X, and -1 is an O. 2. In the <span class="in">`step`</span> function, it makes a move for either player X or O. 3. In the <span class="in">`game_over`</span> function, it checks the state of the board and returns a reward. The reward system is the same as before. An X win gets a reward of 1, an O win gets a reward of -1, and everything else gets a reward of 0.</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> copy</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> env:</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.state <span class="op">=</span> <span class="va">self</span>.reset()</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset(<span class="va">self</span>):</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [<span class="dv">0</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">9</span>)]</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> game_over(<span class="va">self</span>, s):</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>        done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (s[<span class="dv">0</span>] <span class="op">+</span> s[<span class="dv">1</span>] <span class="op">+</span> s[<span class="dv">3</span>]  <span class="op">==</span> <span class="dv">3</span> <span class="kw">or</span> s[<span class="dv">3</span>] <span class="op">+</span> s[<span class="dv">4</span>] <span class="op">+</span> s[<span class="dv">5</span>]  <span class="op">==</span> <span class="dv">3</span> <span class="kw">or</span> s[<span class="dv">6</span>] <span class="op">+</span> s[<span class="dv">7</span>] <span class="op">+</span> s[<span class="dv">8</span>]  <span class="op">==</span> <span class="dv">3</span> <span class="kw">or</span></span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>            s[<span class="dv">0</span>] <span class="op">+</span> s[<span class="dv">3</span>] <span class="op">+</span> s[<span class="dv">6</span>]  <span class="op">==</span> <span class="dv">3</span> <span class="kw">or</span> s[<span class="dv">1</span>] <span class="op">+</span> s[<span class="dv">4</span>] <span class="op">+</span> s[<span class="dv">7</span>]  <span class="op">==</span> <span class="dv">3</span> <span class="kw">or</span> s[<span class="dv">2</span>] <span class="op">+</span> s[<span class="dv">5</span>] <span class="op">+</span> s[<span class="dv">8</span>]  <span class="op">==</span> <span class="dv">3</span> <span class="kw">or</span></span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a>            s[<span class="dv">0</span>] <span class="op">+</span> s[<span class="dv">4</span>] <span class="op">+</span> s[<span class="dv">8</span>]  <span class="op">==</span> <span class="dv">3</span> <span class="kw">or</span> s[<span class="dv">2</span>] <span class="op">+</span> s[<span class="dv">4</span>] <span class="op">+</span> s[<span class="dv">6</span>]  <span class="op">==</span> <span class="dv">3</span>):</span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>            done <span class="op">=</span> <span class="va">True</span></span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a>            reward <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (s[<span class="dv">0</span>] <span class="op">+</span> s[<span class="dv">1</span>] <span class="op">+</span> s[<span class="dv">3</span>]  <span class="op">==</span> <span class="op">-</span><span class="dv">3</span> <span class="kw">or</span> s[<span class="dv">3</span>] <span class="op">+</span> s[<span class="dv">4</span>] <span class="op">+</span> s[<span class="dv">5</span>]  <span class="op">==</span> <span class="op">-</span><span class="dv">3</span> <span class="kw">or</span> s[<span class="dv">6</span>] <span class="op">+</span> s[<span class="dv">7</span>] <span class="op">+</span> s[<span class="dv">8</span>]  <span class="op">==</span> <span class="op">-</span><span class="dv">3</span> <span class="kw">or</span></span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a>            s[<span class="dv">0</span>] <span class="op">+</span> s[<span class="dv">3</span>] <span class="op">+</span> s[<span class="dv">6</span>]  <span class="op">==</span> <span class="op">-</span><span class="dv">3</span> <span class="kw">or</span> s[<span class="dv">1</span>] <span class="op">+</span> s[<span class="dv">4</span>] <span class="op">+</span> s[<span class="dv">7</span>]  <span class="op">==</span> <span class="op">-</span><span class="dv">3</span> <span class="kw">or</span> s[<span class="dv">2</span>] <span class="op">+</span> s[<span class="dv">5</span>] <span class="op">+</span> s[<span class="dv">8</span>]  <span class="op">==</span> <span class="op">-</span><span class="dv">3</span> <span class="kw">or</span></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a>            s[<span class="dv">0</span>] <span class="op">+</span> s[<span class="dv">4</span>] <span class="op">+</span> s[<span class="dv">8</span>]  <span class="op">==</span> <span class="op">-</span><span class="dv">3</span> <span class="kw">or</span> s[<span class="dv">2</span>] <span class="op">+</span> s[<span class="dv">4</span>] <span class="op">+</span> s[<span class="dv">6</span>]  <span class="op">==</span> <span class="op">-</span><span class="dv">3</span>):</span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a>            done <span class="op">=</span> <span class="va">True</span></span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a>            reward <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">sum</span>(<span class="dv">1</span> <span class="cf">for</span> i <span class="kw">in</span> s <span class="cf">if</span> i <span class="op">!=</span> <span class="dv">0</span>)<span class="op">==</span><span class="dv">9</span> <span class="kw">and</span> <span class="kw">not</span> done:</span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a>            done <span class="op">=</span> <span class="va">True</span></span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> done, reward</span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>, state, action, player):</span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a>        next_state <span class="op">=</span> state.copy()</span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> player <span class="op">==</span> <span class="dv">0</span>: next_state[action] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>: next_state[action] <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a>        done, reward <span class="op">=</span> <span class="va">self</span>.game_over(next_state)</span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> next_state, done, reward</span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true" tabindex="-1"></a><span class="fu">## Deep Q-Learning</span></span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true" tabindex="-1"></a>Next, I create a class <span class="in">`DQNagent`</span> where the real work is done. I'll go through the three main functions in detail.</span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true" tabindex="-1"></a>**`build_model` Function.** This is where I create the neural network itself. It's a simple sequential model with two hidden layers that I built some flexibility into. The input shape, the number of nodes, and the output size are all variables because I intend to apply this basic set-up to other problems. In the tic-tac-toe problem, the neural network takes the board state as input, so in this case the <span class="in">`input_shape`</span> is nine. The two hidden layers have 81 nodes each with the <span class="in">`relu`</span> activation function. The output also has a length of nine that are the Q-values for each of the nine actions (i.e., moves on the game board). In other words, the network takes the game board as input, and provides the best move as output.</span>
<span id="cb9-80"><a href="#cb9-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-81"><a href="#cb9-81" aria-hidden="true" tabindex="-1"></a>**`play_ttt` Function.** It might make more sense to explain this function next. Basically, there's an inner loop to play one game of tic-tac-toe, and an outer loop to play the game multiple times. Most of the function just controls the mechanics of the game, but there are two aspects I want to point out. 1. The function uses an *epsilon-greedy policy* to determine whether to make a move based on the neural network Q-values or make a random move. The probability of playing randomly is initially high during training to encourage the algorithm to explore the state space. As training progresses, player X moves are more and more likely to be based on the neural network. Note that the neural network is set up to learn the game from player X's perspective. Player O always plays randomly. 2. During the course of playing one game, the game board states, player moves (actions), and rewards are recorded. These are passed to the <span class="in">`train_model`</span> function at the end of a game.</span>
<span id="cb9-82"><a href="#cb9-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-83"><a href="#cb9-83" aria-hidden="true" tabindex="-1"></a>**`train_model` Function.** This is where the neural network gets trained and things get complicated. I'm going to let Aurélien Géron do the 'splainin from his book <span class="co">[</span><span class="ot">Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</span><span class="co">](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646)</span>.</span>
<span id="cb9-84"><a href="#cb9-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-85"><a href="#cb9-85" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Consider the approximate Q-Value computed by the DQN for a given state-action pair (s, a). Thanks to Bellman, we know we want this approximate Q-Value to be as close as possible to the reward r that we actually observe after playing action a in state s, plus the discounted value of playing optimally from then on. To estimate this sum of future discounted rewards, we can simply execute the DQN on the next state s′ and for all possible actions a′. We get an approximate future Q-Value for each possible action. We then pick the highest (since we assume we will be playing optimally) and discount it, and this gives us an estimate of the sum of future discounted rewards. By summing the reward r and the future discounted value estimate, we get a target Q-Value y(s, a) for the state-action pair (s, a), as shown in the equation:</span></span>
<span id="cb9-86"><a href="#cb9-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-87"><a href="#cb9-87" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-88"><a href="#cb9-88" aria-hidden="true" tabindex="-1"></a>Q_{target}(s,a)=r\gamma\cdot \underset{a'}{max}Q_{\theta}\left(s', a'\right)</span>
<span id="cb9-89"><a href="#cb9-89" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-90"><a href="#cb9-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-91"><a href="#cb9-91" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; With this target Q-Value, we can run a training step using any Gradient Descent algorithm. Specifically, we generally try to minimize the squared error between the estimated Q-Value Q(s, a) and the target Q-Value (or the Huber loss to reduce the algorithm's sensitivity to large errors). And that's all for the basic Deep Q-Learning algorithm!</span></span>
<span id="cb9-92"><a href="#cb9-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-93"><a href="#cb9-93" aria-hidden="true" tabindex="-1"></a>Right. So that's what happening in the <span class="in">`train_model`</span> function.</span>
<span id="cb9-94"><a href="#cb9-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-97"><a href="#cb9-97" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb9-98"><a href="#cb9-98" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DQNagent:</span>
<span id="cb9-99"><a href="#cb9-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-100"><a href="#cb9-100" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, state_size, action_size, iterations):</span>
<span id="cb9-101"><a href="#cb9-101" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha0 <span class="op">=</span> <span class="fl">0.05</span>           <span class="co"># learning rate</span></span>
<span id="cb9-102"><a href="#cb9-102" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decay <span class="op">=</span> <span class="fl">0.005</span>           <span class="co"># learning rate decay</span></span>
<span id="cb9-103"><a href="#cb9-103" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gamma <span class="op">=</span> <span class="fl">0.95</span>            <span class="co"># discount factor</span></span>
<span id="cb9-104"><a href="#cb9-104" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.state_size <span class="op">=</span> state_size</span>
<span id="cb9-105"><a href="#cb9-105" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.action_size <span class="op">=</span> action_size</span>
<span id="cb9-106"><a href="#cb9-106" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.iterations <span class="op">=</span> iterations</span>
<span id="cb9-107"><a href="#cb9-107" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> <span class="va">self</span>.build_model()</span>
<span id="cb9-108"><a href="#cb9-108" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.optimizer <span class="op">=</span> tf.keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">0.02</span>)</span>
<span id="cb9-109"><a href="#cb9-109" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.loss_fn <span class="op">=</span> tf.keras.losses.mean_squared_error</span>
<span id="cb9-110"><a href="#cb9-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-111"><a href="#cb9-111" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build_model(<span class="va">self</span>):</span>
<span id="cb9-112"><a href="#cb9-112" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> tf.keras.models.Sequential([</span>
<span id="cb9-113"><a href="#cb9-113" aria-hidden="true" tabindex="-1"></a>            tf.keras.layers.Dense(<span class="va">self</span>.state_size<span class="op">**</span><span class="dv">2</span>, activation<span class="op">=</span><span class="st">"relu"</span>, input_shape<span class="op">=</span>[<span class="va">self</span>.state_size]),</span>
<span id="cb9-114"><a href="#cb9-114" aria-hidden="true" tabindex="-1"></a>            tf.keras.layers.Dense(<span class="va">self</span>.state_size<span class="op">**</span><span class="dv">2</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb9-115"><a href="#cb9-115" aria-hidden="true" tabindex="-1"></a>            tf.keras.layers.Dense(<span class="va">self</span>.action_size)</span>
<span id="cb9-116"><a href="#cb9-116" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb9-117"><a href="#cb9-117" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> model</span>
<span id="cb9-118"><a href="#cb9-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-119"><a href="#cb9-119" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train_model(<span class="va">self</span>, state_history, action_history, next_state_history, rewards, dones):</span>
<span id="cb9-120"><a href="#cb9-120" aria-hidden="true" tabindex="-1"></a>        next_Q_values <span class="op">=</span> <span class="va">self</span>.model.predict(np.array(next_state_history), verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb9-121"><a href="#cb9-121" aria-hidden="true" tabindex="-1"></a>        max_next_Q_values <span class="op">=</span> np.<span class="bu">max</span>(next_Q_values, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-122"><a href="#cb9-122" aria-hidden="true" tabindex="-1"></a>        target_Q_values <span class="op">=</span> rewards <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> <span class="dv">1</span><span class="op">*</span>np.array(dones)) <span class="op">*</span> <span class="va">self</span>.gamma <span class="op">*</span> max_next_Q_values</span>
<span id="cb9-123"><a href="#cb9-123" aria-hidden="true" tabindex="-1"></a>        target_Q_values <span class="op">=</span> tf.reshape(target_Q_values, [<span class="bu">len</span>(rewards), <span class="dv">1</span>])</span>
<span id="cb9-124"><a href="#cb9-124" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> tf.one_hot(action_history, <span class="dv">9</span>)</span>
<span id="cb9-125"><a href="#cb9-125" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> tf.GradientTape() <span class="im">as</span> tape:</span>
<span id="cb9-126"><a href="#cb9-126" aria-hidden="true" tabindex="-1"></a>            all_Q_values <span class="op">=</span> <span class="va">self</span>.model(np.array(state_history))</span>
<span id="cb9-127"><a href="#cb9-127" aria-hidden="true" tabindex="-1"></a>            Q_values <span class="op">=</span> tf.reduce_sum(all_Q_values <span class="op">*</span> mask, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-128"><a href="#cb9-128" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> tf.reduce_mean(<span class="va">self</span>.loss_fn(target_Q_values, Q_values))</span>
<span id="cb9-129"><a href="#cb9-129" aria-hidden="true" tabindex="-1"></a>        grads <span class="op">=</span> tape.gradient(loss, <span class="va">self</span>.model.trainable_variables)</span>
<span id="cb9-130"><a href="#cb9-130" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.optimizer.apply_gradients(<span class="bu">zip</span>(grads, <span class="va">self</span>.model.trainable_variables))</span>
<span id="cb9-131"><a href="#cb9-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-132"><a href="#cb9-132" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> play_ttt(<span class="va">self</span>):</span>
<span id="cb9-133"><a href="#cb9-133" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.iterations):    <span class="co"># outer loop to play the game a bunch of times</span></span>
<span id="cb9-134"><a href="#cb9-134" aria-hidden="true" tabindex="-1"></a>            state <span class="op">=</span> env().reset()</span>
<span id="cb9-135"><a href="#cb9-135" aria-hidden="true" tabindex="-1"></a>            next_state <span class="op">=</span> state.copy()</span>
<span id="cb9-136"><a href="#cb9-136" aria-hidden="true" tabindex="-1"></a>            done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb9-137"><a href="#cb9-137" aria-hidden="true" tabindex="-1"></a>            dones <span class="op">=</span> []</span>
<span id="cb9-138"><a href="#cb9-138" aria-hidden="true" tabindex="-1"></a>            state_history <span class="op">=</span> []</span>
<span id="cb9-139"><a href="#cb9-139" aria-hidden="true" tabindex="-1"></a>            state_history.append(state)</span>
<span id="cb9-140"><a href="#cb9-140" aria-hidden="true" tabindex="-1"></a>            action_history <span class="op">=</span> []</span>
<span id="cb9-141"><a href="#cb9-141" aria-hidden="true" tabindex="-1"></a>            rewards <span class="op">=</span> []</span>
<span id="cb9-142"><a href="#cb9-142" aria-hidden="true" tabindex="-1"></a>            epsilon <span class="op">=</span> <span class="bu">max</span>(<span class="dv">1</span> <span class="op">-</span> iteration<span class="op">/</span>(<span class="va">self</span>.iterations<span class="op">*</span><span class="fl">0.8</span>), <span class="fl">0.01</span>)</span>
<span id="cb9-143"><a href="#cb9-143" aria-hidden="true" tabindex="-1"></a>            <span class="cf">while</span> <span class="kw">not</span> done:                          <span class="co"># inner loop to play one game</span></span>
<span id="cb9-144"><a href="#cb9-144" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> random.random() <span class="op">&lt;</span> epsilon:        <span class="co"># epsilon-greedy policy</span></span>
<span id="cb9-145"><a href="#cb9-145" aria-hidden="true" tabindex="-1"></a>                    action <span class="op">=</span> random.choice([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(state)) <span class="cf">if</span> state[i] <span class="op">==</span> <span class="dv">0</span>])</span>
<span id="cb9-146"><a href="#cb9-146" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb9-147"><a href="#cb9-147" aria-hidden="true" tabindex="-1"></a>                    action <span class="op">=</span> np.argmax(<span class="va">self</span>.model.predict(np.array(state)[np.newaxis], verbose<span class="op">=</span><span class="dv">0</span>)[<span class="dv">0</span>])</span>
<span id="cb9-148"><a href="#cb9-148" aria-hidden="true" tabindex="-1"></a>                action_history.append(action)</span>
<span id="cb9-149"><a href="#cb9-149" aria-hidden="true" tabindex="-1"></a>                next_state, done, reward <span class="op">=</span> env().step(state, action, <span class="dv">0</span>)</span>
<span id="cb9-150"><a href="#cb9-150" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> done:</span>
<span id="cb9-151"><a href="#cb9-151" aria-hidden="true" tabindex="-1"></a>                    state_history.append(next_state)</span>
<span id="cb9-152"><a href="#cb9-152" aria-hidden="true" tabindex="-1"></a>                    dones.append(done)</span>
<span id="cb9-153"><a href="#cb9-153" aria-hidden="true" tabindex="-1"></a>                    rewards.append(reward)</span>
<span id="cb9-154"><a href="#cb9-154" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="kw">not</span> done:</span>
<span id="cb9-155"><a href="#cb9-155" aria-hidden="true" tabindex="-1"></a>                    omove <span class="op">=</span> random.choice([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(next_state)) <span class="cf">if</span> next_state[i] <span class="op">==</span> <span class="dv">0</span>])</span>
<span id="cb9-156"><a href="#cb9-156" aria-hidden="true" tabindex="-1"></a>                    next_state, done, reward <span class="op">=</span> env().step(next_state, omove, <span class="dv">1</span>)</span>
<span id="cb9-157"><a href="#cb9-157" aria-hidden="true" tabindex="-1"></a>                    state <span class="op">=</span> next_state.copy()</span>
<span id="cb9-158"><a href="#cb9-158" aria-hidden="true" tabindex="-1"></a>                    state_history.append(next_state)</span>
<span id="cb9-159"><a href="#cb9-159" aria-hidden="true" tabindex="-1"></a>                    dones.append(done)</span>
<span id="cb9-160"><a href="#cb9-160" aria-hidden="true" tabindex="-1"></a>                    rewards.append(reward)</span>
<span id="cb9-161"><a href="#cb9-161" aria-hidden="true" tabindex="-1"></a>            next_state_history <span class="op">=</span> state_history[<span class="dv">1</span>:<span class="bu">len</span>(state_history)]</span>
<span id="cb9-162"><a href="#cb9-162" aria-hidden="true" tabindex="-1"></a>            state_history <span class="op">=</span> state_history[<span class="dv">0</span>:<span class="bu">len</span>(action_history)]</span>
<span id="cb9-163"><a href="#cb9-163" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.train_model(state_history, action_history, next_state_history, rewards, dones)</span>
<span id="cb9-164"><a href="#cb9-164" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.model</span>
<span id="cb9-165"><a href="#cb9-165" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-166"><a href="#cb9-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-167"><a href="#cb9-167" aria-hidden="true" tabindex="-1"></a><span class="fu">## Training</span></span>
<span id="cb9-168"><a href="#cb9-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-169"><a href="#cb9-169" aria-hidden="true" tabindex="-1"></a>Here I just provide the <span class="in">`DQNagent`</span> with the number of input and output nodes for the neural network (9 each) and how many games to play (1000). Compare the number of games here with the 200,000 games I used for the Q-learning method - a huge difference!</span>
<span id="cb9-170"><a href="#cb9-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-173"><a href="#cb9-173" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb9-174"><a href="#cb9-174" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb9-175"><a href="#cb9-175" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> DQNagent(<span class="dv">9</span>,<span class="dv">9</span>,<span class="dv">1000</span>).play_ttt()</span>
<span id="cb9-176"><a href="#cb9-176" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-177"><a href="#cb9-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-178"><a href="#cb9-178" aria-hidden="true" tabindex="-1"></a><span class="fu">## Results</span></span>
<span id="cb9-179"><a href="#cb9-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-180"><a href="#cb9-180" aria-hidden="true" tabindex="-1"></a>I want to see real quick if the neural net has learned to play in the center of the board on the opening move, so I have the model give me the Q-values for an empty board. The fifth number returned represents the center of the board, and it is the highest Q-value, so the neural net has already learned it!</span>
<span id="cb9-181"><a href="#cb9-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-184"><a href="#cb9-184" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb9-185"><a href="#cb9-185" aria-hidden="true" tabindex="-1"></a>m.predict(np.zeros(<span class="dv">9</span>)[np.newaxis], verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb9-186"><a href="#cb9-186" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-187"><a href="#cb9-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-188"><a href="#cb9-188" aria-hidden="true" tabindex="-1"></a>Next, the <span class="in">`play_v_random`</span> function pits the AI-enabled player against an opponent that plays randomly.</span>
<span id="cb9-189"><a href="#cb9-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-192"><a href="#cb9-192" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb9-193"><a href="#cb9-193" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> play_v_random (games):</span>
<span id="cb9-194"><a href="#cb9-194" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> [<span class="dv">0</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(games)]</span>
<span id="cb9-195"><a href="#cb9-195" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(games):</span>
<span id="cb9-196"><a href="#cb9-196" aria-hidden="true" tabindex="-1"></a>        board <span class="op">=</span> env().reset()</span>
<span id="cb9-197"><a href="#cb9-197" aria-hidden="true" tabindex="-1"></a>        done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb9-198"><a href="#cb9-198" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb9-199"><a href="#cb9-199" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print(board[0:3])</span></span>
<span id="cb9-200"><a href="#cb9-200" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print(board[3:6])</span></span>
<span id="cb9-201"><a href="#cb9-201" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print(board[6:9])</span></span>
<span id="cb9-202"><a href="#cb9-202" aria-hidden="true" tabindex="-1"></a>            xmoves <span class="op">=</span> m.predict(np.array(board)[np.newaxis], verbose<span class="op">=</span><span class="dv">0</span>)[<span class="dv">0</span>]</span>
<span id="cb9-203"><a href="#cb9-203" aria-hidden="true" tabindex="-1"></a>            xmoves[np.where(np.array(board)<span class="op">!=</span><span class="dv">0</span>)[<span class="dv">0</span>]] <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb9-204"><a href="#cb9-204" aria-hidden="true" tabindex="-1"></a>            xmove <span class="op">=</span> np.argmax(xmoves)</span>
<span id="cb9-205"><a href="#cb9-205" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print("move", xmove)</span></span>
<span id="cb9-206"><a href="#cb9-206" aria-hidden="true" tabindex="-1"></a>            board[xmove] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb9-207"><a href="#cb9-207" aria-hidden="true" tabindex="-1"></a>            done, reward <span class="op">=</span> env().game_over(board)</span>
<span id="cb9-208"><a href="#cb9-208" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> done:</span>
<span id="cb9-209"><a href="#cb9-209" aria-hidden="true" tabindex="-1"></a>                omove <span class="op">=</span> random.choice([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(board)) <span class="cf">if</span> board[i] <span class="op">==</span> <span class="dv">0</span>])</span>
<span id="cb9-210"><a href="#cb9-210" aria-hidden="true" tabindex="-1"></a>                board[omove] <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb9-211"><a href="#cb9-211" aria-hidden="true" tabindex="-1"></a>                done, reward <span class="op">=</span> env().game_over(board)</span>
<span id="cb9-212"><a href="#cb9-212" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print(board[0:3])</span></span>
<span id="cb9-213"><a href="#cb9-213" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print(board[3:6])</span></span>
<span id="cb9-214"><a href="#cb9-214" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print(board[6:9])</span></span>
<span id="cb9-215"><a href="#cb9-215" aria-hidden="true" tabindex="-1"></a>        results[i] <span class="op">=</span> reward</span>
<span id="cb9-216"><a href="#cb9-216" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> results</span>
<span id="cb9-217"><a href="#cb9-217" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-218"><a href="#cb9-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-219"><a href="#cb9-219" aria-hidden="true" tabindex="-1"></a>So here we have it. The AI-enabled player won 90% of the games, lost 8%, and 2% were ties. For comparison, the best performance using Q-learning was 81.2% AI wins, 2.3% losses, and 16.5% ties. The deep Q-network performed significantly better after being trained on 1,000 games than Q-learning did after being trained on 200,000 games. Remarkable!</span>
<span id="cb9-220"><a href="#cb9-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-223"><a href="#cb9-223" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb9-224"><a href="#cb9-224" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb9-225"><a href="#cb9-225" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> play_v_random(<span class="dv">1000</span>)</span>
<span id="cb9-226"><a href="#cb9-226" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-227"><a href="#cb9-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-230"><a href="#cb9-230" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb9-231"><a href="#cb9-231" aria-hidden="true" tabindex="-1"></a><span class="bu">sum</span>(<span class="dv">1</span> <span class="cf">for</span> i <span class="kw">in</span> results <span class="cf">if</span> i <span class="op">==</span> <span class="dv">1</span>), <span class="bu">sum</span>(<span class="dv">1</span> <span class="cf">for</span> i <span class="kw">in</span> results <span class="cf">if</span> i <span class="op">==</span> <span class="op">-</span><span class="dv">1</span>), <span class="bu">sum</span>(<span class="dv">1</span> <span class="cf">for</span> i <span class="kw">in</span> results <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>)</span>
<span id="cb9-232"><a href="#cb9-232" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-233"><a href="#cb9-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-234"><a href="#cb9-234" aria-hidden="true" tabindex="-1"></a>As I mentioned at the beginning of the post, a deep Q-network is the most basic application of neural networks to reinforcement learning. More advanced applications include double deep Q-networks and dueling deep Q networks, and each of these can be enhanced with techniques like actor-critic algorithms, curiosity-based exploration, proximal policy optimization, and so on. There's a lot left to explore.</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Copyright 2022, John King</div>   
  </div>
</footer>



</body></html>