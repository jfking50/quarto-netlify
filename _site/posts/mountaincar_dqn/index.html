<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="John King">
<meta name="dcterms.date" content="2020-04-11">
<meta name="description" content="Solve OpenAI’s MountainCar-v0 environment with a deep Q network.">

<title>A Random Walk - Driving Up A Mountain</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
        <script type="text/javascript">
        window.PlotlyConfig = {MathJaxConfig: 'local'};
        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}
        if (typeof require !== 'undefined') {
        require.undef("plotly");
        requirejs.config({
            paths: {
                'plotly': ['https://cdn.plot.ly/plotly-2.16.1.min']
            }
        });
        require(['plotly'], function(Plotly) {
            window._Plotly = Plotly;
        });
        }
        </script>
        


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">A Random Walk</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-tutorials" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Tutorials</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-tutorials">    
        <li>
    <a class="dropdown-item" href="../../tutorial/slr.html">
 <span class="dropdown-text">Simple Linear Regression</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../tutorial/lm_assumptions.html">
 <span class="dropdown-text">Linear Model Assumptions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../tutorial/mlr.html">
 <span class="dropdown-text">Multiple Linear Regression</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../tutorial/model_selection.html">
 <span class="dropdown-text">Model Selection</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../tutorial/transform.html">
 <span class="dropdown-text">Variable Transformation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../tutorial/logistic.html">
 <span class="dropdown-text">Logistic Regression</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../tutorial/advanced.html">
 <span class="dropdown-text">Advanced Designs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../tutorial/np_anova.html">
 <span class="dropdown-text">Nonparametric ANOVA</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../tutorial/gam.html">
 <span class="dropdown-text">Generalized Additive Models</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../tutorial/svm.html">
 <span class="dropdown-text">Support Vector Machines</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../tutorial/random_forest.html">
 <span class="dropdown-text">Random Forests</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../tutorial/nn_regression.html">
 <span class="dropdown-text">Neural Network Regression</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-presentations" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Presentations</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-presentations">    
        <li>
    <a class="dropdown-item" href="../../presentations/doe.html">
 <span class="dropdown-text">Design of Experiments</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/jfking50"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-mountaincar-environment" id="toc-the-mountaincar-environment" class="nav-link active" data-scroll-target="#the-mountaincar-environment">The MountainCar Environment</a></li>
  <li><a href="#dqn-agent" id="toc-dqn-agent" class="nav-link" data-scroll-target="#dqn-agent">DQN Agent</a></li>
  <li><a href="#let-the-training-begin" id="toc-let-the-training-begin" class="nav-link" data-scroll-target="#let-the-training-begin">Let The Training Begin</a>
  <ul class="collapse">
  <li><a href="#i-tried-a-lot-of-things-that-didnt-work" id="toc-i-tried-a-lot-of-things-that-didnt-work" class="nav-link" data-scroll-target="#i-tried-a-lot-of-things-that-didnt-work">I Tried A Lot Of Things That Didn’t Work</a></li>
  <li><a href="#what-did-work" id="toc-what-did-work" class="nav-link" data-scroll-target="#what-did-work">What Did Work</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Driving Up A Mountain</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
  <div class="quarto-categories">
    <div class="quarto-category">python</div>
    <div class="quarto-category">reinforcement learning</div>
    <div class="quarto-category">OpenAI</div>
    <div class="quarto-category">tensorflow</div>
    <div class="quarto-category">neural network</div>
  </div>
  </div>

<div>
  <div class="description">
    Solve OpenAI’s MountainCar-v0 environment with a deep Q network.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>John King </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 11, 2020</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>A while back, I found OpenAI’s <a href="https://gym.openai.com/">Gym environments</a> and immediately wanted to try to solve one. I didn’t really know what I was doing at the time, so I went back to the basics for a better understanding of <a href="https://jfking.netlify.app/posts/qlearning/">Q-learning</a> and <a href="https://jfking.netlify.app/blog/dqn-ttt/">Deep Q-Networks</a>. Now I think I’m ready to graduate from tic-tac-toe and try a Gym environment again. Best to start simple, though.</p>
<section id="the-mountaincar-environment" class="level2">
<h2 class="anchored" data-anchor-id="the-mountaincar-environment">The MountainCar Environment</h2>
<p>There are two Mountain Car environments: one with a discrete number of actions, and one with a continuous range of actions. Keeping it simple means go with the discrete case. The Gym documentation describes the situation and the goal:</p>
<blockquote class="blockquote">
<p>A car is on a one-dimensional track, positioned between two “mountains”. The goal is to drive up the mountain on the right; however, the car’s engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mc.png" class="img-fluid figure-img" width="300"></p>
<p></p><figcaption class="figure-caption">MountainCar</figcaption><p></p>
</figure>
</div>
<p>One of the nice things about working with Gym environments is that I don’t have to put effort into defining my own environment like I did with tic-tac-toe. All I have to do to create the mountain car environment is <code>gym.make('MountainCar-v0')</code>. It’s that easy! I need to get some information about the <strong>state space and actions</strong> so I know what I’m dealing with. First the state space.</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gym</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">'MountainCar-v0'</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(env.observation_space)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Box([-1.2  -0.07], [0.6  0.07], (2,), float32)</code></pre>
</div>
</div>
<p><code>Box</code> means that it’s a continuous state space, and the (2,) means there are two numbers that represent the state space. The two lists represent the low and high limits of the state space. Going back to the documentation, the state represents the position and velocity of the car in that order. In other words, the position can be between -1.2 and 0.6, and the velocity can be between -0.07 and 0.07.</p>
<p>The documentation states that an episode ends the car reaches 0.5 position, or if 200 iterations are reached. That means the position value is the x-axis with positive values to the right, and that a positive velocity means the car is moving to the right. The documentation also says that the starting state is a random position from -0.6 to -0.4 with no velocity, so the car starts at the bottom of the valley at a stand still. Makes sense. What do I have for actions?</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(env.action_space)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Discrete(3)</code></pre>
</div>
</div>
<p>Three possible actions. 0 means push left, 1 means do nothing (not sure why you’d ever do that), and 2 means push right.</p>
<p>Last thing I need is how the <strong>rewards</strong> work. They say you get “-1 for each time step until the goal position of 0.5 is reached”. So there’s no positive reward? Huh. They also say “there is no penalty for climbing the left hill, which upon reached acts as a wall”. I guess that means the car can bounce off it without crashing.</p>
<p>To <strong>interact with the environment</strong>, I first need to reset it with <code>env.reset()</code>. This gives me the car’s starting state. Then I need an action. I can get a random action from the environment with <code>env.action_space.sample()</code>, or I could just use <code>numpy</code> to generate a random number. Anyway, then to execute that action in the environment, I use <code>env.step(action)</code>. This returns the next observation based on that action, the reward (always -1), whether the episode is over, and some empty information. Also according to the docs, I should close the environment when I’m done with it. Here I’ll take 5 random actions to see what things look like.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>observation <span class="op">=</span> env.reset()</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"observation:"</span>, observation)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    action <span class="op">=</span> env.action_space.sample()</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"action:"</span>, action)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    observation, reward, done, info, empty <span class="op">=</span> env.step(action)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"step results:"</span>, observation, reward, done, info)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>env.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>observation: (array([-0.5010359,  0.       ], dtype=float32), {})
action: 1
step results: [-5.012050e-01 -1.690923e-04] -1.0 False False
action: 2
step results: [-0.5005419   0.00066308] -1.0 False False
action: 1
step results: [-5.000516e-01  4.902922e-04] -1.0 False False
action: 0
step results: [-0.5007378  -0.00068616] -1.0 False False
action: 1
step results: [-0.50159526 -0.00085749] -1.0 False False</code></pre>
</div>
</div>
<p>Ok, got it. I’ll import a bunch of stuff and then get to solving this thing with a DQN agent.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> deque</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="dqn-agent" class="level2">
<h2 class="anchored" data-anchor-id="dqn-agent">DQN Agent</h2>
<p>If you read my last post on <a href="https://jfking.netlify/blog/dqn-ttt/">deep Q-networks</a>, a lot of this will look familiar. I’m only going to explain what’s different, so you might want to go back and read it.</p>
<p><strong><code>build_model</code></strong>: The only thing I changed here was to reduce the number of nodes in the two hidden layers. Since there are only two values that represent the state and only three actions, 32 nodes seemed plenty.</p>
<p><strong><code>add_memory</code></strong>: This is completely new. What I’m doing here is storing information about every step (an “experience”) to create what’s referred to as a replay memory. In the <code>__init__</code> function, I create a deque of length 2000 for this purpose. This function simply adds an experience to the replay memory. Since it’s a deque, once it contains 2000 items, adding a new item to the top of the queue will cause the oldest item to be removed.</p>
<p><strong><code>sample_experiences</code></strong>: This is also new and is part of the replay memory implementation. This function is called only after the replay memory deque is full. Here I randomly sample 64 experiences (defined by <code>batch_size</code>) from the replay memory. According to <a href="https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646">Aurélien Géron</a>,</p>
<blockquote class="blockquote">
<p>This helps reduce the correlations between the experiences in a training batch, which tremendously helps training.</p>
</blockquote>
<p><strong><code>train_model</code></strong>: The only thing different here is the implementation of training from memory replay via <code>self.sample_experiences()</code> instead of training based on the full set of experiences. Otherwise, the neural net is trained exactly the same way as it did for the tic-tac-toe example. Although not strictly part of this function, I also switched from a stochastic gradient decent optimizer to an Adam optimizer and decreased the learning rate to 0.001 in the <code>__init__</code> function.</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DQNagent:</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, state_size, action_size, episodes):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gamma <span class="op">=</span> <span class="fl">0.95</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.state_size <span class="op">=</span> state_size</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.action_size <span class="op">=</span> action_size</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.episodes <span class="op">=</span> episodes</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.replay_memory <span class="op">=</span> deque(maxlen<span class="op">=</span><span class="dv">2000</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.optimizer <span class="op">=</span> tf.keras.optimizers.Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.loss_fn <span class="op">=</span> tf.keras.losses.mean_squared_error</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build_model(<span class="va">self</span>):</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> tf.keras.models.Sequential([</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>            tf.keras.layers.Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">"relu"</span>, input_shape<span class="op">=</span>[<span class="va">self</span>.state_size]),</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>            tf.keras.layers.Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>            tf.keras.layers.Dense(<span class="va">self</span>.action_size)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> model</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> add_memory(<span class="va">self</span>, state, action, reward, next_state, done):</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.replay_memory.append((state, action, reward, next_state, done))</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample_experiences(<span class="va">self</span>):</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>        indices <span class="op">=</span> np.random.randint(<span class="bu">len</span>(<span class="va">self</span>.replay_memory), size<span class="op">=</span><span class="va">self</span>.batch_size)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>        batch <span class="op">=</span> [<span class="va">self</span>.replay_memory[index] <span class="cf">for</span> index <span class="kw">in</span> indices]</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>        states, actions, rewards, next_states, dones <span class="op">=</span> [</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>            np.array([experience[field_index] <span class="cf">for</span> experience <span class="kw">in</span> batch])</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> field_index <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>)]</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> states, actions, rewards, next_states, dones</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train_model(<span class="va">self</span>, model):</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>        states, actions, rewards, next_states, dones <span class="op">=</span> <span class="va">self</span>.sample_experiences()</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>        next_Q_values <span class="op">=</span> model.predict(next_states, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>        max_next_Q_values <span class="op">=</span> np.<span class="bu">max</span>(next_Q_values, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>        target_Q_values <span class="op">=</span> (rewards <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> dones) <span class="op">*</span> <span class="va">self</span>.gamma <span class="op">*</span> max_next_Q_values)</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>        target_Q_values <span class="op">=</span> target_Q_values.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> tf.one_hot(actions, <span class="va">self</span>.action_size)</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> tf.GradientTape() <span class="im">as</span> tape:</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>            all_Q_values <span class="op">=</span> model(states)</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>            Q_values <span class="op">=</span> tf.reduce_sum(all_Q_values <span class="op">*</span> mask, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> tf.reduce_mean(<span class="va">self</span>.loss_fn(target_Q_values, Q_values))</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>        grads <span class="op">=</span> tape.gradient(loss, model.trainable_variables)</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.optimizer.apply_gradients(<span class="bu">zip</span>(grads, model.trainable_variables))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="let-the-training-begin" class="level2">
<h2 class="anchored" data-anchor-id="let-the-training-begin">Let The Training Begin</h2>
<p>This problem kicked my butt for quite a while. I <em>could not</em> get that stupid car up the hill! It seemed so simple, but I was stumped. I actually solved this early on with a simple hard-coded policy: if the car is moving left (a negative velocity), push left. Otherwise, push right. Done. Didn’t even need a neural net, q-learning, or anything. So why bother with solving this with reinforcement learning? I guess in the real world I wouldn’t, but the point here is to see if I can train a neural net (NN) to learn that policy.</p>
<section id="i-tried-a-lot-of-things-that-didnt-work" class="level3">
<h3 class="anchored" data-anchor-id="i-tried-a-lot-of-things-that-didnt-work">I Tried A Lot Of Things That Didn’t Work</h3>
<p>First, I left everything as is, and just fed experiences to the NN hoping it would learn. Recall that each episode consists of 200 steps, and so you get a -1 reward for each step for a total reward of -200. If the car reaches the goal in say 150 steps, then you get a total reward of -150. My idea here was that the NN would learn to maximize the total reward by finishing in under 200 steps. Nope. Never happened. The car never got to the goal during training and so the NN never learned anything.</p>
<p>Then I started thinking about the reward system. It seemed like I needed to give the NN some positive reward to encourage it along. I tried giving a small positive reward if the car made it some distance away from the range of possible starting points. That ended up teaching the NN to drive up and down the same side of the hill over and over. Clearly, the statement, “the only way to succeed is to drive back and forth to build up momentum” is correct, but how do I do that? I tried a one-time positive reward on one side of the hill and only gave additional rewards if the car then tried going up the other side of the hill. That didn’t work, either. I tried giving a huge reward if the car ever made it to the goal position, hoping that would back-propagate quickly through the NN weights. Still nope.</p>
<p>It seemed like this reward system I was creating was getting a lot more complicated that it should need to be, so then I tried all of the above while at the same time adjusting the tuning parameters: learning rate, discount rate, and epsilon. No luck. I tried fiddling with how the NN was constructed: number of hidden layers, nodes, etc. Nope. I also tried changing the replay memory batch size. I even took memory replay completely out of the algorithm thinking that maybe that rare time the car made it to the goal kept getting missed in the random draw from the memory replay. Nope again.</p>
</section>
<section id="what-did-work" class="level3">
<h3 class="anchored" data-anchor-id="what-did-work">What Did Work</h3>
<p>It dawned on me that maybe I should just come up with a reward system that mimicked the hard-coded policy I described earlier. Finally, that worked! In the code below, you’ll see:</p>
<ul>
<li><code>if next_state[0] - state[0] &gt; 0 and action == 2: reward = 1</code> moving right and pushing right = a reward</li>
<li><code>if next_state[0] - state[0] &lt; 0 and action == 0: reward = 1</code> moving left and pushing left = a reward</li>
</ul>
<p>The following code trans the NN for 600 episodes and keeps track of the best score. I define the score as the number of steps it takes to reach the goal, so the fewer, the better. When a new best score is reached, I save the model weights as <code>best_weights</code>. After the 600th episode is over, I reset the model weights with the best weights. Why? It turns out that learning is hard, and forgetting is easy, which I’ll demonstrate shortly.</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>best_score <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>episodes <span class="op">=</span> <span class="dv">600</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">'MountainCar-v0'</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>state_size <span class="op">=</span> env.observation_space.shape[<span class="dv">0</span>]</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>action_size <span class="op">=</span> env.action_space.n</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> DQNagent(state_size, action_size, episodes)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> agent.build_model()</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>rewards <span class="op">=</span> []</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(episodes):</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    state <span class="op">=</span> env.reset()[<span class="dv">0</span>]</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">200</span>):</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        epsilon <span class="op">=</span> <span class="bu">max</span>(<span class="dv">1</span> <span class="op">-</span> episode<span class="op">/</span>(episodes<span class="op">*</span><span class="fl">0.8</span>), <span class="fl">0.01</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.random.rand() <span class="op">&lt;</span> epsilon: action <span class="op">=</span> np.random.randint(action_size)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>: action <span class="op">=</span> np.argmax(model.predict(state[np.newaxis], verbose <span class="op">=</span> <span class="dv">0</span>)[<span class="dv">0</span>])</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        next_state, reward, done, info, empty <span class="op">=</span> env.step(action)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> next_state[<span class="dv">0</span>] <span class="op">-</span> state[<span class="dv">0</span>] <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> action <span class="op">==</span> <span class="dv">2</span>: reward <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> next_state[<span class="dv">0</span>] <span class="op">-</span> state[<span class="dv">0</span>] <span class="op">&lt;</span> <span class="dv">0</span> <span class="kw">and</span> action <span class="op">==</span> <span class="dv">0</span>: reward <span class="op">=</span> <span class="dv">1</span>        </span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>        agent.add_memory(state, action, reward, next_state, done)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> next_state.copy()</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> done:</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    rewards.append(step)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">&lt;</span> best_score:</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>        best_weights <span class="op">=</span> model.get_weights()</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>        best_score <span class="op">=</span> step</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> episode <span class="op">&gt;</span> <span class="dv">50</span>:</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>        agent.train_model(model)</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>model.set_weights(best_weights)</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>env.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>In the figure below, you can see that it took about 330 episodes for the car to reach the goal for the first time. As training progressed, the score improved for a short time but then went back to 200. That happened a number of times, too. This is what Aurélien Géron describes as “catastrophic forgetting”, which makes me laugh but describes it perfectly.</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb604"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb604-1"><a href="#cb604-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb604-2"><a href="#cb604-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb604-3"><a href="#cb604-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.line(x<span class="op">=</span>[i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(rewards))], y<span class="op">=</span>rewards)</span>
<span id="cb604-4"><a href="#cb604-4" aria-hidden="true" tabindex="-1"></a>fig.update_layout(xaxis_title<span class="op">=</span><span class="st">'Episode'</span>, yaxis_title<span class="op">=</span><span class="st">'Score'</span>)</span>
<span id="cb604-5"><a href="#cb604-5" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">

<div>                            <div id="c6adfc34-98f8-49a2-999d-c71be1f2d550" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("c6adfc34-98f8-49a2-999d-c71be1f2d550")) {                    Plotly.newPlot(                        "c6adfc34-98f8-49a2-999d-c71be1f2d550",                        [{"hovertemplate":"x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"","orientation":"v","showlegend":false,"x":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599],"xaxis":"x","y":[199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,89,146,92,157,142,176,107,111,114,199,123,134,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,138,147,146,105,199,91,199,88,199,199,199,199,199,199,199,199,199,199,115,165,162,164,199,199,199,199,199,199,199,199,199,164,101,124,161,194,173,167,199,85,199,199,199,199,199,199,199,199,199,199,199,199,199,101,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,119,166,163,166,95,92],"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Episode"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Score"}},"legend":{"tracegroupgap":0},"margin":{"t":60}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('c6adfc34-98f8-49a2-999d-c71be1f2d550');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
<p>According to the documentation for this environment, MountainCar-v0 is considered “solved” when the agent obtains an average reward of at least -110.0 over 100 consecutive episodes. That would translate into a score of 110 or less over 100 episodes. The best score here was 82, and looking at the graph, the NN didn’t maintain anything less than 200 very long, so there’s still work to do. I’ll go with what I have for now and demonstrate how the NN does visually.</p>
<p>Next, I’ll trained the model for 2000 episodes. I’m not going to re-run that code here because it took hours, and I’ve also been having an issue with Python kernel dying while rendering this page that I haven’t overcome yet. Instead, below is a plot of the results from when I originally trained the model.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2000_episodes.png" class="img-fluid figure-img" width="600"></p>
<p></p><figcaption class="figure-caption">Results after training for 2000 episodes.</figcaption><p></p>
</figure>
</div>
<p>This is much better than the previous results! The NN held a sub-200 score for quite a few episodes towards the end of training. However, the best score wasn’t consistently below 110, so there’s still more work to do. There is no incentive for reaching to goal quickly, so maybe I can get better results by adding a large reward for finishing in fewer than 110 steps. The problem with that approach is that training is conducted on a random sampling of 64 experiences out of the 2000 that are in the replay memory deque. Sub-110 scores are rare, so the chances of that reward making it into the training set is remote. Probably a better approach is to improve the NN performance so that sub-110 scores are more common. That might be possible by playing around with tuning parameters or maybe switching to a different type of reinforcement learning method like a Dueling Deep Q-Network or a Double Dueling Deep Q-Network. That’s for another post, though.</p>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'alternate';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb605" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb605-1"><a href="#cb605-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb605-2"><a href="#cb605-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Driving Up A Mountain"</span></span>
<span id="cb605-3"><a href="#cb605-3" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "Solve OpenAI's MountainCar-v0 environment with a deep Q network."</span></span>
<span id="cb605-4"><a href="#cb605-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "John King"</span></span>
<span id="cb605-5"><a href="#cb605-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "4/11/2020"</span></span>
<span id="cb605-6"><a href="#cb605-6" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb605-7"><a href="#cb605-7" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb605-8"><a href="#cb605-8" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb605-9"><a href="#cb605-9" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb605-10"><a href="#cb605-10" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb605-11"><a href="#cb605-11" aria-hidden="true" tabindex="-1"></a><span class="co">    code-copy: true</span></span>
<span id="cb605-12"><a href="#cb605-12" aria-hidden="true" tabindex="-1"></a><span class="co">    df-print: paged</span></span>
<span id="cb605-13"><a href="#cb605-13" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span><span class="co"> </span></span>
<span id="cb605-14"><a href="#cb605-14" aria-hidden="true" tabindex="-1"></a><span class="co">  warning: false</span></span>
<span id="cb605-15"><a href="#cb605-15" aria-hidden="true" tabindex="-1"></a><span class="co">  message: false</span></span>
<span id="cb605-16"><a href="#cb605-16" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb605-17"><a href="#cb605-17" aria-hidden="true" tabindex="-1"></a><span class="co">  - python</span></span>
<span id="cb605-18"><a href="#cb605-18" aria-hidden="true" tabindex="-1"></a><span class="co">  - reinforcement learning</span></span>
<span id="cb605-19"><a href="#cb605-19" aria-hidden="true" tabindex="-1"></a><span class="co">  - OpenAI</span></span>
<span id="cb605-20"><a href="#cb605-20" aria-hidden="true" tabindex="-1"></a><span class="co">  - tensorflow</span></span>
<span id="cb605-21"><a href="#cb605-21" aria-hidden="true" tabindex="-1"></a><span class="co">  - neural network</span></span>
<span id="cb605-22"><a href="#cb605-22" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "mc.png"</span></span>
<span id="cb605-23"><a href="#cb605-23" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb605-24"><a href="#cb605-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-25"><a href="#cb605-25" aria-hidden="true" tabindex="-1"></a>A while back, I found OpenAI's <span class="co">[</span><span class="ot">Gym environments</span><span class="co">](https://gym.openai.com/)</span> and immediately wanted to try to solve one. I didn't really know what I was doing at the time, so I went back to the basics for a better understanding of <span class="co">[</span><span class="ot">Q-learning</span><span class="co">](https://jfking.netlify.app/posts/qlearning/)</span> and <span class="co">[</span><span class="ot">Deep Q-Networks</span><span class="co">](https://jfking.netlify.app/blog/dqn-ttt/)</span>. Now I think I'm ready to graduate from tic-tac-toe and try a Gym environment again. Best to start simple, though.</span>
<span id="cb605-26"><a href="#cb605-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-27"><a href="#cb605-27" aria-hidden="true" tabindex="-1"></a><span class="fu">## The MountainCar Environment</span></span>
<span id="cb605-28"><a href="#cb605-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-29"><a href="#cb605-29" aria-hidden="true" tabindex="-1"></a>There are two Mountain Car environments: one with a discrete number of actions, and one with a continuous range of actions. Keeping it simple means go with the discrete case. The Gym documentation describes the situation and the goal:</span>
<span id="cb605-30"><a href="#cb605-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-31"><a href="#cb605-31" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; A car is on a one-dimensional track, positioned between two "mountains". The goal is to drive up the mountain on the right; however, the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.</span></span>
<span id="cb605-32"><a href="#cb605-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-33"><a href="#cb605-33" aria-hidden="true" tabindex="-1"></a><span class="al">![MountainCar](mc.png)</span>{width="300"}</span>
<span id="cb605-34"><a href="#cb605-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-35"><a href="#cb605-35" aria-hidden="true" tabindex="-1"></a>One of the nice things about working with Gym environments is that I don't have to put effort into defining my own environment like I did with tic-tac-toe. All I have to do to create the mountain car environment is <span class="in">`gym.make('MountainCar-v0')`</span>. It's that easy! I need to get some information about the **state space and actions** so I know what I'm dealing with. First the state space.</span>
<span id="cb605-36"><a href="#cb605-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-39"><a href="#cb605-39" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb605-40"><a href="#cb605-40" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gym</span>
<span id="cb605-41"><a href="#cb605-41" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">'MountainCar-v0'</span>)</span>
<span id="cb605-42"><a href="#cb605-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(env.observation_space)</span>
<span id="cb605-43"><a href="#cb605-43" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb605-44"><a href="#cb605-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-45"><a href="#cb605-45" aria-hidden="true" tabindex="-1"></a><span class="in">`Box`</span> means that it's a continuous state space, and the (2,) means there are two numbers that represent the state space. The two lists represent the low and high limits of the state space. Going back to the documentation, the state represents the position and velocity of the car in that order. In other words, the position can be between -1.2 and 0.6, and the velocity can be between -0.07 and 0.07.</span>
<span id="cb605-46"><a href="#cb605-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-47"><a href="#cb605-47" aria-hidden="true" tabindex="-1"></a>The documentation states that an episode ends the car reaches 0.5 position, or if 200 iterations are reached. That means the position value is the x-axis with positive values to the right, and that a positive velocity means the car is moving to the right. The documentation also says that the starting state is a random position from -0.6 to -0.4 with no velocity, so the car starts at the bottom of the valley at a stand still. Makes sense. What do I have for actions?</span>
<span id="cb605-48"><a href="#cb605-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-51"><a href="#cb605-51" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb605-52"><a href="#cb605-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(env.action_space)</span>
<span id="cb605-53"><a href="#cb605-53" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb605-54"><a href="#cb605-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-55"><a href="#cb605-55" aria-hidden="true" tabindex="-1"></a>Three possible actions. 0 means push left, 1 means do nothing (not sure why you'd ever do that), and 2 means push right.</span>
<span id="cb605-56"><a href="#cb605-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-57"><a href="#cb605-57" aria-hidden="true" tabindex="-1"></a>Last thing I need is how the **rewards** work. They say you get "-1 for each time step until the goal position of 0.5 is reached". So there's no positive reward? Huh. They also say "there is no penalty for climbing the left hill, which upon reached acts as a wall". I guess that means the car can bounce off it without crashing.</span>
<span id="cb605-58"><a href="#cb605-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-59"><a href="#cb605-59" aria-hidden="true" tabindex="-1"></a>To **interact with the environment**, I first need to reset it with <span class="in">`env.reset()`</span>. This gives me the car's starting state. Then I need an action. I can get a random action from the environment with <span class="in">`env.action_space.sample()`</span>, or I could just use <span class="in">`numpy`</span> to generate a random number. Anyway, then to execute that action in the environment, I use <span class="in">`env.step(action)`</span>. This returns the next observation based on that action, the reward (always -1), whether the episode is over, and some empty information. Also according to the docs, I should close the environment when I'm done with it. Here I'll take 5 random actions to see what things look like.</span>
<span id="cb605-60"><a href="#cb605-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-63"><a href="#cb605-63" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb605-64"><a href="#cb605-64" aria-hidden="true" tabindex="-1"></a>observation <span class="op">=</span> env.reset()</span>
<span id="cb605-65"><a href="#cb605-65" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"observation:"</span>, observation)</span>
<span id="cb605-66"><a href="#cb605-66" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb605-67"><a href="#cb605-67" aria-hidden="true" tabindex="-1"></a>    action <span class="op">=</span> env.action_space.sample()</span>
<span id="cb605-68"><a href="#cb605-68" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"action:"</span>, action)</span>
<span id="cb605-69"><a href="#cb605-69" aria-hidden="true" tabindex="-1"></a>    observation, reward, done, info, empty <span class="op">=</span> env.step(action)</span>
<span id="cb605-70"><a href="#cb605-70" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"step results:"</span>, observation, reward, done, info)</span>
<span id="cb605-71"><a href="#cb605-71" aria-hidden="true" tabindex="-1"></a>env.close()</span>
<span id="cb605-72"><a href="#cb605-72" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb605-73"><a href="#cb605-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-74"><a href="#cb605-74" aria-hidden="true" tabindex="-1"></a>Ok, got it. I'll import a bunch of stuff and then get to solving this thing with a DQN agent.</span>
<span id="cb605-75"><a href="#cb605-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-78"><a href="#cb605-78" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb605-79"><a href="#cb605-79" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb605-80"><a href="#cb605-80" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb605-81"><a href="#cb605-81" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb605-82"><a href="#cb605-82" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> deque</span>
<span id="cb605-83"><a href="#cb605-83" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb605-84"><a href="#cb605-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-85"><a href="#cb605-85" aria-hidden="true" tabindex="-1"></a><span class="fu">## DQN Agent</span></span>
<span id="cb605-86"><a href="#cb605-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-87"><a href="#cb605-87" aria-hidden="true" tabindex="-1"></a>If you read my last post on <span class="co">[</span><span class="ot">deep Q-networks</span><span class="co">](https://jfking.netlify/blog/dqn-ttt/)</span>, a lot of this will look familiar. I'm only going to explain what's different, so you might want to go back and read it.</span>
<span id="cb605-88"><a href="#cb605-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-89"><a href="#cb605-89" aria-hidden="true" tabindex="-1"></a>**`build_model`**: The only thing I changed here was to reduce the number of nodes in the two hidden layers. Since there are only two values that represent the state and only three actions, 32 nodes seemed plenty.</span>
<span id="cb605-90"><a href="#cb605-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-91"><a href="#cb605-91" aria-hidden="true" tabindex="-1"></a>**`add_memory`**: This is completely new. What I'm doing here is storing information about every step (an "experience") to create what's referred to as a replay memory. In the <span class="in">`__init__`</span> function, I create a deque of length 2000 for this purpose. This function simply adds an experience to the replay memory. Since it's a deque, once it contains 2000 items, adding a new item to the top of the queue will cause the oldest item to be removed.</span>
<span id="cb605-92"><a href="#cb605-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-93"><a href="#cb605-93" aria-hidden="true" tabindex="-1"></a>**`sample_experiences`**: This is also new and is part of the replay memory implementation. This function is called only after the replay memory deque is full. Here I randomly sample 64 experiences (defined by <span class="in">`batch_size`</span>) from the replay memory. According to <span class="co">[</span><span class="ot">Aurélien Géron</span><span class="co">](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646)</span>,</span>
<span id="cb605-94"><a href="#cb605-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-95"><a href="#cb605-95" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; This helps reduce the correlations between the experiences in a training batch, which tremendously helps training.</span></span>
<span id="cb605-96"><a href="#cb605-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-97"><a href="#cb605-97" aria-hidden="true" tabindex="-1"></a>**`train_model`**: The only thing different here is the implementation of training from memory replay via <span class="in">`self.sample_experiences()`</span> instead of training based on the full set of experiences. Otherwise, the neural net is trained exactly the same way as it did for the tic-tac-toe example. Although not strictly part of this function, I also switched from a stochastic gradient decent optimizer to an Adam optimizer and decreased the learning rate to 0.001 in the <span class="in">`__init__`</span> function.</span>
<span id="cb605-98"><a href="#cb605-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-101"><a href="#cb605-101" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb605-102"><a href="#cb605-102" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DQNagent:</span>
<span id="cb605-103"><a href="#cb605-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-104"><a href="#cb605-104" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, state_size, action_size, episodes):</span>
<span id="cb605-105"><a href="#cb605-105" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gamma <span class="op">=</span> <span class="fl">0.95</span></span>
<span id="cb605-106"><a href="#cb605-106" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb605-107"><a href="#cb605-107" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.state_size <span class="op">=</span> state_size</span>
<span id="cb605-108"><a href="#cb605-108" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.action_size <span class="op">=</span> action_size</span>
<span id="cb605-109"><a href="#cb605-109" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.episodes <span class="op">=</span> episodes</span>
<span id="cb605-110"><a href="#cb605-110" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.replay_memory <span class="op">=</span> deque(maxlen<span class="op">=</span><span class="dv">2000</span>)</span>
<span id="cb605-111"><a href="#cb605-111" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.optimizer <span class="op">=</span> tf.keras.optimizers.Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb605-112"><a href="#cb605-112" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.loss_fn <span class="op">=</span> tf.keras.losses.mean_squared_error</span>
<span id="cb605-113"><a href="#cb605-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-114"><a href="#cb605-114" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build_model(<span class="va">self</span>):</span>
<span id="cb605-115"><a href="#cb605-115" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> tf.keras.models.Sequential([</span>
<span id="cb605-116"><a href="#cb605-116" aria-hidden="true" tabindex="-1"></a>            tf.keras.layers.Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">"relu"</span>, input_shape<span class="op">=</span>[<span class="va">self</span>.state_size]),</span>
<span id="cb605-117"><a href="#cb605-117" aria-hidden="true" tabindex="-1"></a>            tf.keras.layers.Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb605-118"><a href="#cb605-118" aria-hidden="true" tabindex="-1"></a>            tf.keras.layers.Dense(<span class="va">self</span>.action_size)</span>
<span id="cb605-119"><a href="#cb605-119" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb605-120"><a href="#cb605-120" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> model</span>
<span id="cb605-121"><a href="#cb605-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-122"><a href="#cb605-122" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> add_memory(<span class="va">self</span>, state, action, reward, next_state, done):</span>
<span id="cb605-123"><a href="#cb605-123" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.replay_memory.append((state, action, reward, next_state, done))</span>
<span id="cb605-124"><a href="#cb605-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-125"><a href="#cb605-125" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample_experiences(<span class="va">self</span>):</span>
<span id="cb605-126"><a href="#cb605-126" aria-hidden="true" tabindex="-1"></a>        indices <span class="op">=</span> np.random.randint(<span class="bu">len</span>(<span class="va">self</span>.replay_memory), size<span class="op">=</span><span class="va">self</span>.batch_size)</span>
<span id="cb605-127"><a href="#cb605-127" aria-hidden="true" tabindex="-1"></a>        batch <span class="op">=</span> [<span class="va">self</span>.replay_memory[index] <span class="cf">for</span> index <span class="kw">in</span> indices]</span>
<span id="cb605-128"><a href="#cb605-128" aria-hidden="true" tabindex="-1"></a>        states, actions, rewards, next_states, dones <span class="op">=</span> [</span>
<span id="cb605-129"><a href="#cb605-129" aria-hidden="true" tabindex="-1"></a>            np.array([experience[field_index] <span class="cf">for</span> experience <span class="kw">in</span> batch])</span>
<span id="cb605-130"><a href="#cb605-130" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> field_index <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>)]</span>
<span id="cb605-131"><a href="#cb605-131" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> states, actions, rewards, next_states, dones</span>
<span id="cb605-132"><a href="#cb605-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-133"><a href="#cb605-133" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train_model(<span class="va">self</span>, model):</span>
<span id="cb605-134"><a href="#cb605-134" aria-hidden="true" tabindex="-1"></a>        states, actions, rewards, next_states, dones <span class="op">=</span> <span class="va">self</span>.sample_experiences()</span>
<span id="cb605-135"><a href="#cb605-135" aria-hidden="true" tabindex="-1"></a>        next_Q_values <span class="op">=</span> model.predict(next_states, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb605-136"><a href="#cb605-136" aria-hidden="true" tabindex="-1"></a>        max_next_Q_values <span class="op">=</span> np.<span class="bu">max</span>(next_Q_values, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb605-137"><a href="#cb605-137" aria-hidden="true" tabindex="-1"></a>        target_Q_values <span class="op">=</span> (rewards <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> dones) <span class="op">*</span> <span class="va">self</span>.gamma <span class="op">*</span> max_next_Q_values)</span>
<span id="cb605-138"><a href="#cb605-138" aria-hidden="true" tabindex="-1"></a>        target_Q_values <span class="op">=</span> target_Q_values.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb605-139"><a href="#cb605-139" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> tf.one_hot(actions, <span class="va">self</span>.action_size)</span>
<span id="cb605-140"><a href="#cb605-140" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> tf.GradientTape() <span class="im">as</span> tape:</span>
<span id="cb605-141"><a href="#cb605-141" aria-hidden="true" tabindex="-1"></a>            all_Q_values <span class="op">=</span> model(states)</span>
<span id="cb605-142"><a href="#cb605-142" aria-hidden="true" tabindex="-1"></a>            Q_values <span class="op">=</span> tf.reduce_sum(all_Q_values <span class="op">*</span> mask, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb605-143"><a href="#cb605-143" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> tf.reduce_mean(<span class="va">self</span>.loss_fn(target_Q_values, Q_values))</span>
<span id="cb605-144"><a href="#cb605-144" aria-hidden="true" tabindex="-1"></a>        grads <span class="op">=</span> tape.gradient(loss, model.trainable_variables)</span>
<span id="cb605-145"><a href="#cb605-145" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.optimizer.apply_gradients(<span class="bu">zip</span>(grads, model.trainable_variables))</span>
<span id="cb605-146"><a href="#cb605-146" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb605-147"><a href="#cb605-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-148"><a href="#cb605-148" aria-hidden="true" tabindex="-1"></a><span class="fu">## Let The Training Begin</span></span>
<span id="cb605-149"><a href="#cb605-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-150"><a href="#cb605-150" aria-hidden="true" tabindex="-1"></a>This problem kicked my butt for quite a while. I *could not* get that stupid car up the hill! It seemed so simple, but I was stumped. I actually solved this early on with a simple hard-coded policy: if the car is moving left (a negative velocity), push left. Otherwise, push right. Done. Didn't even need a neural net, q-learning, or anything. So why bother with solving this with reinforcement learning? I guess in the real world I wouldn't, but the point here is to see if I can train a neural net (NN) to learn that policy.</span>
<span id="cb605-151"><a href="#cb605-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-152"><a href="#cb605-152" aria-hidden="true" tabindex="-1"></a><span class="fu">### I Tried A Lot Of Things That Didn't Work</span></span>
<span id="cb605-153"><a href="#cb605-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-154"><a href="#cb605-154" aria-hidden="true" tabindex="-1"></a>First, I left everything as is, and just fed experiences to the NN hoping it would learn. Recall that each episode consists of 200 steps, and so you get a -1 reward for each step for a total reward of -200. If the car reaches the goal in say 150 steps, then you get a total reward of -150. My idea here was that the NN would learn to maximize the total reward by finishing in under 200 steps. Nope. Never happened. The car never got to the goal during training and so the NN never learned anything.</span>
<span id="cb605-155"><a href="#cb605-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-156"><a href="#cb605-156" aria-hidden="true" tabindex="-1"></a>Then I started thinking about the reward system. It seemed like I needed to give the NN some positive reward to encourage it along. I tried giving a small positive reward if the car made it some distance away from the range of possible starting points. That ended up teaching the NN to drive up and down the same side of the hill over and over. Clearly, the statement, "the only way to succeed is to drive back and forth to build up momentum" is correct, but how do I do that? I tried a one-time positive reward on one side of the hill and only gave additional rewards if the car then tried going up the other side of the hill. That didn't work, either. I tried giving a huge reward if the car ever made it to the goal position, hoping that would back-propagate quickly through the NN weights. Still nope.</span>
<span id="cb605-157"><a href="#cb605-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-158"><a href="#cb605-158" aria-hidden="true" tabindex="-1"></a>It seemed like this reward system I was creating was getting a lot more complicated that it should need to be, so then I tried all of the above while at the same time adjusting the tuning parameters: learning rate, discount rate, and epsilon. No luck. I tried fiddling with how the NN was constructed: number of hidden layers, nodes, etc. Nope. I also tried changing the replay memory batch size. I even took memory replay completely out of the algorithm thinking that maybe that rare time the car made it to the goal kept getting missed in the random draw from the memory replay. Nope again.</span>
<span id="cb605-159"><a href="#cb605-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-160"><a href="#cb605-160" aria-hidden="true" tabindex="-1"></a><span class="fu">### What Did Work</span></span>
<span id="cb605-161"><a href="#cb605-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-162"><a href="#cb605-162" aria-hidden="true" tabindex="-1"></a>It dawned on me that maybe I should just come up with a reward system that mimicked the hard-coded policy I described earlier. Finally, that worked! In the code below, you'll see:</span>
<span id="cb605-163"><a href="#cb605-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-164"><a href="#cb605-164" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`if next_state[0] - state[0] &gt; 0 and action == 2: reward = 1`</span> moving right and pushing right = a reward</span>
<span id="cb605-165"><a href="#cb605-165" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`if next_state[0] - state[0] &lt; 0 and action == 0: reward = 1`</span> moving left and pushing left = a reward</span>
<span id="cb605-166"><a href="#cb605-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-167"><a href="#cb605-167" aria-hidden="true" tabindex="-1"></a>The following code trans the NN for 600 episodes and keeps track of the best score. I define the score as the number of steps it takes to reach the goal, so the fewer, the better. When a new best score is reached, I save the model weights as <span class="in">`best_weights`</span>. After the 600th episode is over, I reset the model weights with the best weights. Why? It turns out that learning is hard, and forgetting is easy, which I'll demonstrate shortly.</span>
<span id="cb605-168"><a href="#cb605-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-171"><a href="#cb605-171" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb605-172"><a href="#cb605-172" aria-hidden="true" tabindex="-1"></a>best_score <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb605-173"><a href="#cb605-173" aria-hidden="true" tabindex="-1"></a>episodes <span class="op">=</span> <span class="dv">600</span></span>
<span id="cb605-174"><a href="#cb605-174" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">'MountainCar-v0'</span>)</span>
<span id="cb605-175"><a href="#cb605-175" aria-hidden="true" tabindex="-1"></a>state_size <span class="op">=</span> env.observation_space.shape[<span class="dv">0</span>]</span>
<span id="cb605-176"><a href="#cb605-176" aria-hidden="true" tabindex="-1"></a>action_size <span class="op">=</span> env.action_space.n</span>
<span id="cb605-177"><a href="#cb605-177" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> DQNagent(state_size, action_size, episodes)</span>
<span id="cb605-178"><a href="#cb605-178" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> agent.build_model()</span>
<span id="cb605-179"><a href="#cb605-179" aria-hidden="true" tabindex="-1"></a>rewards <span class="op">=</span> []</span>
<span id="cb605-180"><a href="#cb605-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-181"><a href="#cb605-181" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(episodes):</span>
<span id="cb605-182"><a href="#cb605-182" aria-hidden="true" tabindex="-1"></a>    state <span class="op">=</span> env.reset()[<span class="dv">0</span>]</span>
<span id="cb605-183"><a href="#cb605-183" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">200</span>):</span>
<span id="cb605-184"><a href="#cb605-184" aria-hidden="true" tabindex="-1"></a>        epsilon <span class="op">=</span> <span class="bu">max</span>(<span class="dv">1</span> <span class="op">-</span> episode<span class="op">/</span>(episodes<span class="op">*</span><span class="fl">0.8</span>), <span class="fl">0.01</span>)</span>
<span id="cb605-185"><a href="#cb605-185" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.random.rand() <span class="op">&lt;</span> epsilon: action <span class="op">=</span> np.random.randint(action_size)</span>
<span id="cb605-186"><a href="#cb605-186" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>: action <span class="op">=</span> np.argmax(model.predict(state[np.newaxis], verbose <span class="op">=</span> <span class="dv">0</span>)[<span class="dv">0</span>])</span>
<span id="cb605-187"><a href="#cb605-187" aria-hidden="true" tabindex="-1"></a>        next_state, reward, done, info, empty <span class="op">=</span> env.step(action)</span>
<span id="cb605-188"><a href="#cb605-188" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> next_state[<span class="dv">0</span>] <span class="op">-</span> state[<span class="dv">0</span>] <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> action <span class="op">==</span> <span class="dv">2</span>: reward <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb605-189"><a href="#cb605-189" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> next_state[<span class="dv">0</span>] <span class="op">-</span> state[<span class="dv">0</span>] <span class="op">&lt;</span> <span class="dv">0</span> <span class="kw">and</span> action <span class="op">==</span> <span class="dv">0</span>: reward <span class="op">=</span> <span class="dv">1</span>        </span>
<span id="cb605-190"><a href="#cb605-190" aria-hidden="true" tabindex="-1"></a>        agent.add_memory(state, action, reward, next_state, done)</span>
<span id="cb605-191"><a href="#cb605-191" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> next_state.copy()</span>
<span id="cb605-192"><a href="#cb605-192" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> done:</span>
<span id="cb605-193"><a href="#cb605-193" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb605-194"><a href="#cb605-194" aria-hidden="true" tabindex="-1"></a>    rewards.append(step)</span>
<span id="cb605-195"><a href="#cb605-195" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">&lt;</span> best_score:</span>
<span id="cb605-196"><a href="#cb605-196" aria-hidden="true" tabindex="-1"></a>        best_weights <span class="op">=</span> model.get_weights()</span>
<span id="cb605-197"><a href="#cb605-197" aria-hidden="true" tabindex="-1"></a>        best_score <span class="op">=</span> step</span>
<span id="cb605-198"><a href="#cb605-198" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\r</span><span class="st">Episode: </span><span class="sc">{}</span><span class="st">, Best Score: </span><span class="sc">{}</span><span class="st">, eps: </span><span class="sc">{:.3f}</span><span class="st">"</span>.<span class="bu">format</span>(episode, best_score, epsilon), end<span class="op">=</span><span class="st">""</span>)</span>
<span id="cb605-199"><a href="#cb605-199" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> episode <span class="op">&gt;</span> <span class="dv">50</span>:</span>
<span id="cb605-200"><a href="#cb605-200" aria-hidden="true" tabindex="-1"></a>        agent.train_model(model)</span>
<span id="cb605-201"><a href="#cb605-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-202"><a href="#cb605-202" aria-hidden="true" tabindex="-1"></a>model.set_weights(best_weights)</span>
<span id="cb605-203"><a href="#cb605-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-204"><a href="#cb605-204" aria-hidden="true" tabindex="-1"></a>env.close()</span>
<span id="cb605-205"><a href="#cb605-205" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb605-206"><a href="#cb605-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-207"><a href="#cb605-207" aria-hidden="true" tabindex="-1"></a>In the figure below, you can see that it took about 330 episodes for the car to reach the goal for the first time. As training progressed, the score improved for a short time but then went back to 200. That happened a number of times, too. This is what Aurélien Géron describes as "catastrophic forgetting", which makes me laugh but describes it perfectly.</span>
<span id="cb605-208"><a href="#cb605-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-211"><a href="#cb605-211" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb605-212"><a href="#cb605-212" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb605-213"><a href="#cb605-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-214"><a href="#cb605-214" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.line(x<span class="op">=</span>[i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(rewards))], y<span class="op">=</span>rewards)</span>
<span id="cb605-215"><a href="#cb605-215" aria-hidden="true" tabindex="-1"></a>fig.update_layout(xaxis_title<span class="op">=</span><span class="st">'Episode'</span>, yaxis_title<span class="op">=</span><span class="st">'Score'</span>)</span>
<span id="cb605-216"><a href="#cb605-216" aria-hidden="true" tabindex="-1"></a>fig.show()</span>
<span id="cb605-217"><a href="#cb605-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-218"><a href="#cb605-218" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb605-219"><a href="#cb605-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-220"><a href="#cb605-220" aria-hidden="true" tabindex="-1"></a>According to the documentation for this environment, MountainCar-v0 is considered "solved" when the agent obtains an average reward of at least -110.0 over 100 consecutive episodes. That would translate into a score of 110 or less over 100 episodes. The best score here was 82, and looking at the graph, the NN didn't maintain anything less than 200 very long, so there's still work to do. I'll go with what I have for now and demonstrate how the NN does visually.</span>
<span id="cb605-221"><a href="#cb605-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-222"><a href="#cb605-222" aria-hidden="true" tabindex="-1"></a>Next, I'll trained the model for 2000 episodes. I'm not going to re-run that code here because it took hours, and I've also been having an issue with Python kernel dying while rendering this page that I haven't overcome yet. Instead, below is a plot of the results from when I originally trained the model.</span>
<span id="cb605-223"><a href="#cb605-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-224"><a href="#cb605-224" aria-hidden="true" tabindex="-1"></a><span class="al">![Results after training for 2000 episodes.](2000_episodes.png)</span>{width="600"}</span>
<span id="cb605-225"><a href="#cb605-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb605-226"><a href="#cb605-226" aria-hidden="true" tabindex="-1"></a>This is much better than the previous results! The NN held a sub-200 score for quite a few episodes towards the end of training. However, the best score wasn't consistently below 110, so there's still more work to do. There is no incentive for reaching to goal quickly, so maybe I can get better results by adding a large reward for finishing in fewer than 110 steps. The problem with that approach is that training is conducted on a random sampling of 64 experiences out of the 2000 that are in the replay memory deque. Sub-110 scores are rare, so the chances of that reward making it into the training set is remote. Probably a better approach is to improve the NN performance so that sub-110 scores are more common. That might be possible by playing around with tuning parameters or maybe switching to a different type of reinforcement learning method like a Dueling Deep Q-Network or a Double Dueling Deep Q-Network. That's for another post, though.</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Copyright 2022, John King</div>   
  </div>
</footer>



</body></html>