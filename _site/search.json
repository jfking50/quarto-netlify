[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "A Random Walk",
    "section": "",
    "text": "B.A., German/Math, Willamette University, Oregon\nB.S., Geology, Portland State University, Oregon\nM.S., Geology, Portland State University, Oregon\nM.S., Operations Research, Naval Postgraduate School, California\n\n\n\n\n\nKing, John F., Magmatic Evolution and Eruptive History of the Granitic Bumping Lake Pluton, Washington: Source of the Bumping River and Cash Prairie Tuffs (1994). Dissertations and Theses. Paper 4765.\nKing, John, Major, U.S. Army, An Implementation of Designed Experiments in Combat XXI, M.S. in Operations Research, June, 2018 (RESTRICTED).\n\n\n\n\n\n\n\n\nDALL-E: A happy old Geologist in a dark pub typing on a laptop, digital art."
  },
  {
    "objectID": "about.html#travels",
    "href": "about.html#travels",
    "title": "A Random Walk",
    "section": "Travels",
    "text": "Travels\nIf it’s dark blue, I did stuff there. If medium blue, then just an airport layover. It’s Interactive!"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "A Random Walk",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nTripeaks Solver\n\n\n\n\n\n\n\nR\n\n\nData Visualization\n\n\nggplot2\n\n\n\n\nA stochastic method for solving tripeaks games.\n\n\n\n\n\n\nJul 2, 2022\n\n\nJohn King\n\n\n\n\n\n\n  \n\n\n\n\nMaze Generator\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\ngenerative art\n\n\n\n\nA stochastic method for generating mazes.\n\n\n\n\n\n\nMay 29, 2022\n\n\nJohn King\n\n\n\n\n\n\n  \n\n\n\n\nFlow Fields\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\ngenerative art\n\n\n\n\nManual and algorithm-based methods for generating flow fields.\n\n\n\n\n\n\nMay 26, 2022\n\n\nJohn King\n\n\n\n\n\n\n  \n\n\n\n\nKNN Art\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\ngenerative art\n\n\nmachine learning\n\n\n\n\nGemstones and mosaics based on k-nearest neighbors.\n\n\n\n\n\n\nMay 22, 2022\n\n\nJohn King\n\n\n\n\n\n\n  \n\n\n\n\nSpace Colonization\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\ngenerative art\n\n\n\n\nOrganic looking life forms from math.\n\n\n\n\n\n\nMay 8, 2022\n\n\nJohn King\n\n\n\n\n\n\n  \n\n\n\n\nRecaman Sequence\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\ngenerative art\n\n\n\n\nOrganic structures from number sequences.\n\n\n\n\n\n\nMay 2, 2022\n\n\nJohn King\n\n\n\n\n\n\n  \n\n\n\n\nCollatz Conjecture\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\ngenerative art\n\n\n\n\nOrganic structures from number sequences.\n\n\n\n\n\n\nMay 1, 2022\n\n\nJohn King\n\n\n\n\n\n\n  \n\n\n\n\nWordle\n\n\n\n\n\n\n\nR\n\n\n\n\nA method to find the best Wordle starting word.\n\n\n\n\n\n\nMar 29, 2022\n\n\nJohn King\n\n\n\n\n\n\n  \n\n\n\n\nAdjusted Statistics\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\ncollege football\n\n\nsports analytics\n\n\n\n\nAdjusted statistics for college football teams based on strength of schedule.\n\n\n\n\n\n\nMar 20, 2022\n\n\nJohn King\n\n\n\n\n\n\n  \n\n\n\n\nPredictive Modeling\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\ncollege football\n\n\nsports analytics\n\n\nmachine learning\n\n\n\n\nA variety of machine learning models to predict beating the spread.\n\n\n\n\n\n\nMar 12, 2022\n\n\nJohn King\n\n\n\n\n\n\n  \n\n\n\n\nSimple Rating System\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\ncollege football\n\n\nsports analytics\n\n\n\n\nHow football simple rating systems work. One sentence at a time.\n\n\n\n\n\n\nFeb 14, 2022\n\n\nJohn King\n\n\n\n\n\n\n  \n\n\n\n\nOregon Football 2021\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\ncollege football\n\n\nsports analytics\n\n\n\n\nA look at Oregon football at the end of the 2021 season.\n\n\n\n\n\n\nFeb 13, 2022\n\n\nJohn King\n\n\n\n\n\n\n  \n\n\n\n\nResolution 5 Fractional Factorial Designs\n\n\n\n\n\n\n\nR\n\n\ndesign of experiments\n\n\n\n\nUse of a Walsh matrix to generate an experimental design.\n\n\n\n\n\n\nMay 21, 2021\n\n\nJohn King\n\n\n\n\n\n\n  \n\n\n\n\nDeep Q-Networks\n\n\n\n\n\n\n\npython\n\n\nreinforcement learning\n\n\nAI\n\n\ntensorflow\n\n\nneural network\n\n\n\n\nTrain an AI agent to play tic-tac-toe.\n\n\n\n\n\n\nApr 10, 2020\n\n\nJohn King\n\n\n\n\n\n\n  \n\n\n\n\nQ Learning\n\n\n\n\n\n\n\npython\n\n\nreinforcement learning\n\n\n\n\nTrain an agent to play tic-tac-toe without using a neural network.\n\n\n\n\n\n\nApr 5, 2020\n\n\nJohn King\n\n\n\n\n\n\n  \n\n\n\n\nSubstitution Cyphers\n\n\n\n\n\n\n\npython\n\n\ncryptology\n\n\nstochastic methods\n\n\n\n\nDecrypt a substitution cypher using the Markov Chain Monte Carlo method.\n\n\n\n\n\n\nMar 30, 2020\n\n\nJohn King\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "A Random Walk",
    "section": "Welcome",
    "text": "Welcome\nThis site contains R and Python scripts for data analysis, data visualization, machine learning, and miscellaneous other projects. It’s primarily meant to be a try things out and be a repository of useful tidbids of code."
  },
  {
    "objectID": "index.html#examples",
    "href": "index.html#examples",
    "title": "A Random Walk",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "tutorial/advanced.html",
    "href": "tutorial/advanced.html",
    "title": "Advanced Designs",
    "section": "",
    "text": "So far, we have considered designs with just two levels per factor, which is fine for inherently binary factors. However, if our study requires us to include a factor that can take on more than two values, or even a continuous range of values, or if we suspect that the relationship between a predictor and a response is non-linear, we need a different experimental design."
  },
  {
    "objectID": "tutorial/advanced.html#introduction-and-background",
    "href": "tutorial/advanced.html#introduction-and-background",
    "title": "Advanced Designs",
    "section": "Introduction and Background",
    "text": "Introduction and Background\nIn this chapter, we will design experiments that can accommodate factors with three or more levels (up to continuous). The designs presented in this chapter belong to two broad categories: central composite designs (CCD) and nearly orthogonal Latin hypercube (NOLH) designs. These designs are particularly useful if we find ourselves in one (or both) of the following situations:\n\nWe suspect there may be non-linear relationships between one or more predictors and the response variable.\nThe levels of one or more predictors are not well known or clearly defined. This is often the case when developing requirements for a future system. For example, perhaps the Army is considering developing a new helicopter, and the developers are interested in identifying the optimal combination of speed, fuel capacity, range, and various weapon and sensor systems.\n\nFor background, please read the following:\n\nFrom Work Smarter, Not Harder: A Tutorial On Designing And Conducting Simulation Experiments , skim Sections 1 and 2, and then carefully read Sections 3, 4, and 5.\nFrom A User’s Guide to the Brave New World of Designing Simulation Experiments, read Sections 2.2 - 2.4, and all of Sections 3 and 4. Figure 1 is a nice visual guide for design selection. Note that the authors prefer designs towards the top of the figure.\n\nFrom the readings, you can see that there is an entire field of study regarding design generation and that the properties of the various designs make them well or poorly suited for a given study. The authors of both articles discussed gridded designs, which are simple to construct and allow for the evaluation of non-linear relationships. The main drawback of gridded designs, especially when it comes to applying them to combat simulations, is that they are very inefficient. For example, a gridded design for 10 factors at 4 levels each requires \\(4^{10} = 1,048,576\\) design points, and due to this property we will not consider them further for AWARS applications. Instead, we’ll focus on CCDs and NOLH-based designs."
  },
  {
    "objectID": "tutorial/advanced.html#central-composite-designs",
    "href": "tutorial/advanced.html#central-composite-designs",
    "title": "Advanced Designs",
    "section": "Central Composite Designs",
    "text": "Central Composite Designs\nTo detect non-linearity in the response, at a minimum, we need to add a center point to a factorial design. One option is to add a single center point that is shared by multiple factors to minimize the number of runs. Using plotly to plot in three dimensions, this design is visualized as follows (this is an interactive plot, so click and drag the plot to rotate the points):\n\nlibrary(tidyverse)\nlibrary(plotly)\n\ncenterPoint = tibble(\n  x1 = c(rep(c(-1, 1), 4), 0),\n  x2 = c(rep(c(-1, -1, 1, 1), 2), 0),\n  x3 = c(rep(c(-1, 1), each=4), 0),\n  center = c(rep(\"y\", 8), \"n\")\n)\n\nplot_ly() %>%\n  add_trace(data = centerPoint, x = ~x1, y = ~x2, z = ~x3,\n            color=~center, \n            colors = c('#0C4B8E', '#BF382A'), \n            type='scatter3d', mode='markers') %>%\n  layout(title = 'CCD With Center Point', showlegend = FALSE,\n         scene = list(camera = list(eye = list(x = -1.0, y = 1.25, z = 0.3))))\n\n\n\n\n\nThis design projected on to any of the 2D faces of the cube appears as:\n\nplot_ly() %>%\n  add_trace(data = centerPoint, x = ~x1, y = ~x2,\n            color=~center, \n            colors = c('#0C4B8E', '#BF382A'), \n            type='scatter', mode='markers', size=10) %>%\n  layout(title = 'CCD With Center Point', showlegend = FALSE, \n         xaxis=list(zeroline=F), yaxis=list(zeroline=F))\n\n\n\n\n\nTwo considerations with this design:\n\nThis design contains the minimum number of design points to identify non-linear relationships. If no non-linear relationships exist, we simply continue with the modeling methods presented earlier in this tutorial.\nIf one or more non-linear relationships exist, this design does not allow you to determine which factors are non-linear.\n\nTo address #2, we need to add a point to each face of the cube, which are collectively referred to as star points. You can also think of this design as starting with a gridded design and removing unnecessary design points to make it more efficient. For comparison, a 3-factor, 3-level gridded design has \\(3^{3} = 27\\) design points compared to 15 design points for a CCD (8 for the corners and 7 for the star). Below is the CCD with star points shown in blue and connected with black lines.\n\nccdGrid = tibble(\n  x1 = c(c(-1,1,0,0,0,0,0), rep(c(-1,1), 4)),\n  x2 = c(c(0,0,-1,1,0,0,0), rep(c(-1,1,-1,1), each = 2)),\n  x3 = c(c(0,0,0,0,-1,1,0), rep(-1,4), rep(1,4)),\n  star = c(rep('y', 7), rep('n',8)),\n  line = c(rep('line1',2), rep('line2',2), rep('line3',2), rep('none',9))\n)\n\nplot_ly() %>%\n  add_trace(data = ccdGrid, x = ~x1, y = ~x2, z = ~x3,\n            color=~star, \n            colors = c('#BF382A', '#0C4B8E'), \n            type='scatter3d', mode='markers') %>%\n  add_trace(data = ccdGrid %>% filter(line=='line1'), \n            x = ~x1, y = ~x2, z = ~x3,\n            line = list(color = 'black', width = 2),\n            type = 'scatter3d', mode='lines') %>%\n  add_trace(data = ccdGrid %>% filter(line=='line2'), \n            x = ~x1, y = ~x2, z = ~x3,\n            line = list(color = 'black', width = 2),\n            type = 'scatter3d', mode='lines') %>%\n  add_trace(data = ccdGrid %>% filter(line=='line3'), \n            x = ~x1, y = ~x2, z = ~x3,\n            line = list(color = 'black', width = 2),\n            type = 'scatter3d', mode='lines')%>%\n  layout(title = 'Central Composite Design', showlegend = FALSE,\n         scene = list(camera = list(eye = list(x = -1.0, y = 1.25, z = 0.3))))\n\n\n\n\n\n\nAugmented Central Composite Designs\nThe design generated above can be used to identify several kinds of non-linear relationships, \\(log(x)\\), \\(e^x\\), \\(1/x\\), and \\(x^2\\). It can not, however, be used to identify trends with more than one bend in the curve (e.g., \\(x^3\\)). The CCD generated above can be further augmented with additional design points to either capture higher order relationships or to obtain additional data for use in response surface methodology (presented later in this section). Additionally, since these designs are increasing in complexity, we will make use of the rsm package to generate them.\nTo get a feel for the rsm package, we’ll re-generate the CCD with the star points as in the previous section. According to the documentation, the preferred method for generating this design is to follow a two-step process. First, we generate the corner points with cube().\n\nlibrary(rsm)\n\ncu = cube(3, n0=0) # 3 factors and 0 center points\ncu\n\n\n\n  \n\n\n\nThen we combine the cube points using djoin() with the star points using star()\n\n# alpha = 1 generates the \"face\" points, n0 gives the central point\nccd = djoin(cu, star(alpha=1, n0=1)) \nccd\n\n\n\n  \n\n\n\nPlotting this design demonstrates that it’s equivalent to what we produced manually.\n\nplot_ly() %>%\n  add_trace(data = ccd, x = ~x1, y = ~x2, z = ~x3, \n            color=~Block, colors = c('#BF382A', '#0C4B8E'), \n            type='scatter3d', mode='markers')%>%\n  layout(title = 'rsm Central Composite Design', showlegend = FALSE,\n         scene = list(camera = list(eye = list(x = -1.0, y = 1.25, z = 0.3))))\n\n\n\n\n\nAn interesting feature of the CCD we just generated is that we can modify it slightly to accommodate additional factor levels without increasing the number of design points. We can do this by moving the corner points closer to the central point. In a spherical design as shown below, the corner point at \\((1 ,1)\\) moves to \\((1/\\sqrt{k}, 1/\\sqrt{k})\\) where \\(k\\) is the number of factors in the design. Since \\(1/\\sqrt{3} = 0.577\\), the new point becomes \\((0.577, 0.577)\\).\n\ncu2 = cube(3, n0=0, inscribed = TRUE)  # inscribed limits the axis points to +/- 1\nccd_s = djoin(cu2, star(alpha=\"spherical\", n0=1))\n\npt <- data.frame(x =0.6, y = 0.6)\n\nplot_ly() %>%\n  add_trace(data = ccd_s, x = ~x1, y = ~x2,\n            color=~Block, colors = c('#BF382A', '#0C4B8E'), \n            type='scatter', mode='markers', size =10) %>%\n  add_annotations(data=pt, ax=1, ay=1, axref=\"x\", ayref=\"y\", x=pt$x, y=pt$y, text=\"Moved Point\") %>%\n  layout(title = 'Spherical Design', showlegend = FALSE,\n    xaxis=list(zeroline=F), yaxis=list(zeroline=F))\n\n\n\n\n\nBelow, we see that the design still consists of 15 design points, and each factor has levels of -1.0, -0.58, 0, 0.58, and 1.0. Clearly, this is much more efficient than a gridded design, which would have \\(5^3 = 125\\) design points.\n\nccd_s\n\n\n\n  \n\n\n\nNotice that although the design consists of five levels for each factor, the number of design points at each level varies. For example, x1 at levels -1 and 1 are only represented once each in the design matrix, whereas x1 at level 0 is represented 5 times. Therefore, if multiple measurements at the extreme high and low ranges of the factor ranges is important, a standard CCD is a better choice than a spherical design. Alternatively, the spherical design can be replicated using the dupe() function to provide an additional design point at the extreme high and low factor levels.\n\ndjoin(ccd_s, dupe(ccd_s))"
  },
  {
    "objectID": "tutorial/advanced.html#response-surface-methodology",
    "href": "tutorial/advanced.html#response-surface-methodology",
    "title": "Advanced Designs",
    "section": "Response Surface Methodology",
    "text": "Response Surface Methodology\nAs mentioned in the introduction, suppose we are interested in assisting with developing requirements for a future Army helicopter. For simplicity, say we’re only interested in determining how speed, stealth, and sensor range contribute to lethality (measured in the number of kills achieved by the helicopter). One can imagine that a low amount of stealth might result in few kills. As stealth increases, we would expect the number of kills to increase, but it may also be that increasing stealth beyond some threshold might begin to reduce the number of kills - perhaps the pilot spends so much effort remaining stealthy, that it becomes difficult to detect and engage targets. In this case, there is some optimal combination stealth, speed and sensor range that produces the greatest number of kills, and this optimal combination is not the maximum levels of each factor. Response surface methodology in conjunction with efficient experimental design can be applied in a situation like this to identify the optimal combination of factor levels.\nFor our example, we will consider the following ranges of predictor values:\n\nSpeed ranges from 100 to 300 km/hr.\nStealth ranges from 0, which represents no stealth, to 1, which represents full ninja.\nSensor detection ranges from 5 to 15 km.\n\nResponse surface methodology involves plotting pairs of factors on the x and y axes and the response on the z axis. We can accomplish this using either a contour or a 3D plot. Since we don’t know what the optimal factor values are, ideally, we’d like to evaluate as many factor value combinations as possible. One consideration when choosing a design for this purpose is the design’s space-filling properties, which is demonstrated in the following plots using just speed and stealth. From left to right, the space-filling properties of the designs improve, which means that if the optimal values for speed and stealth are 261 and 0.81 (shown in red on the plots), respectively, then the design on the right will most accurately identify these optimal values.\n\nss3 = expand_grid(speed=seq(100, 300, 100), stealth=seq(0, 1, 0.5))\nss5 = expand_grid(speed=seq(100, 300, 50), stealth=seq(0, 1, 0.25))\nss10= expand_grid(speed=seq(100, 300, 20), stealth=seq(0, 1, 0.1))\n\nget_plot = function(df){\n  plot_ly() %>%\n  add_trace(data = df, x=~speed, y=~stealth, type='scatter', mode='markers', \n            marker=list(color='black', size=5), showlegend=FALSE) %>%\n  add_trace(x=261, y=0.81, type='scatter', mode='markers', \n            marker=list(color='red', size=5), showlegend=FALSE)\n}\n\nf1 = get_plot(ss3)\nf2 = get_plot(ss5)\nf3 = get_plot(ss10)\n\nsubplot(f1, f2, f3, heights=0.5, shareY = TRUE, shareX=TRUE) %>%\n  layout(xaxis=list(zeroline=F), yaxis=list(zeroline=F))\n\n\n\n\n\nIf we were to use the designs that generated for the above three plots in a simulation, then we could measure the response variable for each factor value combination to produce the following three response surfaces.\n\nget_kills = function(df){\n  df %>% mutate(kills = case_when(\n  stealth<=0.81 & speed<=261 ~ 3*stealth + speed/100, \n  stealth<=0.81 & speed>261 ~600/speed,\n  stealth>0.81 & speed>261 ~ stealth/2 + 400/speed,\n  stealth>0.81 & speed<261 ~ stealth/2 + speed/100))\n}\n\nss3 = get_kills(ss3)\nss5 = get_kills(ss5)\nss10 = get_kills(ss10)\n\nget_contours = function(df){\n  plot_ly() %>%\n  add_trace(data = df, x=~speed, y=~stealth, z=~kills, type = \"contour\") %>%\n  add_trace(data = df, x=~speed, y=~stealth, type='scatter', mode='markers', \n            marker=list(color='black', size=5), showlegend=FALSE) %>%\n  add_trace(x=261, y=0.81, type='scatter', mode='markers', \n            marker=list(color='red', size=5), showlegend=FALSE)\n}\n\nf1 = get_contours(ss3)\nf2 = get_contours(ss5)\nf3 = get_contours(ss10)\n\nsubplot(f1, f2, f3, heights=0.5, shareY = TRUE, shareX=TRUE)\n\n\n\n\n\nGiven the above three plots, if you didn’t know the true optimal values for speed and stealth, clearly the plot on the right with the best space-filling properties provides the best estimates. To determine the optimal values from the plot, we simply find the speed and stealth values that result in the maximum number of kills, which appears to be approximately speed = 220 and stealth = 0.70. The 2D contour plot on the right can also be plotted as a 3D surface, as shown below.\n\nkillz = as.matrix(ss10 %>% pivot_wider(names_from = speed, values_from = kills))\n\nplot_ly(z=~killz) %>% \n  add_surface(contours = list(\n    z = list(show = TRUE)), z = ~killz) %>% \n  layout(\n    title = \"3D Response Surface\",\n    scene = list(\n      xaxis = list(title = \"Speed\"),\n      yaxis = list(title = \"Stealth\"),\n      zaxis = list(title = \"Kills\", nticks=10)\n    ))\n\n\n\n\n\nThe designs for the plots above are gridded designs, which we’ve demonstrated to be inefficient. Therefore, we’ll replace the gridded design with a spherical CCD.\nWhen creating the design matrix with the rsm functions, we can convert the default factor codings into the ranges we’re interested in using the formulas shown in the following code chunk. We set the ranges of the factor levels using the coding parameter. The formula (speed-200)/100 centers the speed factor on 200 and varies it by +/- 100.\n\nhelo = cube(3,                        # 3 factors\n            n0=0,                     # no center point to the cube\n            inscribed=TRUE,           # required when using alpha=\"spherical\"\n            coding = list(\n              x1~(speed-200)/100,     # center on 200 and vary by 100\n              x2~(stealth-0.5)/0.5,   # center on 0.5 and vary by 0.5\n              x3~(sensor-10)/5))      # center on 10 and vary by 5\nhelo = djoin(helo,                    # join the cube with the star points\n             star(alpha=\"spherical\",  # brings corner points in\n                  n0=1))              # add a center point to the star\nhelo\n\n\n\n  \n\n\n\nOddly (to me, anyway), this doesn’t preserve the factor names and coded values as shown above. For example, helo$speed doesn’t exist.\n\nhelo$speed    # doesn't exist\n\nNULL\n\nnames(helo)   # what does exist?\n\n[1] \"run.order\" \"std.order\" \"x1\"        \"x2\"        \"x3\"        \"Block\"    \n\n\nTo get the factor names and coded values, we must use decode.data() as shown.\n\nhelo_coded = decode.data(helo)\n\nnames(helo_coded)\n\n[1] \"run.order\" \"std.order\" \"speed\"     \"stealth\"   \"sensor\"    \"Block\"    \n\n\nNow we get the number of kills based on our design and plot it.\n\nccd_results = get_kills(helo_coded)\n\nget_contours(ccd_results) %>% layout(title = \"CCD Response Surface\")"
  },
  {
    "objectID": "tutorial/advanced.html#nearly-orthogonal-latin-hypercube-designs",
    "href": "tutorial/advanced.html#nearly-orthogonal-latin-hypercube-designs",
    "title": "Advanced Designs",
    "section": "Nearly Orthogonal Latin Hypercube Designs",
    "text": "Nearly Orthogonal Latin Hypercube Designs\nIn the CCD section, we saw the benefit of the good space filling properties of gridded designs when using response surface methodology. A benefit of CCDs is that they are much more efficient than gridded designs; however, their space-filling properties are not as good. Latin hypercube-based designs combine the efficiency of CCDs with the space-filling properties of gridded designs. As such, they are well-suited for response surface methodology.\nIf we take a step back for a moment, how might we construct a design matrix that uniformly covers the range of the predictor variables? Why not just select randomly from uniform distributions? This is referred to as a random Latin hypercube.\n\nset.seed(0)\ndm = tibble(\n  x1 = runif(10),\n  x2 = runif(10),\n  x3 = runif(10),\n  x4 = runif(10)\n)\n\npairs(dm)\n\n\n\n\nThat’s not bad. We have a design matrix for 4 factors at 10 levels each, and we only needed 10 design points! However, if we take a closer look, we discover a significant issue.\n\nlibrary(GGally)\n\nsmooth_fn <- function(data, mapping, ...){\n  ggplot(data = data, mapping = mapping) + \n    geom_point() + \n    geom_smooth(formula = y~x, method=lm, fill=\"red\", color=\"red\", se=FALSE, ...)\n}\n\nGGally::ggpairs(dm, lower=list(continuous=smooth_fn), progress=FALSE) + \n  ggtitle(\"Random Latin Hypercube\") +\n  theme_bw()\n\n\n\n\nEven though we randomly selected the values, some factor combinations have a significant amount of correlation. x2 and x3 have a correlation of -0.411, for example. Also notice that factor combination doesn’t have any design points in the upper right corner. Orthogonal and nearly-orthogonal Latin hypercubes were developed to overcome this issue using optimization techniques that minimize correlations and maximize space-filling properties.\nIn this section, we’ll focus on the nearly orthogonal Latin hypercube (NOLH) designs developed by Cioppa and MacCalman. Since NOLH designs efficiently accommodate a large number of factors, they are particularly useful for screening purposes. Excel-based tools for generating Cioppa’s and MacCalman’s designs are available to download from the Naval Postgraduate School’s Simulation Experiments and Efficient Designs (SEED) center.\nA few things to be aware of when considering these designs:\n\nCioppa’s designs are for continuous factors only, although discrete factors can be included if the number of discrete values is at least nine or ten.\nEach of Cioppa’s designs can accommodate up to the number of stated factors, so you can use fewer factors if needed. (e.g., the 17-point design can accommodate anywhere from 1 to 7 factors). However, you cannot remove design points.\nCioppa’s designs are not intended to be used to study quadratic effects or interactions. If you need to include these terms, use MacCalman’s designs instead.\nMacCalman’s designs can accommodate continuous, discrete, binary, and categorical variables.\nWith MacCalman’s designs, you *can specify the number of design points*.\nMacCalman’s designs nearly guarantee that all first and second order terms are not confounded with others. Second order effects include both two-way interactions and quadratic effects.\n\n\nFactor Codings\nThe tools for generating Cioppa’s and MacCalman’s designs provide a user interface for naming factors, setting minimum and maximum factor values, and specifying the number of decimal places. The Excel tools both generate the resulting design as a .csv file, which you can then read into R for further use. In my experience, generating a design matrix for a study can sometimes be an iterative process. This requires switching back and forth between Excel and R and generating a new .csv file for each change to the design, which can be somewhat time consuming. When creating designs using the Excel tool for MacCalman’s design, there’s no getting around this because of the stochastic processes contained within the tool. With Cioppa’s Excel file, however, the entire iterative process can be conducted in R by using the underlying factor codings.\nCioppa’s designs are the result of a Mixed Integer Program and therefore produce integer values (aka, factor codings) for each design point. The Excel tool then converts these integers into the ranges and significant figures specified by the user to generate a .csv file. This is a straight forward conversion that we can perform in an R script, so all we really need are the factor codings themselves to do everything in an R session.\nFactor codings for Cioppa’s 17-point design are as follows:\n\n\n\n\n  \n\n\n\nA pairs plot of factor codings for Cioppa’s 17-point design is shown below. Compare this plot with the pairs plot for the random Latin hypercube design.\n\nGGally::ggpairs(dm17, \n                lower = list(continuous = wrap(\"points\", size=0.1)), \n                progress=FALSE) + \n  ggtitle(\"OLH With 17 Design Points\") +\n  theme_bw()\n\n\n\n\nNext is Cioppa’s 257-point design. To reduce the size of the plot, just the first 7 of the possible 29 factors are shown. Note the improved space-filling properties compared to the 17-point design.\n\nggpairs(dm257 %>% select(1:7), \n        lower = list(continuous = wrap(\"points\", size=0.1)), \n        progress=FALSE) + \n  ggtitle(\"NOLH With 257 Design Points\") +\n  theme_bw()"
  },
  {
    "objectID": "tutorial/advanced.html#orthogonality-and-variance-inflation-factors",
    "href": "tutorial/advanced.html#orthogonality-and-variance-inflation-factors",
    "title": "Advanced Designs",
    "section": "Orthogonality and Variance Inflation Factors",
    "text": "Orthogonality and Variance Inflation Factors\nCioppa’s 17-point design is, in fact, strictly orthogonal, which we can confirm by calculating the dot product of each factor pair.\n\nfor (i in 1:6){\n  for (j in (i+1):7){\n    print(dm17[[i]] %*% dm17[[j]])\n  }\n}\n\n     [,1]\n[1,] 1377\n     [,1]\n[1,] 1377\n     [,1]\n[1,] 1377\n     [,1]\n[1,] 1377\n     [,1]\n[1,] 1377\n     [,1]\n[1,] 1377\n     [,1]\n[1,] 1377\n     [,1]\n[1,] 1377\n     [,1]\n[1,] 1377\n     [,1]\n[1,] 1377\n     [,1]\n[1,] 1377\n     [,1]\n[1,] 1377\n     [,1]\n[1,] 1377\n     [,1]\n[1,] 1377\n     [,1]\n[1,] 1377\n     [,1]\n[1,] 1377\n     [,1]\n[1,] 1377\n     [,1]\n[1,] 1377\n     [,1]\n[1,] 1377\n     [,1]\n[1,] 1377\n     [,1]\n[1,] 1377\n\n\nWe can also see that the pair-wise correlation coefficients are all zero.\n\ncor(dm17)\n\n   f4 f5 f7 f2 f3 f6 f1\nf4  1  0  0  0  0  0  0\nf5  0  1  0  0  0  0  0\nf7  0  0  1  0  0  0  0\nf2  0  0  0  1  0  0  0\nf3  0  0  0  0  1  0  0\nf6  0  0  0  0  0  1  0\nf1  0  0  0  0  0  0  1\n\n\nA common method to check for multicollinearity in a design is to calculate the variance inflation factor (VIF) for each predictor. Penn State has a good online resource that describes VIF, which I’ll paraphrase. When multicollinearity exists, the standard errors of the estimated coefficients are inflated, which means the variances are also inflated. The VIF for the estimated regression coefficient \\(\\beta_i\\) is the amount the variance of \\(\\beta_i\\) is “inflated” by the existence of correlation among the predictor variables in the model. The VIF for the ith predictor is:\n\\[VIF_i = \\frac{1}{1-R^{2}_{i}}\\]\nwhere \\(R^{2}_{i}\\) is the \\(R^2\\) obtained by regressing the ith predictor on the remaining predictors. If no multicollinearity exists, then each predictor’s VIF will equal 1. A rule of thumb is that a VIF above about 5 should be investigated, and a VIF above 10 indicates strong multicollinearity that should be corrected. In R, we can use the vif() function from the faraway package. With the 17-point design, we see that no multicollinearity exists.\n\nfaraway::vif(dm17)\n\nf4 f5 f7 f2 f3 f6 f1 \n 1  1  1  1  1  1  1 \n\n\nRepeating the tests on the 257-point design, we see that not every column is exactly orthogonal. For example, these two dot products are not equal.\n\ndm257$f1 %*% dm257$f17\n\n        [,1]\n[1,] 4274287\n\ndm257$f2 %*% dm257$f23\n\n        [,1]\n[1,] 4275511\n\n\nWe also see that the correlation coefficients are slightly greater than zero. For example, the first four columns:\n\ncor(dm257)[, 1:4]\n\n               f1            f2            f3            f4\nf1   1.000000e+00  0.0000000000  0.000000e+00  0.000000e+00\nf2   0.000000e+00  1.0000000000  0.000000e+00  0.000000e+00\nf3   0.000000e+00  0.0000000000  1.000000e+00  0.000000e+00\nf4   0.000000e+00  0.0000000000  0.000000e+00  1.000000e+00\nf5   0.000000e+00  0.0000000000  0.000000e+00  0.000000e+00\nf6   0.000000e+00  0.0000000000  0.000000e+00  0.000000e+00\nf7   0.000000e+00  0.0000000000  0.000000e+00  0.000000e+00\nf8   0.000000e+00  0.0000000000  0.000000e+00  0.000000e+00\nf9  -1.269681e-03  0.0010575966 -2.209924e-03  4.496199e-04\nf10 -1.384207e-03  0.0014831803  1.411071e-03 -1.272509e-05\nf11  1.286648e-04 -0.0009416569  1.286648e-04 -2.324450e-03\nf12 -3.152995e-04 -0.0016302258  1.969562e-03 -4.510338e-04\nf13 -8.638924e-04 -0.0005994933 -7.126052e-04 -1.852208e-03\nf14  1.573670e-03 -0.0012908900 -1.979459e-04  3.017261e-03\nf15  7.196747e-04 -0.0003124717  1.894625e-03 -8.780314e-04\nf16  1.823930e-04  0.0022141661  5.132454e-04 -2.323036e-03\nf17 -1.732027e-03  0.0010830468  7.210886e-05 -8.200615e-04\nf18 -8.907565e-04  0.0016061895 -5.980794e-04 -6.263573e-04\nf19 -9.063094e-04  0.0002757103  5.160732e-04  2.465840e-03\nf20 -2.813659e-04 -0.0007055357  8.214754e-04 -9.770043e-04\nf21 -7.069496e-06 -0.0005500068 -5.273844e-04  2.573297e-04\nf22  8.356144e-04  0.0011452583  1.531253e-03 -3.492331e-04\nf23 -3.803389e-04 -0.0008667202 -1.979459e-04 -5.528346e-04\nf24  6.207017e-04  0.0004086169 -5.655597e-04  4.001335e-04\nf25  1.241403e-03  0.0002403629  7.635056e-05 -6.150461e-04\nf26  1.173536e-04 -0.0003916501 -1.428038e-03 -5.245566e-04\nf27  1.320582e-03 -0.0001399760  1.172122e-03 -1.905936e-03\nf28  5.316261e-04 -0.0016443648  1.148086e-03  1.324824e-03\nf29  5.104176e-04 -0.0006207017 -1.527011e-04  5.217288e-04\n\n\nThe VIF for each predictor is also slightly greater than one. Notice that the VIFs are nowhere near our rule of thumb values.\n\nfaraway::vif(dm257)\n\n      f1       f2       f3       f4       f5       f6       f7       f8 \n1.000017 1.000024 1.000024 1.000040 1.000013 1.000033 1.000028 1.000017 \n      f9      f10      f11      f12      f13      f14      f15      f16 \n1.000064 1.000049 1.000030 1.000033 1.000038 1.000039 1.000026 1.000060 \n     f17      f18      f19      f20      f21      f22      f23      f24 \n1.000062 1.000056 1.000037 1.000039 1.000027 1.000040 1.000029 1.000027 \n     f25      f26      f27      f28      f29 \n1.000024 1.000034 1.000050 1.000030 1.000042 \n\n\nOf Cioppa’s designs, only the 17-point design is strictly orthogonal. Why is that? The short answer is that he needed to slightly relax the orthogonality requirement in the mixed integer program that generated the larger designs, hence the term “nearly orthogonal”. For the long answer, read his Ph.D. dissertation at the link provided earlier. MacCalman took the same approach, so designs from his Excel tool are also nearly orthogonal."
  },
  {
    "objectID": "tutorial/advanced.html#shifting-and-stacking",
    "href": "tutorial/advanced.html#shifting-and-stacking",
    "title": "Advanced Designs",
    "section": "Shifting and Stacking",
    "text": "Shifting and Stacking\nSay we have a design matrix with a relatively small number of factors, in this case four. A 17-point OLH could easily accommodate just four factors, but what if we have the time and resources available for more than 17 runs? We wouldn’t want to just replicate the 17-point design as is because if we’re going to do more runs, we might as well improve the space-filling properties of the design. There are two common approaches in this situation. First, we can select a design that accommodates more factors than necessary and just keep the four factors that we need. For example, the 17-point design:\n\nggpairs(dm17 %>% select(1:4), lower = list(continuous = wrap(\"points\", size=0.1)), progress=FALSE) + \n  ggtitle(\"NOLH With 17 Design Points\") +\n  theme_bw()\n\n\n\n\nCompare that to four factors from the 257-point design.\n\nggpairs(dm257 %>% select(1:4), lower = list(continuous = wrap(\"points\", size=0.1)), progress=FALSE) + \n  ggtitle(\"NOLH With 257 Design Points\") +\n  theme_bw()\n\n\n\n\nAnother technique is to apply shifting and stacking to a base design. Starting with the 17-point design, we make a copy of the design, move the last column to the first column and shift the other columns to the right. Then we stack the original design on top of the shifted design.\n\noriginal_dm = dm17 %>% select(f1, f2, f3, f4) %>% mutate(design=\"original\")\n\nshifted_dm = dm17 %>% select(f4, f1, f2, f3) %>% mutate(design=\"shifted\")\ncolnames(shifted_dm) = colnames(original_dm)\n\nstacked_dm = bind_rows(original_dm, shifted_dm)\n\nggpairs(stacked_dm, mapping = ggplot2::aes(color = design),\n        lower = list(continuous = wrap(\"points\", size=1)), progress=FALSE) + \n  ggtitle(\"Shifted and Stacked Design Matrix\") +\n  theme_bw()"
  },
  {
    "objectID": "tutorial/gam.html",
    "href": "tutorial/gam.html",
    "title": "Generalized Additive Models",
    "section": "",
    "text": "Recall that if there is a non-linear relationship between predictor and response, we can attempt to transform the predictor using a known function (log, reciprocal, polynomial, etc.) to improve the model structure and fit. What if the relationship is more complex and is not well captured with a known function? Generalized additive models may be used in these cases.\nRecall that a linear model takes the form:\n\\[y=\\beta_{0}+\\beta_{1}x_{1}+\\beta_{2}x_{2}+...+\\varepsilon\\]\nAdditive models replace the linear terms (the \\(\\beta\\)s) with flexible smoothing functions and take the form:\n\\[y=\\beta_{0}+f_{1}(x_{1})+f_{2}(x_{2})+...+\\varepsilon\\]\nThere are many techniques and options for selecting the smoothing functions, but for this tutorial, we’ll discuss two: locally weighted error sum of squares (lowess and also commonly abbreviated as loess) and smoothing splines.\n\nLoess\nFor the theory behind loess smoothing, please read this page on the NIST website. This chapter will focus on implementing loess smoothing in R.\nAll smoothers have a tuning parameter that controls how smooth the smoother is. The tuning parameter in loess is referred to as the span with larger values producing more smoothness.\n\nlibrary(tidyverse)\nset.seed(42)\n\ndf = tibble(\n  x = runif(100, 1.5, 5.5),\n  y = sin(x*pi) + 2 + runif(100, 0, 0.5)\n)\n\nex1.ls = loess(y~x, span=0.25, data=df)\nex2.ls = loess(y~x, span=0.5, data=df)\nex3.ls = loess(y~x, span=0.75, data=df)\nxseq = seq(1.6, 5.4,length=100)\n\ndf = df %>% mutate(\n  span25 = predict(ex1.ls, newdata=tibble(x=xseq)),\n  span50 = predict(ex2.ls, newdata=tibble(x=xseq)),\n  span75 = predict(ex3.ls, newdata=tibble(x=xseq))\n  )\n\nggplot(df) +\n  geom_point(aes(x, y)) +\n  geom_line(aes(x=xseq, span25, linetype='span = 0.25'), color='red') +\n  geom_line(aes(x=xseq, span50, linetype='span = 0.50'), color='red') +\n  geom_line(aes(x=xseq, span75, linetype='span = 0.75'), color='red') +\n  scale_linetype_manual(name=\"Legend\", values=c(1,2,3)) +\n  ggtitle(\"Loess Smoother Example\") +\n  theme_bw()\n\n\n\n\nFrom this plot, a span of 0.75 provided too much smoothness, whereas the lower values of span we tested appear to be a better fit. Now let’s apply this to the airqaulity data set from the previous chapter. Initially, we’ll just consider the response(Ozone) and one predictor (Solar.R).\n\naq1.ls = loess(Ozone ~ Solar.R, span=0.25, data=airquality)\naq2.ls = loess(Ozone ~ Solar.R, span=0.5, data=airquality)\naq3.ls = loess(Ozone ~ Solar.R, span=0.75, data=airquality)\nsrseq = seq(10, 330, length=nrow(airquality))\n\naq = airquality %>% mutate(\n  span25 = predict(aq1.ls, newdata=tibble(Solar.R=srseq)),\n  span50 = predict(aq2.ls, newdata=tibble(Solar.R=srseq)),\n  span75 = predict(aq3.ls, newdata=tibble(Solar.R=srseq))\n  )\n\nggplot(aq) +\n  geom_point(aes(Solar.R, Ozone)) +\n  geom_line(aes(x=srseq, span25, linetype='span = 0.25'), color='red') +\n  geom_line(aes(x=srseq, span50, linetype='span = 0.50'), color='red') +\n  geom_line(aes(x=srseq, span75, linetype='span = 0.75'), color='red') +\n  scale_linetype_manual(name=\"Legend\", values=c(1,2,3)) +\n  ggtitle(\"Loess Smoother Example\") +\n  theme_bw()\n\n\n\n\nHere we can see that the higher span values appear to provide a better fit. In this case, choosing a low span value would be akin to over fitting a linear model with too high of a degree of polynomial. We can repeat this process to determine appropriate values of span for the other predictors.\nIncluding loess smoothers in a GAM is as simple as including the non-linear terms within lo(). The gam package provides the needed functionality. The script below applies loess smoothers to three of the predictors and displays the model summary (note that the default value for span is 0.5).\n\nlibrary(gam)\n\naq.gam = gam(Ozone ~ lo(Solar.R, span=0.75) + lo(Wind) + lo(Temp), data=airquality, na=na.gam.replace)\nsummary(aq.gam)\n\n\nCall: gam(formula = Ozone ~ lo(Solar.R, span = 0.75) + lo(Wind) + lo(Temp), \n    data = airquality, na.action = na.gam.replace)\nDeviance Residuals:\n    Min      1Q  Median      3Q     Max \n-47.076  -9.601  -2.721   8.977  76.583 \n\n(Dispersion Parameter for gaussian family taken to be 319.3603)\n\n    Null Deviance: 125143.1 on 115 degrees of freedom\nResidual Deviance: 33679.85 on 105.4604 degrees of freedom\nAIC: 1010.117 \n\nNumber of Local Scoring Iterations: NA \n\nAnova for Parametric Effects\n                             Df Sum Sq Mean Sq F value    Pr(>F)    \nlo(Solar.R, span = 0.75)   1.00  14248   14248  44.615 1.160e-09 ***\nlo(Wind)                   1.00  35734   35734 111.894 < 2.2e-16 ***\nlo(Temp)                   1.00  15042   15042  47.099 4.794e-10 ***\nResiduals                105.46  33680     319                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova for Nonparametric Effects\n                         Npar Df Npar F     Pr(F)    \n(Intercept)                                          \nlo(Solar.R, span = 0.75)     1.2 2.9766 0.0804893 .  \nlo(Wind)                     2.8 9.2752 2.617e-05 ***\nlo(Temp)                     2.5 6.8089 0.0006584 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\npar(mfrow=c(1,3))\nplot(aq.gam, se=TRUE)\n\n\n\n\n\n\nSplines\nSpline smoothing can be conceptualized by imagining that your task is to bend a strip of soft metal into a curved shape. One way to do this would be to place pegs on a board (referred to as “knots” in non-linear regression parlance) to control the bends, and then guide the strip of metal over and under the pegs. Mathematically, this is accomplished by combining cubic regression at each knot with calculus to smoothly join the individual bends. The tuning parameter in the smooth.splines function is spar.\n\naq = aq %>% drop_na()\n\nss25 = smooth.spline(aq$Solar.R,aq$Ozone,spar=0.25)\nss50 = smooth.spline(aq$Solar.R,aq$Ozone,spar=0.5)\nss75 = smooth.spline(aq$Solar.R,aq$Ozone,spar=0.75)\n\nggplot() +\n  geom_point(data=aq, aes(Solar.R, Ozone)) +\n  geom_line(aes(x=ss25$x, ss25$y, linetype='spar = 0.25'), color='red') +\n  geom_line(aes(x=ss50$x, ss50$y, linetype='spar = 0.50'), color='red') +\n  geom_line(aes(x=ss75$x, ss75$y, linetype='spar = 0.75'), color='red') +\n  scale_linetype_manual(name=\"Legend\", values=c(1,2,3)) +\n  ggtitle(\"Spline Smoother Example\") +\n  theme_bw()\n\n\n\n\n\n\nCross Validation\nComparing the spline smoother plot to the one generated with loess smoothers, we can see that the two methods essentially accomplish the same thing. It’s just a matter of finding the right amount of smoothness, which can be done through cross validation. The fANCOVA package contains a function loess.aq() that includes a criterion parameter that we can set to gcv for generalized cross validation, which is an approximation for leave-one-out cross-validation Trevor Hastie and Friedman (2008). Applying this function to the airquality data with Solar.R as the predictor and Ozone as the response, we can obtain a cross validated value for span.\n\nlibrary(fANCOVA)\n\naq.solar.cv = loess.as(aq$Solar.R, aq$Ozone, criterion=\"gcv\")\nsummary(aq.solar.cv)\n\nCall:\nloess(formula = y ~ x, data = data.bind, span = span1, degree = degree, \n    family = family)\n\nNumber of Observations: 111 \nEquivalent Number of Parameters: 3.09 \nResidual Standard Error: 29.42 \nTrace of smoother matrix: 3.56  (exact)\n\nControl settings:\n  span     :  0.6991628 \n  degree   :  1 \n  family   :  gaussian\n  surface  :  interpolate     cell = 0.2\n  normalize:  TRUE\n parametric:  FALSE\ndrop.square:  FALSE \n\n\nloess.as also includes a plot method so we can visualize the loess smoother.\n\nloess.as(aq$Solar.R, aq$Ozone, criterion=\"gcv\", plot=TRUE)\n\n\n\n\nCall:\nloess(formula = y ~ x, data = data.bind, span = span1, degree = degree, \n    family = family)\n\nNumber of Observations: 111 \nEquivalent Number of Parameters: 3.09 \nResidual Standard Error: 29.42 \n\n\nCross validation is also built in to smooth.spline() and is set to generalized cross validation by default. Instead of specifying spar in the call to smooth.spline(), we just leave it out to invoke cross validation.\n\naq.spl = smooth.spline(aq$Solar.R, aq$Ozone)\naq.spl\n\nCall:\nsmooth.spline(x = aq$Solar.R, y = aq$Ozone)\n\nSmoothing Parameter  spar= 0.9837718  lambda= 0.01867197 (12 iterations)\nEquivalent Degrees of Freedom (Df): 4.060081\nPenalized Criterion (RSS): 66257.74\nGCV: 892.29\n\n\nPlotting the cross validated spline smoother, we get a line that looks very similar to the lasso smoother.\n\nggplot() +\n  geom_point(data=aq, aes(Solar.R, Ozone)) +\n  geom_line(aes(x=aq.spl$x, aq.spl$y), color='red') +\n  ggtitle(\"CV Spline Smoother\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nReferences\n\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman. 2008. The Elements of Statistical Learning. Springer."
  },
  {
    "objectID": "tutorial/lm_assumptions.html",
    "href": "tutorial/lm_assumptions.html",
    "title": "Linear Model Assumptions",
    "section": "",
    "text": "There are four assumptions fundamental to linear regression:\nWhen conducting linear regression, we need to always perform diagnostic check to ensure we are not violating any of the inherent assumptions."
  },
  {
    "objectID": "tutorial/lm_assumptions.html#linearity",
    "href": "tutorial/lm_assumptions.html#linearity",
    "title": "Linear Model Assumptions",
    "section": "Linearity",
    "text": "Linearity\nThe assumption is that the relationship between x and the mean of y is linear, but what does that mean exactly? A regression model is linear if \\(E[Y|X =x]\\) is a linear function of the \\(\\beta\\) parameters, not of \\(x\\). That means each of the following is a linear model:\n\n\\(\\beta_{0} + \\beta_{1}x\\) 1\n\\(\\beta_{0} + \\beta_{1}x + \\beta_{2}x^{2} + \\beta_{3}x^{3}\\)\n\\(\\beta_{0} + \\beta_{1}log(x) + \\beta_{2}sin(x)\\)\n\nThese are not linear models:\n\n\\(\\beta_{0} + x^{\\beta_{1}}\\)\n\\(\\frac{1}{\\beta_{0} + \\beta_{1}x}\\)\n\\(\\frac{e^{\\beta_{0}+\\beta_{1}x_{1}}}{1+e^{\\beta_{0}+\\beta_{1}x_{1}}}\\) 2\n\nAs with ANOVA, R produces diagnostic plots for objects created by the lm() function. The first plot may be used to evaluate both linearity and homoscedasticity. A linear relationship will be indicated by a (relatively) horizontal red line on the plot. Since our height-weight data is so simple, we’ll switch to the teengamb dataset from the faraway package. This dataset consists of four predictor variables and one response (gamble). Read the help for teengamb to familiarize yourself with the data. Since one of the predictors is binary (sex), we’ll exclude it for this example. 3 A summary of the resulting linear model is as follows.\n\nlibrary(tidyverse)\nlibrary(faraway)\n\ntg.lm = lm(gamble ~ . -sex, data=teengamb)\nsummary(tg.lm)\n\n\nCall:\nlm(formula = gamble ~ . - sex, data = teengamb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-49.649 -12.008  -1.242   8.239 103.390 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -1.3044    15.7760  -0.083   0.9345    \nstatus        0.4701     0.2509   1.873   0.0678 .  \nincome        5.7707     1.0494   5.499 1.95e-06 ***\nverbal       -4.1211     2.2785  -1.809   0.0775 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.28 on 43 degrees of freedom\nMultiple R-squared:  0.445, Adjusted R-squared:  0.4062 \nF-statistic: 11.49 on 3 and 43 DF,  p-value: 1.161e-05\n\n\nThe diagnostic plot to check the linearity assumption is the first plot returned, and we see a slight “U” shape to the red line. Notice that there are only three observations on the far right which appear to be heavily influencing the results. The conical spread of the data also strongly suggests heteroscedasticity might be an issue.\n\nplot(tg.lm, which = 1)\n\n\n\n\nAnother screening method is with a pairs plot, which we can quickly produce in base R with pairs(). This is a great way to do a quick check potential nonlinear relationships between pairs of variables. This is a screening method only, however, because we’re projecting onto two dimensions, so we may be missing things lurking in higher dimensions.\n\npairs(teengamb[, 2:5], upper.panel=NULL, lower.panel=panel.smooth)\n\n\n\n\nIf evidence of a nonlinear relationship exists, a linear model can still be used; however, either the response variable or one or more of the predictors must be transformed. This topic is covered in detail in the Advanced Designs chapter."
  },
  {
    "objectID": "tutorial/lm_assumptions.html#homoscedasticity",
    "href": "tutorial/lm_assumptions.html#homoscedasticity",
    "title": "Linear Model Assumptions",
    "section": "Homoscedasticity",
    "text": "Homoscedasticity\nThe procedure for testing constant variance in residuals in a linear model is similar to ANOVA. A plot of residuals versus fitted values is shown two plots ago, and we can look at the square root of standardized residuals versus fitted values. Both plots show strong evidence of heteroscedasticity.\n\nplot(tg.lm, which = 3)\n\n\n\n\nThere is no doubt some subjectivity to visual inspections. As a guide, consider the next three sets of plots that show constant variance, mild heteroscedasticity, and strong heteroscedasticity.\nConstant variance:\n\npar(mfrow = c(3,3), oma = c(5,4,0,0) + 0.1, mar = c(0,0,1,1) + 0.1)\n\nn <- 50 \nfor(i in 1:9) {x <- runif(n); plot(x,rnorm(n))} \n\n\n\n\nMild heteroscedasticity:\n\npar(mfrow = c(3,3), oma = c(5,4,0,0) + 0.1, mar = c(0,0,1,1) + 0.1)\n\nfor(i in 1:9) {x <- runif(n); plot(x,sqrt((x))*rnorm(n))} \n\n\n\n\nStrong heteroscedasticity:\n\npar(mfrow = c(3,3), oma = c(5,4,0,0) + 0.1, mar = c(0,0,1,1) + 0.1)\n\nfor(i in 1:9) {x <- runif(n); plot(x,x*rnorm(n))}\n\n\n\n\nThe linear model analog to the Levene test is the Breusch-Pagan test. The null hypothesis is that the residuals have constant variance, and the alternative is that the error variance changes with the level of the response or with a linear combination of predictors. The ncvTest() from the car (companion to applied regression) package performs the test, and when applied to the tg.lm object confirms our suspicion of non-constant variance based on our visual inspection.\n\ncar::ncvTest(tg.lm)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 26.18623, Df = 1, p = 3.1003e-07"
  },
  {
    "objectID": "tutorial/lm_assumptions.html#independence",
    "href": "tutorial/lm_assumptions.html#independence",
    "title": "Linear Model Assumptions",
    "section": "Independence",
    "text": "Independence\nThe concept of independent (and identically distributed) data was covered in the statistics review and ANOVA chapters. It is no different when conducting linear regression and so will not be repeated here."
  },
  {
    "objectID": "tutorial/lm_assumptions.html#normality",
    "href": "tutorial/lm_assumptions.html#normality",
    "title": "Linear Model Assumptions",
    "section": "Normality",
    "text": "Normality\nAgain, checking whether the residuals are normally distributed is the same for linear regression as for ANOVA. Create a Q-Q plot and apply the Shapiro-Wilk test as shown below.\n\nplot(tg.lm, which=2)\n\n\n\nshapiro.test(residuals(tg.lm))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(tg.lm)\nW = 0.8651, p-value = 6.604e-05"
  },
  {
    "objectID": "tutorial/lm_assumptions.html#unusual-observations",
    "href": "tutorial/lm_assumptions.html#unusual-observations",
    "title": "Linear Model Assumptions",
    "section": "Unusual Observations",
    "text": "Unusual Observations\nAlthough not an assumption inherent to a linear model, it’s good practice to also check for unusual observations when performing diagnostic checks. There are two types of unusual observations: outliers and influential. An outlier is an observation with a large residual - it plots substantially above or below the regression line. An influential observation is one that substantially changes the model fit. Keep in mind that it is possible for an observation to have both characteristics. Examples of both types of observations are shown on the following plot (note that I rigged observations 11 and 12 to be unusual observations).\n\n\n\n\n\nIt’s not necessarily bad to have unusual observations, but it’s good practice to check for them, and, if found, decide what to do about them. A point with high leverage falls within the predictor space but is significantly separated from the other points. It has the potential to influence the fit but may not actually do so.\n\nLeverage Points\nThe amount of leverage associated with each observation is called the hat value and are the diagonal elements of the hat matrix, which you can read more about here, if you’re interested (or just really like linear algebra). The gist of it is that the sum of the hat values equals the number of observations. If every observation has exactly the same leverage, then the hat values will all equal \\(p/n\\), where p is the number of parameters and n is the number of observations (in our example we just have two parameters, so it’s \\(2/n\\)). Increasing the hat value of one observation necessitates decreasing the hat values of the others, so we’re essentially looking for hat values significantly greater than this theoretical average. The generally accepted rule of thumb is that hat values greater than ~ \\(2p/n\\) times the averages should be looked at more carefully. Extracting hat values from a linear model in R is done using the hatvalues() or influence() functions.\n\ndf.lm = lm(y~x, data=df)\nhatv = hatvalues(df.lm)\nprint(hatv)\n\n         1          2          3          4          5          6          7 \n0.14483261 0.12736536 0.11280932 0.10116448 0.09243086 0.08660844 0.08369723 \n         8          9         10         11         12 \n0.08369723 0.08660844 0.09243086 0.10116448 0.88719068 \n\ninfluence(df.lm)$hat\n\n         1          2          3          4          5          6          7 \n0.14483261 0.12736536 0.11280932 0.10116448 0.09243086 0.08660844 0.08369723 \n         8          9         10         11         12 \n0.08369723 0.08660844 0.09243086 0.10116448 0.88719068 \n\n\nVerify that the sum of the hat values equals the number of parameters (2):\n\nprint(paste(\"Sum of hat values:\", sum(hatv)))\n\n[1] \"Sum of hat values: 2\"\n\n\nAre any hat values > \\(2p/n\\) (recall I rigged observation 12)?\n\nhatv > 4/length(df$x)\n\n    1     2     3     4     5     6     7     8     9    10    11    12 \nFALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE \n\n\nA graphical way of looking at leverage is with the halfnorm() function in the faraway package, which plots leverage against the positive normal quantiles. I added a red line to indicate the rule of thumb threshold.\n\nfaraway::halfnorm(hatv,ylab=\"Leverages\")\nabline(h=2*mean(hatv), col='red')\n\n\n\n\nAnother measure of leverage is Cook’s Distance, defined as:\n\\[D_{i}=\\frac{r^{2}_{i}}{p}\\left(\\frac{h_{i}}{1-h_{i}}\\right)\\]\nThe rule of thumb for Cook’s Distance is an observation with \\(D>1\\), and we can get these values in R with cooks.distance().\n\ncooks.distance(df.lm)\n\n           1            2            3            4            5            6 \n6.193970e-02 4.398193e-05 6.553697e-04 4.789149e-10 3.736810e-03 1.917568e-02 \n           7            8            9           10           11           12 \n3.490432e-03 4.701066e-02 1.130421e-02 9.893220e-02 3.409595e-01 1.703556e+01 \n\n\nThe fourth linear model plot also contains Cook’s Distance.\n\nplot(df.lm, which=4)\n\n\n\n\n\n\nOutliers\nThe hat values from the previous section are also used to calculate standardized residuals, \\(r_{i}\\).\n\\[r_{i}=\\frac{\\hat{\\varepsilon}_{i} }{\\hat{\\sigma}\\sqrt{1-h_{i}}}, i=1,...,n \\]\nwhere \\(\\hat{\\varepsilon}\\) are the residuals, \\(\\hat{\\sigma}\\) is the estimated residual standard error, and \\(h\\) is the leverage. The rule of thumb for identifying unusually large standardized residuals is if \\(|r_{i}| > 2\\). We can get standardized residuals in R with rstandard().\n\nrstandard(df.lm)\n\n            1             2             3             4             5 \n 8.552478e-01 -2.454950e-02  1.015300e-01 -9.225082e-05 -2.708924e-01 \n            6             7             8             9            10 \n-6.359731e-01 -2.764512e-01 -1.014559e+00 -4.882963e-01 -1.393847e+00 \n           11            12 \n 2.461458e+00  2.081408e+00 \n\n\nHere we see that observation 11 is a potential outlier, and the observation 12 is both a high leverage point and a potential outlier.\nWe can also look at studentized residuals, which are defined as:\n\\[t_{i} = r_{i}\\sqrt{\\frac{n-p-1}{n-p-r^{2}_{i}}}\\]\nIn R, we can use rstudent():\n\nrstudent(df.lm)\n\n            1             2             3             4             5 \n 8.427665e-01 -2.329041e-02  9.636946e-02 -8.751682e-05 -2.579393e-01 \n            6             7             8             9            10 \n-6.159215e-01 -2.632726e-01 -1.016216e+00 -4.688619e-01 -1.473142e+00 \n           11            12 \n 3.719616e+00  2.622850e+00 \n\n\nIt may be useful to view all of these measures together and apply some conditional formatting.\n\nlibrary(kableExtra)\n\ndf %>% \n  mutate(\n    obs = 1:nrow(df),\n    r.standard = round(rstandard(df.lm), 3),\n    r.student = round(rstudent(df.lm), 3), \n    i.hatv = round(hatvalues(df.lm), 3),\n    i.cook = round(cooks.distance(df.lm), 3)) %>%\n  mutate(\n    r.standard = cell_spec(\n      r.standard, \"html\", color=ifelse(abs(r.standard)>2,\"red\", \"green\")),\n    r.student = cell_spec(\n      r.student, \"html\", color=ifelse(abs(r.student)>2,\"red\", \"green\")),\n    i.hatv = cell_spec(\n      i.hatv, \"html\", color=ifelse(i.hatv>4/nrow(df),\"red\", \"green\")),\n    i.cook = cell_spec(i.cook, \"html\", color=ifelse(i.cook>1,\"red\", \"green\"))) %>%\n  kable(format = \"html\", escape = F) %>%\n  kable_styling(\"striped\", full_width = F)\n\n\n\n \n  \n    x \n    y \n    obs \n    r.standard \n    r.student \n    i.hatv \n    i.cook \n  \n \n\n  \n    0.0000000 \n    1.6854792 \n    1 \n    0.855 \n    0.843 \n    0.145 \n    0.062 \n  \n  \n    0.3333333 \n    0.7509842 \n    2 \n    -0.025 \n    -0.023 \n    0.127 \n    0 \n  \n  \n    0.6666667 \n    1.2482309 \n    3 \n    0.102 \n    0.096 \n    0.113 \n    0.001 \n  \n  \n    1.0000000 \n    1.4164313 \n    4 \n    0 \n    0 \n    0.101 \n    0 \n  \n  \n    1.3333333 \n    1.3354675 \n    5 \n    -0.271 \n    -0.258 \n    0.092 \n    0.004 \n  \n  \n    1.6666667 \n    1.1136044 \n    6 \n    -0.636 \n    -0.616 \n    0.087 \n    0.019 \n  \n  \n    2.0000000 \n    1.9557610 \n    7 \n    -0.276 \n    -0.263 \n    0.084 \n    0.003 \n  \n  \n    2.3333333 \n    1.1860038 \n    8 \n    -1.015 \n    -1.016 \n    0.084 \n    0.047 \n  \n  \n    2.6666667 \n    2.2758785 \n    9 \n    -0.488 \n    -0.469 \n    0.087 \n    0.011 \n  \n  \n    3.0000000 \n    1.2686430 \n    10 \n    -1.394 \n    -1.473 \n    0.092 \n    0.099 \n  \n  \n    1.0000000 \n    5.0000000 \n    11 \n    2.461 \n    3.72 \n    0.101 \n    0.341 \n  \n  \n    10.0000000 \n    11.0000000 \n    12 \n    2.081 \n    2.623 \n    0.887 \n    17.036 \n  \n\n\n\n\n\n\n\nWhat To Do About Unusual Observations\nIn the book, Linear Models With R (Faraway 2014), Faraway gives advice on this topic that I’ll paraphrase:\n\nCheck for data entry errors and correct any that are found.\nConsider the context. An unusual observation may be the single most important observation in the study.\nExclude the observation from the dataset and refit a model. If it makes little to no difference in your analysis, then it’s usually best to leave it in.\nDo not automate the process of excluding outliers (see #2 above).\nIf you exclude an observation, document it in your report and explain your rationale so that your analytic integrity is not questioned."
  },
  {
    "objectID": "tutorial/logistic.html",
    "href": "tutorial/logistic.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "So far, we’ve seen regression techniques for continuous and categorical response variables. There is a different form of regression called logistic regression for the case when the response is a binary variable.\n\nMotivating Example\nThe need for a different type of regression can be seen using an example. We’ll look at just one predictor (age) and the response (chd) in the SAheart dataset from the bestglm package. Here the response, chd is a binary variable indicating whether someone did (1) or did not (0) develop coronoary heart disease. For now, we’ll just look at the first 20 observations with a scatter plot.\n\nlibrary(tidyverse)\n\nSAheart = bestglm::SAheart\n\n# look at the first 20 observations only and only age vs. chd\nsaheart = SAheart[1:21, c('age', 'chd')]\n\nggplot(saheart[1:20,], aes(x=age, y=chd)) +\n  geom_point() +\n  ylim(0, 1) +\n  theme_bw()\n\n\n\n\nThere’s a clear trend here - younger typically didn’t have heart disease while older people did - but what exactly is the nature of the relationship? We can also think about this relationship in terms of probability. People under 20 have a virtually 0 probability of heart disease, and people over 60 have a near 1.0 probability of heart disease. But how do we connect those two extremes? If we assume there is a linear relationship, we’d get the following plot.\n\nggplot(saheart[1:20,], aes(x=age, y=chd)) +\n  geom_point() +\n  geom_smooth(formula = y ~ x, method=lm, se=FALSE) +\n  ylim(0, 1) +\n  theme_bw()\n\n\n\n\nThe 21st observation happens to be associated with a 20-year old who happened to have heart disease. If we include this new observation and re-fit the linear regression line, we get the following.\n\nggplot(saheart[1:21,], aes(x=age, y=chd)) +\n  geom_point() +\n  geom_smooth(formula = y ~ x, method=lm, se=FALSE) +\n  ylim(0, 1) +\n  theme_bw()\n\n\n\n\nAdding the single observation didn’t give us any new information about the probability of heart disease for people in their 40s, 50, and 60s, but considerably changed the fit. Additionally, if we were to extend the regression line to the right to predict the probability of heart disease of an 80-year old, we’d get a probability > 1. For these reasons, linear regression doesn’t model the relationship well, so we need to find something better.\n\n\nLogit Function\nAn alternative to linear regression is to use a logit function, \\(\\eta\\) to replace \\(y\\) in the linear regression equation.\n\\[\\eta = \\beta_{0}+\\beta_{1}x_{1}+...+\\beta_{i}x_{i}+\\varepsilon\\]\nwhere,\n\\[\\eta = log\\left\\lgroup{\\frac{p}{1-p}}\\right\\rgroup\\]\nand where \\(p\\) is the probability of heart disease. In this form \\(\\eta\\) can also be thought of in terms of \\(log(odds)\\). To enforce \\(0\\le p \\le 1\\), we further define \\(p\\) as:\n\\[p=\\frac{e^{\\eta}}{1+e^{\\eta}}\\]\nWith one predictor, as in our case, we can rewrite this to become:\n\\[p=\\frac{e^{\\beta_{0}+\\beta_{1}x_{1}}}{1+e^{\\beta_{0}+\\beta_{1}x_{1}}}\\]\nIf we now set \\(\\beta_{0}=0\\) and allow \\(\\beta_{1}\\) to vary, we can see the shape of the response for different coefficient values.\n\n\n\n\n\nNote that if \\(\\beta_{1}=0\\), that is the equivalent of saying that \\(p\\) is not a function of \\(x\\). The reverse (allowing \\(\\beta_{0}\\) to vary while holding \\(\\beta_{1}=1\\)), shifts the curve horizontally.\n\n\n\n\n\n\n\nLogistic Regression in R\nTo fit a logistic regression model in R, use glm() instead of lm() and specify family=binomial.\n\nsa.glm = glm(chd~age, family=binomial, data=SAheart)\nsummary(sa.glm)\n\n\nCall:\nglm(formula = chd ~ age, family = binomial, data = SAheart)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.4321  -0.9215  -0.5392   1.0952   2.2433  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -3.521710   0.416031  -8.465  < 2e-16 ***\nage          0.064108   0.008532   7.513 5.76e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 596.11  on 461  degrees of freedom\nResidual deviance: 525.56  on 460  degrees of freedom\nAIC: 529.56\n\nNumber of Fisher Scoring iterations: 4\n\n\nFrom the summary, we see that \\(\\beta_{0} = -3.522\\) and \\(\\beta_{1} = 0.064\\), which gives us the equation for the estimated linear predictor:\n\\[\\hat{\\eta} = -3.522 + 0.064x\\]\nand the equation for the fitted probabilities.\n\\[\\hat{p}=\\frac{e^{-3.522 + 0.064x}}{1+e^{-3.522 + 0.064x}}\\]\nGiven a 40-year old, we find \\(\\hat{\\eta}=\\) -0.962 and \\(\\hat{p}=\\) 0.2764779. This highlights an important distinction when using predict() with a binomial response. To calculate \\(\\hat{\\eta}\\):\n\npredict(sa.glm, newdata=tibble(age=40))\n\n        1 \n-0.957389 \n\n\nbut to calculate \\(\\hat{p}\\), we need to specify type = \"response\".\n\np.hat = predict(sa.glm, newdata=tibble(age=40), type=\"response\")\np.hat\n\n        1 \n0.2774013 \n\n\nWe can see that this is a much lower estimate of the probability of heart disease than was estimated by the linear model produced by lm(). Since \\(\\beta_{0}\\) is negative, the regression curve will be shifted to the right of the mean age, and a low value for \\(\\beta_{1}\\) will stretch out the “s” curve. A plot of \\(\\hat{p}\\) versus age with the binomial regression curve and our estimated probability for a 40-year old is shown below.\n\nages = seq(10, 80, length.out = nrow(SAheart))\npred = tibble(\n  p = predict(sa.glm, newdata=tibble(age=ages), type=\"response\"),\n  se = predict(sa.glm, newdata=tibble(age=ages), type=\"response\", se=TRUE)$se, # standard error\n  age = ages\n)\n\nggplot() +\n  geom_line(data = pred, aes(x=age, y=p), color='blue') +\n  geom_line(data = pred, aes(x=age, y=p+se), color='blue', linetype=3, size=0.5) +\n  geom_line(data = pred, aes(x=age, y=p-se), color='blue', linetype=3, size=0.5) +\n  geom_jitter(data = SAheart, aes(x=age, y=chd), shape=124, size = 4, width=0.2, height=0, alpha=0.5) +\n  geom_segment(aes(x=40, xend=40, y=0, yend=p.hat), color='red', linetype=2, size=0.5) +\n  geom_segment(aes(x=10, xend=40, y=p.hat, yend=p.hat), color='red', linetype=2, size=0.5) +\n  xlab(\"Age (years)\") +\n  ylab(\"Heart Disease (1=yes, 0=no)\\n p.hat\") +\n  theme_bw()\n\n\n\nrm(ages, p.hat)\n\n\n\nLogistic Regression Diagnostics\nDiagnostics for logistic regression follows the same philosophy as linear regression: we will check the model assumptions and look for outliers and high leverage observations. First, we’ll look for violations of the equal variance assumption, but instead of using the raw residuals as we did in linear regression, we need to look at the deviance residuals. For logistic regression, we have the following definitions:\n\nFitted values are \\(\\hat{\\eta} = \\hat{\\beta_0} + \\sum_{i=1}^n \\hat{\\beta_i}x_i\\)\nRaw residuals are \\(e_{i} = y_{i} - \\hat{p_{i}}\\)\nDeviance residuals are \\(r_{i} = sign(y_{i}-\\hat{p_{i}}) \\sqrt{-2 \\left\\{y_{i} ln(\\hat{p_{i}}) + (1-y_{i}) ln(1-\\hat{p_{i}})\\right\\}}\\)\n\nwhere \\(y_{i}\\) is either 0 or 1, so if \\(y_{i}=0\\), then \\(sign() = +\\)\n\n\nAs with linear model diagnostics, we can plot fitted values and deviance residuals; however, the plot is not particularly useful. Note that the upper row of points correspond to \\(y_{i}=1\\), and the lower row to \\(y_{i}=0\\). With a sufficiently large dataset, we can generate a more useful diagnostic plot by binning the observations based on their predicted value, and calculating the mean deviance residual for each bin.\n\ndf = tibble(\n  resid = residuals(sa.glm), # for raw residuals, specify residuals(sa.glm, type = \"response\")\n  preds = predict(sa.glm))\n\nggplot(df) +\n  geom_rect(aes(xmin=-2.2, xmax=-1.8, ymin=-0.8, ymax=2.2), fill='lightgray', alpha=0.5) +\n  geom_point(aes(x=preds, y=resid)) +\n  annotate(\"text\", x=-2, y=1, label=\"Example\\nBin\") +\n    xlab(\"Fitted Linear Predictor\") +\n    ylab(\"Deviance Residuals\") +\n    theme_bw()\n\n\n\n# alternatively, plot(sa.glm, which=1)\n\nA general guideline is to create bins with at least 30 observations each, which for the SAheart dataset, means 462 %/% 30 = 15 bins. Now we have a much more useful diagnostic plot.\n\ndf = df %>%\n  arrange(preds) %>%\n  mutate(bin = c(rep(1:15, each=30), rep(15, nrow(SAheart)-15*30)))\n\ndf %>%\n  group_by(bin) %>%\n  summarize(\n    meanResid = mean(resid),\n    meanPred = mean(preds), .groups = 'drop') %>%\n  ggplot() +\n    geom_point(aes(x=meanPred, y=meanResid)) +\n    xlab(\"Fitted Linear Predictor\") +\n    ylab(\"Deviance Residuals\") +\n    theme_bw()\n\n\n\n\nWe identify unusual observations in logistic regression the same way as we did with linear regression but with slightly different definitions for residuals. We already covered raw residuals and deviance residuals. If we now represent the deviance residuals as \\(r_{D}\\), then we have the following additional definitions:\n\n\n\n\n\n\n\n\nTerm\nDefinition\nR Command\n\n\n\n\nStandardized deviance residuals\n\\(r_{SD}=\\frac{r_{D}}{\\sqrt{1-h}}\\)\nrstandard(sa.glm)\n\n\nPearson residuals\n\\(r_{P}=\\frac{y-\\hat{p}}{\\sqrt{\\hat{p}(1-\\hat{p})}}\\)\nresiduals(sa.glm, type=\"pearson\")\n\n\nPearson standardized residuals\n\\(r_{SP}=\\frac{r_{P}}{\\sqrt{1-h}}\\)\nnone\n\n\nCook’s Distance\n\\(D=\\frac{(r_{SP})^{2}}{q+1} \\left({\\frac{h}{1-h}} \\right)\\)\ncooks.distance(sa.glm)\n\n\n\nApply the same rules of thumb when identifying unusual observations as with linear regression.\nLastly, we can assess the goodness of fit for a model using several methods. A simple approximation akin to measuring \\(R^2\\) is:\n\\[R^{2}=\\frac{D_{NULL}-D}{D_{NULL}}\\]\nwhere \\(D_{NULL}\\) is the null model devience (i.e., the total sum of squares) and \\(D\\) is the logistic regression model deviance. From the calculation below, we find that approximately 12% of the variance in chd is explained by age.\n\n(sa.glm$null - sa.glm$dev) / sa.glm$null\n\n[1] 0.1183444\n\n\nFaraway (2006) proposes a more sophisticated measure:\n\\[R^{2}=\\frac{1 - exp\\left\\{ (D-D_{NULL})/N \\right\\}} {1 - exp\\left\\{-D_{NULL}/N \\right\\}}\\]\nwhere \\(N\\) is the number of binary trials.\n\n(1-exp( (sa.glm$dev - sa.glm$null)/nrow(SAheart))) / (1-exp( (- sa.glm$null)/nrow(SAheart)))\n\n[1] 0.195377\n\n\nLastly, there’s the Hosmer-Lemeshow goodness of fit test where the null hypothesis is the the model fit is “good”, and the alternative hypothesis is the the model is saturated (i.e, not a good fit). For our example, we fail to reject the null hypothesis at the 95% confidence level. For a detailed treatment of the test, read this article.\n\nlibrary(ResourceSelection)\n\np.hat = predict(sa.glm, type=\"response\")\nhoslem.test(SAheart$chd, p.hat)\n\n\n    Hosmer and Lemeshow goodness of fit (GOF) test\n\ndata:  SAheart$chd, p.hat\nX-squared = 9.6408, df = 8, p-value = 0.2911\n\n\n\n\n\n\n\nReferences\n\nFaraway, Julian. 2006. Extending the Linear Model with r: Generalized Linear, Mixed Effects and Nonparametric Regression Models. CRC Press."
  },
  {
    "objectID": "tutorial/mlr.html",
    "href": "tutorial/mlr.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "In the previous section we considered just one predictor and one response. The linear model can be expanded to include multiple predictors by simply adding terms to the equation:\n\\[y = \\beta_{0} + \\beta_{1}x_{1}+ \\beta_{2}x_{2} + ... + \\beta_{(p-1)}x_{(p-1)} + \\varepsilon\\]\nWith one predictor, least squares regression produces a regression line. With two predictors, we get a regression plane (shown below), and so on up to a \\((p-1)\\) dimensional hyperplane."
  },
  {
    "objectID": "tutorial/mlr.html#linear-algebra-solution",
    "href": "tutorial/mlr.html#linear-algebra-solution",
    "title": "Multiple Linear Regression",
    "section": "Linear Algebra Solution",
    "text": "Linear Algebra Solution\nWith two or more predictors, we can’t solve for the linear model coefficients the same way we did for the one predictor case. Solving for the coefficients (the \\(\\beta\\)s) requires some linear algebra. Given a data set with \\(n\\) observations and one predictor, the \\(y\\) (response), \\(X\\) (predictor), and \\(\\beta\\) (coefficient) matrices are written as:\n\\[y= \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{pmatrix} , X= \\begin{pmatrix} 1 & x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{pmatrix}, \\beta= \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}\\]\nIncorporating the error term, we have:\n\\[\\varepsilon= \\begin{pmatrix} \\varepsilon_1 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix} = \\begin{pmatrix} y_1-\\beta_0-\\beta_1x_1 \\\\ \\vdots \\\\ y_n-\\beta_0-\\beta_1x_n \\end{pmatrix} = y-X\\beta\\]\nOnce we solve for the coefficients, multiplying them by the predictors gives the estimated response, \\(\\hat{y}\\).\n\\[X\\beta \\equiv \\hat{y}\\]\nWith multiple linear regression, we expand the \\(X\\) and \\(\\beta\\) matrices accordingly.\n\\[y= \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{pmatrix} , X= \\begin{pmatrix} 1 & x_{11} & \\ldots & x_{1p-1} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & \\ldots & x_{np-1} \\end{pmatrix}, \\beta= \\begin{pmatrix} \\beta_0 \\\\ \\vdots \\\\ \\beta_{p-1} \\end{pmatrix}\\]\nIncorporating error:\n\\[\\varepsilon= \\begin{pmatrix} \\varepsilon_1 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix} = \\begin{pmatrix} y_1-(\\beta_0-\\beta_1x_{11} + \\ldots + \\beta_{p-1}x_{1p-1}) \\\\ \\vdots \\\\ y_n-(\\beta_0-\\beta_1x_{n1} + \\ldots + \\beta_{p-1}x_{np-1}) \\end{pmatrix} = y-X\\beta\\]\nHowever, notice that the final equation remains unchanged.\n\\[X\\beta \\equiv \\hat{y}\\]\nThe residual sum of squares (RSS) also remains unchanged, and so do the other equations that have RSS as a term, such as residual standard error and \\(R^2\\). The following is an example of solving the system of equations for a case with two predictors and no error. Given \\(n=4\\) observations, we have the following system of equations:\n\\[x_0 = 10\\] \\[x_0 + x_2 = 17\\] \\[x_0 + x_1 = 15\\] \\[x_0 + x_1 + x_2 = 22\\]\nIn this example, we technically have all of the information we need to solve this system of equations without linear algebra, but we’ll apply it anyway to demonstrate the method. Rewriting the above system of equations into matrix form gives:\n\\[X= \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 0 \\\\ 1 & 1 & 1 \\end{pmatrix}, y= \\begin{pmatrix} 10 \\\\ 17 \\\\ 15 \\\\ 22 \\end{pmatrix}\\]\nOne way to solve for the \\(\\beta\\) vector is to transpose the \\(X\\) matrix and multiply it by the \\(X|y\\) augmented matrix.\n\\[X^TX|y = \\begin{pmatrix} 1&1&1&1 \\\\ 0&0&1&1 \\\\ 0&1&0&1 \\end{pmatrix} \\begin{pmatrix} 1&0&0&|&10 \\\\ 1&0&1&|&17 \\\\ 1&1&0&|&15 \\\\ 1&1&1&|&22 \\end{pmatrix} = \\begin{pmatrix} 4&2&2&|&64 \\\\ 2&2&1&|&37 \\\\ 2&1&2&|&39 \\end{pmatrix}\\]\nUse Gaussian elimination to reduce the resulting matrix by first multiplying the top row by \\(-\\frac{1}{2}\\) and adding those values to the second row.\n\\[\\begin{pmatrix} 4&2&2&|&64 \\\\ 0&1&0&|&5 \\\\ 2&1&2&|&39 \\end{pmatrix}\\]\nReduce further using the same process on the third row.\n\\[\\begin{pmatrix} 4&2&2&|&64 \\\\ 0&1&0&|&5 \\\\ 0&0&1&|&7 \\end{pmatrix}\\]\nWe find that\n\\[\\beta_2 = 7\\] and\n\\[\\beta_1 = 5\\]\nand using back substitution we get\n\\[4\\beta_0 + 2(5) + 2(7) = 64, \\enspace so \\enspace \\beta_0 = 10\\]\nThe resulting equation:\n\\[y=10+5x_1+7x_2\\]\ndefines the best fit plane for this data, which is visualized below.\n\n\n\n\n\n\nOf course, R has linear algebra functions, so we don’t have to do all of that by hand. For example, we can solve for the \\(\\beta\\) vector by multiplying both sides of the equation \\(X\\beta \\equiv \\hat{y}\\) by \\(X^T\\).\n\\[X^TX\\beta = X^Ty\\]\nSolving for \\(\\beta\\), we get:\n\\[\\beta=(X^TX)^{-1}X^Ty\\]\nNow use solve() function to calculate the \\(\\beta\\) vector (note that solve() inverts \\(X^TX\\) automatically).\n\nX = matrix(c(1,0,0,\n             1,0,1,\n             1,1,0,\n             1,1,1), byrow = TRUE, ncol=3)\n\ny = 10 + 5*X[, 2] + 7*X[, 3]\n\nsolve(t(X) %*% X) %*% (t(X) %*% y)\n\n     [,1]\n[1,]   10\n[2,]    5\n[3,]    7\n\n\nFitting a linear model in R using the lm() function produces coefficients identical to the above results.\n\ncoef(lm(y ~ X[, 2] + X[, 3]))\n\n(Intercept)      X[, 2]      X[, 3] \n         10           5           7 \n\n\nTechnically, neither the solve() nor the lm() functions use Gaussian elimination when solving the system of equations. According to this site, for overdetermined systems (where there are more equations than unknowns) like the example we’re working with, they use QR factorization instead. The details of QR factorization are beyond the scope of this course, but are explained well on these slides from a course at UCLA’s School of Engineering and Applied Sciences. In essence, the \\(X\\) matrix is decomposed into \\(Q\\) and \\(R\\) matrices that are substituted for \\(X\\) in the equation.\n\\[X^TX\\beta = X^Ty\\]\n\\[(QR)^T(QR)\\beta = (QR)^Ty\\]\nSkipping a lot of math, we end up with:\n\\[R\\beta=Q^Ty\\]\nIn R, use qr(X) to decompose \\(X\\), and then use solve.qr() to calculate the \\(\\beta\\) vector.\n\nQR = qr(X)\nsolve.qr(QR, y)\n\n[1] 10  5  7\n\n\nNow we’ll make it a little more complicated by returning to the data set plotted at the beginning of this section. It consists of \\(n=10\\) observations with random error.\n\nmlr # multiple linear regression data set\n\n\n\n  \n\n\n\nUsing QR decomposition, we get the following coefficients:\n\n# we need to add a column of 1's to get beta_0 for the intercept\nintercept = rep(1, 10)\nQR = qr(cbind(intercept, mlr[, 1:2])) \nbetas = solve.qr(QR, mlr$y)\nbetas\n\nintercept        x1        x2 \n0.1672438 0.4908812 0.1964567 \n\n\nAnd we get the following coefficients in the linear model:\n\ncoef(lm(y ~ ., data=mlr))\n\n(Intercept)          x1          x2 \n  0.1672438   0.4908812   0.1964567 \n\n\nThe following code chunk shows how the earlier interactive plot was generated. Note the following:\n\nThe value of y defined by the plane at (x1, x2) = (0,0) is \\(\\beta_0\\) (shown by the red dot).\nThe slope of the line at the intersection of the plane with the x1 axis is \\(\\beta_1\\).\nThe slope of the line at the intersection of the plane with the x2 axis is \\(\\beta_2\\),\n\n\n# define the bast fit plane using the betas from QR decomposition\nplane = tibble(x1=c(0, 10, 0, 10),\n               x2=c(0, 0, 10, 10),\n               y=c(betas[1], betas[1]+10*betas[2], betas[1]+10*betas[3], betas[1]+sum(10*betas[2:3])))\n\n# use plotly for interactive 3D graphs\nplot_ly() %>%\n  # add the points to the graph\n  add_trace(data = mlr, x=~x1, y = ~x2, z = ~y, type='scatter3d', mode='markers', \n            marker=list(color='black', size=7), showlegend=FALSE) %>%\n  # add the plane\n  add_trace(data = plane, x=~x1, y = ~x2, z = ~y, type='mesh3d', \n            facecolor=c('blue', 'blue'), opacity = 0.75, showlegend=FALSE) %>%\n  # add the red dot\n  add_trace(x=0, y=0, z=betas[1], type='scatter3d', mode='markers',\n            marker=list(color='red', size=7), showlegend=FALSE) %>%\n  # adjust the layout\n  layout(title = 'Best Fit Plane', showlegend = FALSE,\n         scene = list(xaxis = list(range=c(0,10)),\n                      yaxis = list(range=c(0,10)),\n                      camera = list(eye = list(x = 0, y = -2, z = 0.3))))"
  },
  {
    "objectID": "tutorial/mlr.html#multiple-linear-regression-in-r",
    "href": "tutorial/mlr.html#multiple-linear-regression-in-r",
    "title": "Multiple Linear Regression",
    "section": "Multiple Linear Regression in R",
    "text": "Multiple Linear Regression in R\n\n\n\n\n\n\nNote\n\n\n\nThis section is the result of a collaborative effort with Dr. Stephen E. Gillespie.\n\n\nBelow is a short example on doing multiple linear regression in R. This example uses a data set on patient satisfaction as a function of their age, illness severity, anxiety level, and a surgery variable (this is a binary variable, we will ignore for this exercise). We will attempt to model patient satisfaction as a function of age, illness severity, and anxiety level. First, read the data and build the linear model.\n\n# Read the data\npt <- read.csv('data/PatientSatData.csv', sep = ',', header = T)\n\n# let's drop SurgMed as we're not going to use it\npt <- pt %>% select(-SurgMed)\n\n# View the data\npt\n\n\n\n  \n\n\n\nNote that our data is formatted in numeric format, which is what we need for this sort of modeling.\nWe can look at our data. In multiple regression, pairs is useful.\n\npairs(pt)\n\n\n\n\nWe can see some useful things:\n\nAge and satisfaction appear to have a linear relationship (the bottom left corner)\nIllness severity and satisfaction appear to have a linear relationship, though not as strongly\nIt’s less clear for anxiety and satisfaction.\nAge and illness severity do not appear to have a relationship.\nAge and anxiety might have a relationship, but its not fully clear.\nIllness severity and anxiety do not appear to have a relationship\n\nModel the data. Note how this format is analogous to ANOVA with multiple factors and simple linear regression.\n\nptLM <- lm(Satisfaction ~ Age + illSeverity + Anxiety, data = pt)\n\nWe can now view our model results. We will use \\(\\alpha = .05\\) as our appropriate significance level.\n\n# we view the summary results\nsummary(ptLM)\n\n\nCall:\nlm(formula = Satisfaction ~ Age + illSeverity + Anxiety, data = pt)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.2812  -3.8635   0.6427   4.5324  11.8734 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 143.8952     5.8975  24.399  < 2e-16 ***\nAge          -1.1135     0.1326  -8.398 3.75e-08 ***\nillSeverity  -0.5849     0.1320  -4.430 0.000232 ***\nAnxiety       1.2962     1.0560   1.227 0.233231    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.037 on 21 degrees of freedom\nMultiple R-squared:  0.9035,    Adjusted R-squared:  0.8897 \nF-statistic: 65.55 on 3 and 21 DF,  p-value: 7.85e-11\n\n\nWe see several things. First, we can say that the intercept, age, and illness severity are all statistically significant. Anxiety does not appear to be significant. As expected given our individual results, our F-statistic (sometimes called a model utility test) shows us that there is at least one predictor that is significant. Further we can see that our \\(R^2\\) and \\(R_{adj}^2\\) are both relatively high, which shows that these predictors explain much of the variability in the data. We can see our RSE is about 7, which is not too extreme given our the range on our outputs.\nAs we do not find anxiety significant, we can drop it as an independent variable (we discuss model selection in the next chapter). Our new model is then:\n\nptLM2 <- lm(Satisfaction ~ Age + illSeverity, data = pt)\nsummary(ptLM2)\n\n\nCall:\nlm(formula = Satisfaction ~ Age + illSeverity, data = pt)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.2800  -5.0316   0.9276   4.2911  10.4993 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 143.4720     5.9548  24.093  < 2e-16 ***\nAge          -1.0311     0.1156  -8.918 9.28e-09 ***\nillSeverity  -0.5560     0.1314  -4.231 0.000343 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.118 on 22 degrees of freedom\nMultiple R-squared:  0.8966,    Adjusted R-squared:  0.8872 \nF-statistic: 95.38 on 2 and 22 DF,  p-value: 1.446e-11\n\n\nWe get similar results. We can then build our linear model:\n\\[[Patient Satisfaction] = 143 + -1.03[Age] + -0.556[Illness Severity] + \\epsilon\\]\n\nInterpretation\nWe can interpret this as saying that for every additional year of age, a patient’s satisfaction drops about a point and for every additional point of illness severity, a patient loses about half a point of satisfaction. That is, the older and sicker you are, the less likely you are to be satisfied. This generally seems to make sense.\n\n\nConfidence Intervals\nMoreover, we can use the model to show our confidence intervals on our coefficients using confint.\n\nconfint(ptLM2, level = .95)\n\n                 2.5 %      97.5 %\n(Intercept) 131.122434 155.8215898\nAge          -1.270816  -0.7912905\nillSeverity  -0.828566  -0.2835096\n\n# We can then say, with 95% confidence that our intercept is in the interval ~ (131, 156)\n\n\n\n\n\n\n\nNote\n\n\n\nconfint requires:\n\nA model, called ptLM2 in this case.\nYou can also pass it your 1-alpha level (the default is alpha = .05, or .95 confidence).\nYou can also pass it specific parameters to check (useful if working with amodel with many parameters).\n\n\n\n\n\nPredictions\nWe can use our model to predict a patient’s satisfaction given their age and illness severity using predict in the same manner as simple linear regression. For a point estimate:\n\npredict(ptLM2, list(Age = 35, illSeverity=50))\n\n       1 \n79.58325 \n\n\nAn individual response will be in this interval:\n\npredict(ptLM2,list(Age=35, illSeverity=50),interval=\"prediction\")\n\n       fit      lwr      upr\n1 79.58325 63.87546 95.29105\n\n\nThe mean response for someone with these inputs will be:\n\npredict(ptLM2,list(Age=35, illSeverity=50),interval=\"confidence\")\n\n       fit      lwr      upr\n1 79.58325 74.21262 84.95388\n\n\nWe can also plot these values. Note that our result with two predictors is a plane in this case. With 3+ predictors, it is a hyperplane. Often, we will plot either a contour map where each line corresponds to a fixed level of a predictor or just choose a single predictor.\nWe can produce a contour plot using geom_contour (one can also use stat_contour). Of course, this works for two predictors. As the number of independent variables increases, visualizing the data becomes somewhat more challenging and requires visualizing the solution only a few dimensions at a time.\n\n# Requires a set of points with predictions:\n# Produce a data frame that is every combination of Age and issEverity\nmySurface <- expand_grid( \n  Age = seq(min(pt$Age), max(pt$Age), by = 1), \n  illSeverity = seq(min(pt$illSeverity), max(pt$illSeverity), by = 1)) \n\n# look at our data\nhead(mySurface)\n\n\n\n  \n\n\n\nNow add in our predictions. Recall predict takes a data frame with columns that have the same name as the variables in the model.\n\nmySurface$Satisfaction <- predict(ptLM2, mySurface) \nhead(mySurface)\n\n\n\n  \n\n\n\nPlot the contours for our response surface.\n\nggplot(data = mySurface,\n       aes(x = Age, y = illSeverity, z = Satisfaction)) + \n  # you can use a number of ways to do this.  geom_contour works\n  geom_contour(aes(color = after_stat(level))) + \n  # This color argument varies the color of your contours by their level\n  scale_color_distiller(palette = 'Spectral', direction = -1) + \n  # clean up the plot\n  theme_minimal() + \n  xlab('Age') + ylab('Illness Severity') + \n  ggtitle('Patient Satisfaction Response Surface') \n\n\n\n\nWith this plot, we can clearly see at least two things:\n\nOur mathematical interpretation holds true. The younger and less severely ill the patient, the more satisfied they are (in general, as based on our model).\nOur model is a plane. We see this with the evenly spaced, linear contour lines.\n\nIt is also useful to overlay the actual observations on the plot. We can do this as follows:\n\n# This is our original contour plot as produced above, with one exception.  \n# We move the data for the contour to the geom_contour so we can also plot the observations\nggplot() + \n  geom_contour(data = mySurface, aes(x = Age, y = illSeverity, z = Satisfaction, color = after_stat(level))) + \n  # This color argument varies the color of your contours by their level\n  scale_color_distiller(palette = 'Spectral', direction = -1) + \n  theme_minimal() + xlab('Age') + ylab('Illness Severity') + \n  ggtitle('Patient Satisfaction Response Surface')  + \n  geom_point(data = pt, aes(x = Age, y = illSeverity, color = Satisfaction))\n\n\n\n\n\n\nOutliers\nBy plotting these points, we can compare our results (the contour lines) to the observations. The first thing this allows us to do is look for outliers. For example, there are two around the age 30 and a severity of illness; note how their colors are disjoint from what the contour colors predict. This, of course, is harder to interpret than a simple linear regression as it involves comparing colors. In general, it is easier to use the numbers for higher dimensional models. Second, we can get an idea of leverage or areas of our model that are not informed by data. For example, there are no observations in this region:\n\n\n\n\n\nThat means that any predictions in this region are ill-informed and extrapolations beyond the data. In an advanced design section, we will discuss how ensuring we get “coverage” or “space-filling” is an important property for good experimental designs so we can avoid this problem.\n\n\nAssumptions\nFinally, we can check our model to ensure that it is legitimate. Check assumptions:\n\n# Plot our standard diagnostic plots\npar(mfrow = c(2,2))\nplot(ptLM2)\n\n\n\n\nIt appears that we meet our linearity, independence, normality and homoscedasticity assumptions. There are no significant patterns, though we may have a few unusual observations.\n\n# Check normality\nshapiro.test(ptLM2$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ptLM2$residuals\nW = 0.95367, p-value = 0.3028\n\n\nCheck homoscedasticity.\n\ncar::ncvTest(ptLM2)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 1.380242, Df = 1, p = 0.24006\n\n\nWe meet our assumptions, so also check for unusual observations.\n\n# We can identify points that have residuals greater than 2 standard deviations away from our model's prediction\nptLM2$residuals[abs(ptLM2$residuals) >= 2*sd(ptLM2$residuals)]\n\n        9 \n-17.27998 \n\n# Point 9 is an outlier\n\nWe can check for leverage points with a number of ways. We’ll check using Cook’s distance.\n\nplot(ptLM2, which = 4)\n\n\n\n# Again point 9 is a point of significant leverage. \n\nBased on these results, we may consider dropping point nine. Before doing so, we should check for data entry errors or anything unusual about that data point. If we do drop it, we should note that we did so in our analysis.\nIf we do conclude that point nine should be dropped, we can build a new linear model:\n\n# Just check the summary of the model with Point 9 dropped\nsummary(lm(Satisfaction ~ Age + illSeverity, data = pt[-9,]))\n\n\nCall:\nlm(formula = Satisfaction ~ Age + illSeverity, data = pt[-9, \n    ])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.870  -3.700   0.834   3.595  12.169 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 147.9415     5.2186  28.349  < 2e-16 ***\nAge          -1.1484     0.1044 -11.003 3.54e-10 ***\nillSeverity  -0.5054     0.1120  -4.513 0.000191 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.003 on 21 degrees of freedom\nMultiple R-squared:  0.9292,    Adjusted R-squared:  0.9224 \nF-statistic: 137.8 on 2 and 21 DF,  p-value: 8.449e-13\n\n\nNote that our standard errors decrease somewhat and our \\(R^2\\) increases, indicating this is a better model (although on a smaller subset of the data)."
  },
  {
    "objectID": "tutorial/model_selection.html",
    "href": "tutorial/model_selection.html",
    "title": "Model Selection",
    "section": "",
    "text": "This post presents methods for finding a balance between under fitting and over fitting a model. Under fitting is when the model is a poor predictor of the response. With linear regression, this is largely addressed through diagnostic checks, which was covered in the tutorial on linear model assumptions. A linear model is over fitted when it includes more predictors than are needed to represent the relationship to the response variable. Appropriately reducing the complexity of the model improves its ability to make predictions based on new data, and it helps with interpretability.\nThere are three general approaches to reducing model complexity:\nDimension reduction is beyond the scope of this post and will not be covered. This tutorial presents two methods of variable selection (testing- and criterion-based methods) and regularization through lasso regression."
  },
  {
    "objectID": "tutorial/model_selection.html#testing-based-methods",
    "href": "tutorial/model_selection.html#testing-based-methods",
    "title": "Model Selection",
    "section": "Testing-Based Methods",
    "text": "Testing-Based Methods\nTesting-based methods are the easiest to implement but should only be considered when there are only a few predictors. The idea is simple. In forward elimination, we start with a linear model with no predictors, manually add them one at a time, and keep only those predictors with a low p-value. Backward elimination is just the opposite: we start with a linear model that contains all predictors (including interactions, if suspected), remove the predictor with the highest p-value, build a new linear model with the reduced set or predictors, and continue that process until only those predictors with low p-values remain.\nWe’ll use the teengamb dataset from the faraway package to demonstrate backward elimination. This dataset contains survey results from a study of teenage gambling in Britain. The response variable is gamble, which is the expenditure on gambling in pounds per year. The predictors are information regarding each survey respondent, such as gender and income.\n\nlibrary(faraway)\ndata(teengamb)\nhead(teengamb)\n\n\n\n  \n\n\n\nA linear model with all predictors is as follows (we’ll assume this model passes all of the required diagnostic checks):\n\ntg = lm(gamble~., data=teengamb)\nsummary(tg)\n\n\nCall:\nlm(formula = gamble ~ ., data = teengamb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-51.082 -11.320  -1.451   9.452  94.252 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  22.55565   17.19680   1.312   0.1968    \nsex         -22.11833    8.21111  -2.694   0.0101 *  \nstatus        0.05223    0.28111   0.186   0.8535    \nincome        4.96198    1.02539   4.839 1.79e-05 ***\nverbal       -2.95949    2.17215  -1.362   0.1803    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.69 on 42 degrees of freedom\nMultiple R-squared:  0.5267,    Adjusted R-squared:  0.4816 \nF-statistic: 11.69 on 4 and 42 DF,  p-value: 1.815e-06\n\n\nSince the p-value for status is the highest, we remove it first.\n\ntg = update(tg, . ~ . -status) # remove status\nsummary(tg)\n\n\nCall:\nlm(formula = gamble ~ sex + income + verbal, data = teengamb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-50.639 -11.765  -1.594   9.305  93.867 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  24.1390    14.7686   1.634   0.1095    \nsex         -22.9602     6.7706  -3.391   0.0015 ** \nincome        4.8981     0.9551   5.128 6.64e-06 ***\nverbal       -2.7468     1.8253  -1.505   0.1397    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.43 on 43 degrees of freedom\nMultiple R-squared:  0.5263,    Adjusted R-squared:  0.4933 \nF-statistic: 15.93 on 3 and 43 DF,  p-value: 4.148e-07\n\n\nThen we remove verbal.\n\ntg = update(tg, . ~ . -verbal) # remove verbal\nsummary(tg)\n\n\nCall:\nlm(formula = gamble ~ sex + income, data = teengamb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-49.757 -11.649   0.844   8.659 100.243 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    4.041      6.394   0.632  0.53070    \nsex          -21.634      6.809  -3.177  0.00272 ** \nincome         5.172      0.951   5.438 2.24e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.75 on 44 degrees of freedom\nMultiple R-squared:  0.5014,    Adjusted R-squared:  0.4787 \nF-statistic: 22.12 on 2 and 44 DF,  p-value: 2.243e-07\n\n\nNotice that even though we eliminated half of the predictors from the model, we only slightly reduced the adjusted \\(R^{2}\\). The simpler model explains almost as much variance in the response with only half the number of predictors. Something to keep in mind when conducting forward or backward elimination is that the predictor p-value does not necessarily have to be above 0.05 to eliminate the predictor from the model. You could also choose something higher - even up to around 0.15 to 0.20 if predictive performance is the goal. For example, note that the p-value for verbal in the second model was 0.14, and the adjusted \\(R^{2}\\) for the model was the highest of the three. The coefficient for verbal was also negative, which is what we’d expect: teens with higher verbal scores spend less money on gambling. We should therefore consider keeping verbal in the model. As you can see, there’s a little bit of an art to this method."
  },
  {
    "objectID": "tutorial/model_selection.html#criterion-based-methods",
    "href": "tutorial/model_selection.html#criterion-based-methods",
    "title": "Model Selection",
    "section": "Criterion-Based Methods",
    "text": "Criterion-Based Methods\nAs previously stated, testing-based procedures should only be considered when there are just a few factors to consider. The more potential factors in your model, the greater the chance that you’ll miss the optimal combination. We saw in the previous section that we had two competing goals: model simplicity versus model fit. Akaike (Akaike 1974) developed a method to measure this balance between simplicity and fit called the Akaike Information Criterion (AIC), which takes the form of:\n\\[AIC = 2(p+1) - 2ln(\\hat{L})\\]\nwhere,\n\n\\(p\\) is the number of predictors, and\n\\(\\hat{L}\\) is the maximized likelihood for the predictive model.\n\nWe then choose the model with the lowest AIC.\nThe Bayes Information Criterion (BIC) is an alternative to AIC and replaces \\(2(p+1)\\) with \\(ln(n)(p+1)\\), where \\(n\\) is the number of observations. Adding \\(ln(n)\\) increases the penalty for the number of factors in the model more for larger data sets. Which criterion you use can therefore depend on the dataset you’re working with.\nAnother common estimator of error is Mallow’s Cp, which is defined as:\n\\[C_{p}=\\frac{1}{n}(RSS+2p\\hat{\\sigma}^{2})\\]\nwhere,\n\n\\(RSS\\) is the root sum of squares,\n\\(p\\) is the number of predictor, and\n\\(\\hat{\\sigma}^{2}\\) is an estimate of the variance of the error, \\(\\varepsilon\\), in the linear regression equation.\n\nAs with AIC and BIC, the penalty term (in this case \\(2p\\hat{\\sigma}^{2}\\)) increases as the number of predictors in the model increases, which is intended to balance the corresponding decrease in \\(RSS\\). With each of these methods, as we vary \\(p\\), we get an associated criterion value from which we select the minimum as the best model. In R, we can calculate AIC and BIC with the bestglm() function from the bestglm package. Be aware that bestglm() expects the data to be in a dataframe with the response variable in the last column.\n\n\n\n\n\n\nImportant\n\n\n\nbestglm() is picky about how your dataset is structured. It expects a dataframe with the response variable in the last column and all other columns are predictors. Don’t include any other “extra” columns. Fortunately, teengamb is already set up that way.\n\n\n\nlibrary(bestglm)\n\ntg.AIC = bestglm(teengamb, IC=\"AIC\")\n\n# this will provide the best model\ntg.AIC\n\nAIC\nBICq equivalent for q in (0.672366796081496, 0.87054246206156)\nBest Model:\n              Estimate Std. Error   t value     Pr(>|t|)\n(Intercept)  24.138972 14.7685884  1.634481 1.094591e-01\nsex         -22.960220  6.7705747 -3.391177 1.502436e-03\nincome        4.898090  0.9551179  5.128256 6.643750e-06\nverbal       -2.746817  1.8252807 -1.504874 1.396672e-01\n\n\nNotice that verbal is included in the best fit model even though its p-value is > 0.05. Using summary(), we get a likelihood-ratio test for the best model compared to the null model.\n\nsummary(tg.AIC)\n\nFitting algorithm:  AIC-leaps\nBest Model:\n           df deviance\nNull Model 43 21641.54\nFull Model 46 45689.49\n\n    likelihood-ratio test - GLM\n\ndata:  H0: Null Model vs. H1: Best Fit AIC-leaps\nX = 24048, df = 3, p-value < 2.2e-16\n\n\nTo get the best model in a lm() format:\n\ntg.AIC$BestModel\n\n\nCall:\nlm(formula = y ~ ., data = data.frame(Xy[, c(bestset[-1], FALSE), \n    drop = FALSE], y = y))\n\nCoefficients:\n(Intercept)          sex       income       verbal  \n     24.139      -22.960        4.898       -2.747  \n\n\nWe can also see a comparison of the best model (model 1) to the next 4 best models.\n\ntg.AIC$BestModels\n\n\n\n  \n\n\n\nWe can also see the best model (row 3 below) and its subsets. Row 0 contains just the y-intercept, and in each successive row one predictor is added tp the model.\n\ntg.AIC$Subsets\n\n\n\n  \n\n\n\nUsing BIC, however, verbal is excluded from the best fit model.\n\ntg.BIC = bestglm(teengamb, IC=\"BIC\")\ntg.BIC\n\nBIC\nBICq equivalent for q in (0.0507226962510261, 0.672366796081496)\nBest Model:\n              Estimate Std. Error    t value     Pr(>|t|)\n(Intercept)   4.040829  6.3943499  0.6319374 5.306977e-01\nsex         -21.634391  6.8087973 -3.1774174 2.717320e-03\nincome        5.171584  0.9510477  5.4377755 2.244878e-06\n\n\nFor Mallow’s Cp, we can use the leaps package.\n\nlibrary(leaps)\n\n# leaps expects x and y to be passed separately\ntg.cp = leaps(x=teengamb[-5], y=teengamb$gamble, method=\"Cp\")\ntg.cp\n\n$which\n      1     2     3     4\n1 FALSE FALSE  TRUE FALSE\n1  TRUE FALSE FALSE FALSE\n1 FALSE FALSE FALSE  TRUE\n1 FALSE  TRUE FALSE FALSE\n2  TRUE FALSE  TRUE FALSE\n2 FALSE  TRUE  TRUE FALSE\n2 FALSE FALSE  TRUE  TRUE\n2  TRUE  TRUE FALSE FALSE\n2  TRUE FALSE FALSE  TRUE\n2 FALSE  TRUE FALSE  TRUE\n3  TRUE FALSE  TRUE  TRUE\n3  TRUE  TRUE  TRUE FALSE\n3 FALSE  TRUE  TRUE  TRUE\n3  TRUE  TRUE FALSE  TRUE\n4  TRUE  TRUE  TRUE  TRUE\n\n$label\n[1] \"(Intercept)\" \"1\"           \"2\"           \"3\"           \"4\"          \n\n$size\n [1] 2 2 2 2 3 3 3 3 3 3 4 4 4 4 5\n\n$Cp\n [1] 11.401283 30.984606 41.445676 45.517426  3.248323 12.003293 12.276400\n [8] 25.967108 26.743051 42.897591  3.034526  4.856329 10.256053 26.416920\n[15]  5.000000\n\n\nIt takes a little finagling to get the predictors that we should include in the best model. We want the index of the minimum value in $Cp, and we use that to find the corresponding row in $which to determine the predictors that should remain in the model. Columns 1, 2, and 4 correspond to sex, status, and verbal, which is the same as the AIC result.\n\ntg.cp$which[which.min(tg.cp$Cp), ]\n\n    1     2     3     4 \n TRUE FALSE  TRUE  TRUE"
  },
  {
    "objectID": "tutorial/model_selection.html#cross-validation",
    "href": "tutorial/model_selection.html#cross-validation",
    "title": "Model Selection",
    "section": "Cross Validation",
    "text": "Cross Validation\nAn alternative approach to using AIC, BIC, or Cp is to use cross validation (CV) to select the best model. The idea is that we randomly divide our data into a training set and a test set. An 80/20 split between the training set and test set is common but will depend on your sample size. For very large sample sizes (in the millions), the training set can contain a larger percentage, while for relatively small sample sizes, the split may be closer to 50/50.\nThe training set is further randomly divided into \\(k\\) subsets (also called folds), and one of these folds is withheld as the validation set. We fit a model to the remaining training set, and then measure the prediction error using the validation set. Typically, the prediction error is measured by the mean squared error (MSE) for a quantitative response variable. We repeat this process by cycling though each of the folds and holding it out as the validation set. The cross validated error (CV error) is then the average prediction error for the \\(k\\) folds.\nThe website for the scikit-learn module for Python has a good visualization (shown below) of these various data sets and a good explanation of this and other cross validation methods. A more thorough, academic treatment of cross validation may be found in Chapter 7.10 of Elements of Statistical Learning written by Trevor Hastie, Robert Tibshirani, and Jerome Friedman.\n\nOnce the CV process is complete, we re-combine each of the folds into a single training set for a final evaluation against the test set. With this approach, we can compare multiple CV methods and choose the method with the best performance.\nNotice that we are not using an Information Criterion (IC) anywhere in this method. Another difference is that with criterion-based methods, we chose the model with the lowest IC score, but with CV, we don’t choose the model with the lowest CV error. Instead, we calculate the standard deviation (\\(\\sigma\\)) of the CV error for each of the \\(p\\) predictors and then choose the smallest model that’s CV error is within one standard error of the lowest. Standard error is defined as \\(se = \\sigma/\\sqrt(k)\\). This is best shown graphically, which you’ll see below.\nCV techniques are particularly useful for datasets with many predictors, but for consistency, we’ll stick with the teengamb dataset. Below, we’ll perform k-fold cross validation on the teamgamb dataset, once again using bestglm(). We’ll use an 80/20 train/test split.\n\nset.seed(2)\ntest_set = sample(47, 10, replace=FALSE)  # randomly select row indices\ntg_test = teengamb[test_set, ]              # create test set\ntg_train = teengamb[-test_set, ]            # create training set \n\nThe training set has only 24 observations, so if we further partition it into a large number of folds, we’ll have a small number of observations in each of the validation folds. For this example, we’ll choose just 3 folds. In the bestglm() function, we specify CV as the IC and pass three arguments to specify cross validation parameters. As mentioned, there are a variety of cross validation methods to choose from. For the method described above, we specify Method=\"HTF\", which you might have noticed are the first letters of the last names of the authors mentioned in the “Elements of Statistical Learning” reference above. K=3 specifies the number of k-folds, and we can chose one or more repetition with REP. Remember that cross validation randomly partitions the data into folds, so if we want to repeat the CV process with different random partitions, we increase the REP value. Due to the small sample size and number of folds, we’ll do 10 repetitions.\n\ntg.cv = bestglm(tg_train, IC=\"CV\", CVArgs=list(Method=\"HTF\", K=3, REP=10))\ntg.cv\n\nCV(K = 3, REP = 10)\nBICq equivalent for q in (0.000199326484859652, 0.329344259543028)\nBest Model:\n             Estimate Std. Error    t value     Pr(>|t|)\n(Intercept) -6.132874   6.892883 -0.8897401 3.796805e-01\nincome       5.877955   1.149221  5.1147292 1.134028e-05\n\n\nThe model above is the model with the fewest predictors that is within one standard error of the model with the lowest CV error. To illustrate this relationship, next we’ll visualize how this model was determined based on the CV and standard errors. We can get the CV errors and the \\(se\\) from the tg.cv object.\n\n\n\n\n\n\nWhat About The Test Set?\nThis model selection method included income as the only predictor variable in their respective best model. However, the coefficients differ between the two models, so now we can bring in the test set and compare against the best BIC model. For a fair comparison with the CV results, we’ll find the best model using BIC on the training set only.\n\n# get the BIC model on the training set only\ntg_train.BIC = bestglm(tg_train, IC=\"BIC\")\nbic_preds = predict(tg_train.BIC$BestModel, newdata = data.frame(tg_test[, -5]))\n\nprint(\"BIC predictors included are:\")\n\n[1] \"BIC predictors included are:\"\n\nprint(tg_train.BIC$BestModel$coefficients)\n\n(Intercept)         sex      income \n   3.515245  -19.116151    5.362915 \n\n\nNow we’ll get the CV model.\n\n# based on the CV results, only income should be included as a factor\ncv.glm = glm(gamble~income, data=tg_train)\ncv_preds = predict(cv.glm, newdata = data.frame(tg_test[, -5]))\n\nWe’ll use mean absolute error as our measure of error.\n\n# calculate and compare mean absolute error\nprint(paste(\"BIC mean absolute error:\", round(mean(abs(bic_preds - tg_test$gamble)), 1)))\n\n[1] \"BIC mean absolute error: 10.9\"\n\nprint(paste(\"CV mean absolute error:\", round(mean(abs(cv_preds - tg_test$gamble)), 1)))\n\n[1] \"CV mean absolute error: 16.3\"\n\n\nUsing mean absolute error, BIC out-performed the cross-validated model. This result shouldn’t be too surprising given that the BIC model contained additional predictor variables that appeared to be statistically significant."
  },
  {
    "objectID": "tutorial/model_selection.html#lasso-regression",
    "href": "tutorial/model_selection.html#lasso-regression",
    "title": "Model Selection",
    "section": "Lasso Regression",
    "text": "Lasso Regression\nRidge and lasso regression are closely related regularization techniques to reduce model complexity. The primary difference between the two methods is that ridge regression reduces factor coefficients close to (but not equal to) zero, while lasso regression reduces the coefficients all the way to zero, which makes it useful for reducing model complexity by eliminating factors.\n\nBackground Reading\nFor the theoretical framework, please refer to this article. Don’t worry about the Python code if you’re not familiar with it. Just read the text portions of the article that explain the how ridge and, more importantly, lasso regression work.\n\n\nLasso Regression In R\nLasso regression is particularly useful when a dataset has many factors, but we’ll continue to use the teengamb data so we can compare the results with the stepAIC() method. Performing lasso regression with the glmnet package is straight forward. The function has two required arguments, an x and a y, where x are the data associated with the predictors (note x must be a data.matrix, not a data.frame), and y is the response as a vector. By default, glmnet automatically scales and centers the data, and then converts them back to the original scale when providing results. If we plot the results, we get the following.\n\nlibrary(glmnet)\n\n# for some reason, glmnet works best with data.matrix instead of as.matrix\nx = data.matrix(tg_train[-5])\ny = tg_train$gamble\n\ntg.lasso = glmnet(x, y)\nplot(tg.lasso, xvar=\"lambda\", label=TRUE)\n\n\n\n\nEach of the above lines represents a predictor. The number next to each line on the left side of the plot refers to the column number in the x matrix. The vertical axis represents the factor coefficient. The bottom x axis is \\(log(\\lambda)\\), and the top x axis is the associated number of predictors included in the model.\nSo how do we interpret this plot? At the far right, we can see that the coefficient for every predictor is zero. In other words, this is the null model. As \\(\\lambda\\) decreases, predictors are added one at a time to the model. Since predictor #3 (income) is the first to have a non-zero coefficient, it is the most significant. sex (predictor #1) is the next non-zero coefficient followed by verbal (predictor #4) and then status (predictor #2). If we compare this order with the p-values from the best fit linear model, we see that there is consistency. Note that income was the first non-zero coefficient, and it has the lowest p-value in the linear model. Also note that the maximum coefficients in the lasso regression plot are also consistent with the linear model coefficients.\nOur task now is to find the model that has good predictive power while including only the most significant predictors. In other words, we need a method to find the right \\(\\lambda\\) value. Before we get to how we identify that \\(\\lambda\\), let’s look at some other useful information from tg.lasso. If we print our glmnet object, we see (going by columns from left to right) the number of predictors included in the model (Df, not to be confused with the degrees of freedom in a linear model summary), the percent of null deviance explained, and the associated \\(\\lambda\\) value.\n\nprint(tg.lasso)\n\n\nCall:  glmnet(x = x, y = y) \n\n   Df  %Dev  Lambda\n1   0  0.00 21.9800\n2   1  7.26 20.0300\n3   1 13.29 18.2500\n4   1 18.30 16.6300\n5   1 22.45 15.1500\n6   1 25.90 13.8100\n7   1 28.77 12.5800\n8   1 31.15 11.4600\n9   2 34.07 10.4400\n10  2 36.78  9.5160\n11  2 39.03  8.6710\n12  2 40.90  7.9010\n13  2 42.46  7.1990\n14  2 43.75  6.5590\n15  2 44.82  5.9770\n16  2 45.71  5.4460\n17  3 46.46  4.9620\n18  3 47.38  4.5210\n19  3 48.14  4.1190\n20  3 48.77  3.7530\n21  3 49.30  3.4200\n22  3 49.73  3.1160\n23  3 50.10  2.8390\n24  3 50.40  2.5870\n25  3 50.65  2.3570\n26  3 50.85  2.1480\n27  3 51.03  1.9570\n28  3 51.17  1.7830\n29  3 51.29  1.6250\n30  3 51.39  1.4800\n31  3 51.47  1.3490\n32  3 51.54  1.2290\n33  3 51.59  1.1200\n34  3 51.64  1.0200\n35  3 51.68  0.9298\n36  3 51.71  0.8472\n37  3 51.74  0.7719\n38  3 51.76  0.7033\n39  3 51.78  0.6409\n40  3 51.79  0.5839\n41  3 51.80  0.5320\n42  4 51.83  0.4848\n43  4 51.85  0.4417\n44  4 51.87  0.4025\n45  4 51.89  0.3667\n46  4 51.90  0.3341\n47  4 51.91  0.3045\n48  4 51.92  0.2774\n49  4 51.93  0.2528\n50  4 51.93  0.2303\n51  4 51.94  0.2099\n52  4 51.94  0.1912\n53  4 51.95  0.1742\n54  4 51.95  0.1587\n55  4 51.95  0.1446\n56  4 51.95  0.1318\n57  4 51.96  0.1201\n58  4 51.96  0.1094\n59  4 51.96  0.0997\n60  4 51.96  0.0908\n61  4 51.96  0.0828\n62  4 51.96  0.0754\n63  4 51.96  0.0687\n64  4 51.96  0.0626\n\n\nWe can also see the coefficient values for any given \\(\\lambda\\) with coef. We can see that small values of \\(\\lambda\\) include more predictors and so correspond with the right side of the plot above. We can get the coefficients for any given \\(\\lambda\\) value with coef(). If we choose the smallest values of \\(\\lambda\\) from the above data, we get:\n\n# Note that we specify lambda with s\ncoef(tg.lasso, s=0.0626)\n\n5 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s1\n(Intercept)  18.03799848\nsex         -18.24900253\nstatus        0.08230075\nincome        5.20255057\nverbal       -2.69223049\n\n\nNow we can more directly compare these coefficients to the full linear model coefficients. Recall that we withheld a test set prior to performing lasso regression, so the coefficients are close, but not equal to the linear model coefficients.\n\nsumary(lm(gamble~., data=teengamb))\n\n              Estimate Std. Error t value  Pr(>|t|)\n(Intercept)  22.555651  17.196803  1.3116   0.19677\nsex         -22.118330   8.211115 -2.6937   0.01011\nstatus        0.052234   0.281112  0.1858   0.85349\nincome        4.961979   1.025392  4.8391 1.792e-05\nverbal       -2.959493   2.172150 -1.3625   0.18031\n\nn = 47, p = 5, Residual SE = 22.69034, R-Squared = 0.53\n\n\nIf we choose a \\(\\lambda\\) associated with 2 Df, we see that only two predictors have non-zero coefficients.\n\ncoef(tg.lasso, s=5.9770)\n\n5 x 1 sparse Matrix of class \"dgCMatrix\"\n                   s1\n(Intercept)  5.858393\nsex         -8.912086\nstatus       .       \nincome       4.039764\nverbal       .       \n\n\nTo find the optimal value for \\(\\lambda\\), we use cross validation again. We can include cross validation in the glmnet() function by prepending cv. as shown below. The default number of folds in the cv.glmnet function is 10, which is fine for this example. There’s a built-in method for plotting the results as we did manually above.\n\ntg.cv = cv.glmnet(x, y)\nplot(tg.cv)\n\n\n\n\nWhat we get is the cross validation curve (red dots) and two values for \\(\\lambda\\) (vertical dashed lines). The left dashed line is the value of lambda that gives the minimum mean cross-validated error. The right dashed line is the value of \\(\\lambda\\) whose error is within one standard deviation of the minimum. This is the \\(\\lambda\\) we’ve been after. We can get the coefficients associated with this \\(\\lambda\\) by specifying s = \"lambda.1se\". Our cross validated best fit lasso regression model is shown below.\n\ncoef(tg.cv, s = \"lambda.1se\")\n\n5 x 1 sparse Matrix of class \"dgCMatrix\"\n                   s1\n(Intercept) 12.864028\nsex          .       \nstatus       .       \nincome       1.826509\nverbal       .       \n\n\nFor a more thorough discussion of the glmnet package, including its use with non-Gaussian data, refer to the vignette written by Trevor Hastie and Junyang Qian."
  },
  {
    "objectID": "tutorial/model_selection.html#parting-thought",
    "href": "tutorial/model_selection.html#parting-thought",
    "title": "Model Selection",
    "section": "Parting Thought",
    "text": "Parting Thought\nIn this chapter, we have seen that different methods for model selection can produce different “best” models, which might make you leery about the whole thing. Remember the George Box quote:\n\nAll models are wrong…\n\nWe’re just trying to find one that’s useful."
  },
  {
    "objectID": "tutorial/nn_regression.html",
    "href": "tutorial/nn_regression.html",
    "title": "Neural Network Regression",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "tutorial/nn_regression.html#neural-network-regression",
    "href": "tutorial/nn_regression.html#neural-network-regression",
    "title": "Neural Network Regression",
    "section": "Neural Network Regression",
    "text": "Neural Network Regression\nLike support vector machines and tree-based models, neural networks can be applied to both regression and classification tasks. Neural networks originated in the field of neurophysiology as an attempt to model human brain activity at the neuron level (Warren S. McCulloch 1943), but it wasn’t until the 1980s and 1990s (Bishop 1995) that they began to be developed into their current form. Neural network models are fit using a training algorithm that slowly reduces prediction error using a process called gradient descent.\n\nSimple Neural Network Model\nBefore we get into that, let’s look at visualization of a neural network regression model in it’s simplest form: one to solve for \\(\\beta_0\\) and \\(\\beta_1\\) given the equation \\(y=\\beta_0x_0+\\beta_1x_1+\\epsilon\\) where we know \\(x_0=1\\). Below, the two blue circles are referred to as the input layer and consist of two nodes: an input node that will be used to solve for \\(\\beta_1\\), and a bias node to solve for \\(\\beta_0\\), the y-intercept. Each node of the input layer is connected to the output layer, which consists of just one node because we’ll be predicting a single continuous variable, \\(\\hat{y}\\). If this was a classification problem, and we were trying to classify the three types of irises found in the iris data set, then the output layer would have three nodes, each producing a probability. There is a model parameter, referred to as a weight, associated with each connected node as indicated by the \\(\\omega_0\\) and \\(\\omega_1\\) terms. The output node produces a prediction, \\(\\hat{y}\\), using an activation function. In the case of linear regression, we use a linear activation function of the form \\(f \\left( \\sum\\limits_{h}{\\omega_h} x_h \\right)\\). That’s it - that’s the model!\n\n\n\n\n\n\n\nGradient Descent\nThe algorithm used to train the model is called gradient descent, and to demonstrate how it works, we need to set the stage first. Let’s assume that we’re trying to find the \\(\\beta\\)s that have the following relationship with the predictor:\n\\[\ny=1+0.5x+\\epsilon\n\\]\nWe’ll create a data set with 10 observations and fit a linear model for comparison later.\n\nset.seed(42)\nnn_reg = tibble(\n  x = runif(10, 0, 5),\n  y = 1 + 0.5*x + rnorm(10, sd=0.5)\n)\n\nggplot(nn_reg, aes(x=x, y=y)) +\n  geom_point() +\n  geom_smooth(formula='y~x', method='lm', se=FALSE) +\n  coord_fixed(xlim=c(0,5), ylim=c(0,5)) +\n  ggtitle(\"Linear Model Fit\") +\n  theme_bw()\n\n\n\nnn.lm = lm(y ~ x, data=nn_reg)\n\nsummary(nn.lm)\n\n\nCall:\nlm(formula = y ~ x, data = nn_reg)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.67369 -0.38063 -0.08963  0.41550  0.75801 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.4127     0.4516   0.914 0.387494    \nx             0.7641     0.1324   5.773 0.000418 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5164 on 8 degrees of freedom\nMultiple R-squared:  0.8064,    Adjusted R-squared:  0.7822 \nF-statistic: 33.32 on 1 and 8 DF,  p-value: 0.000418\n\n\nBefore the model is trained, its weights are initialized with random numbers. I’ll just pick two random numbers between -1 and 1.\n\nset.seed(42)\n(w0 = runif(1, -1, 1))\n\n[1] 0.8296121\n\n(w1 = runif(1, -1, 1))\n\n[1] 0.8741508\n\n\nSince these model parameters are just random numbers, the predictions will not be very accurate. That’s ok, though, and it’s the starting point for all untrained neural network models. We’ll go through the following iterative process to slowly train the model to make more and more accurate predictions.\nThe Training Process\n\nMake predictions for input values.\nMeasure the difference between those predictions and the true values (called the loss).\nCompute the partial derivative (gradient) of the loss with respect to the model parameters.\nUpdate the model parameters using the partial derivative values computed in the previous step.\nRepeat this process until the loss is either unchanged or is sufficiently low.\n\nThe next several code chunks demonstrate this process one step at a time.\nStep 1. Make predictions.\nWe can make predictions manually using the randomly initialized weight and bias.\n\nget_estimate = function(omega0, omega1){nn_reg$x * omega1 + omega0}\n(y_hat = get_estimate(w0, w1))\n\n [1] 4.828004 4.925338 2.080258 4.459294 3.634524 3.098453 4.049059 1.418207\n [9] 3.701164 3.911277\n\n\nStep 2. Calculate the loss.\nThere are a number of ways we could do this, but for this example, we’ll calculate the loss by determining the mean squared error of the predictions and the target values. Mean squared error is defined as:\n\\[\nMSE = \\frac{1}{n} \\sum\\limits_{i=1}^{n}{\\left( y_i - \\hat{y}_i \\right)^2}\n\\]\n\nmse = function(predicted){1/length(predicted)* sum((nn_reg$y - predicted)^2)}\n# the loss\n(loss = mse(y_hat))\n\n[1] 0.8201885\n\n\nStep 3. Compute the partial derivatives of the loss.\nAt this point, we have two random values for \\(\\omega_0\\) and \\(\\omega_1\\) and an associated loss (error). Now we need to find new values for the ωs that will decrease the loss. How do we do that? For each ω, we need to determine whether we should increase or decrease it’s value and by how much. We determine whether to increase or decrease its value by calculating the gradient of the loss function at the current ω values. To demonstrate graphically, the loss as a function of \\(\\omega_0\\) and \\(\\omega_1\\) are plotted below.\n\n# sequence of w0 values\nBvec = seq(from=(w0-1), to=(w0+1), length.out = 21)\n\n# calculate loss while holding w1 constant\nBloss = Bvec %>% \n  map(function(x) get_estimate(x, w1)) %>% \n  map_dbl(function(x) mse(x))\n\n# get a curve through the points and get the gradient\nBspl = smooth.spline(Bloss ~ Bvec)\n\n# get the gradient at w0\nBgrad = predict(Bspl, x=w0, deriv=1)$y\n\n# same thing for w1\nWvec = seq(from=(w1-1), to=(w1+1), length.out = 21)\n\nWloss = Wvec %>% \n  map(function(x) get_estimate(w0,x)) %>% map_dbl(function(x) mse(x))\n\nWspl = smooth.spline(Wloss ~ Wvec)\n\nWgrad = predict(Wspl, x=w1, deriv=1)$y\n\nw0plot = ggplot() + \n  geom_line(aes(x=Bvec, y=Bloss), color='red', size=1.5) + \n  geom_line(aes(x=c(w0-0.5, w0+0.5), \n                y=c(Bloss[11]-Bgrad/2, Bloss[11]+Bgrad/2)), \n            color='blue', size=1.5) +\n  geom_point(aes(x=Bvec[11], y = Bloss[11]), size=5) +\n  annotate(\"text\", x=0.5, y=2, \n           label=paste(\"Slope =\", round(Bgrad, 2))) +\n  coord_fixed(ylim=c(0,3.5)) +\n  xlab(\"w0\") + ylab(\"Loss\") +\n  theme_bw()\n\nw1plot = ggplot() + \n  geom_line(aes(x=Wvec, y=Wloss), color='red', size=1.5) + \n  geom_line(aes(x=c(w1-0.5, w1+0.5), \n                y=c(Wloss[11]-Wgrad/2, Wloss[11]+Wgrad/2)), \n            color='blue', size=1.5) +\n  geom_point(aes(x=Wvec[11], y=Wloss[11]), size=5) +\n  annotate(\"text\", x=1.4, y=0.5, label=paste(\"Slope =\", round(Wgrad, 2))) +\n  coord_fixed(ylim=c(0,3.5)) +\n  xlab(\"w1\") + ylab(\"Loss\") +\n  theme_bw()\n\ngridExtra::grid.arrange(w0plot, w1plot, nrow=1, ncol=2)\n\n\n\n\nIn practice, the partial derivatives are calculated as follows:\n\n(Bpartial = sum(-(nn_reg$y - y_hat)))\n\n[1] 7.670526\n\n(Wpartial = sum(-(nn_reg$y - y_hat) * nn_reg$x))\n\n[1] 26.07812\n\n\nStep 4. Update the model parameters.\nFrom the above plots, we can see that to decrease the loss, we need to decrease both \\(\\omega\\)s. In fact, the following rules always apply:\n\nWith a positive gradient, decrease the parameter value.\nWith a negative gradient, increase the parameter value.\n\nWe know we need to decrease the parameter values, so now we need to determine how much to decrease them. Right now, all we have to go on are the magnitude of the gradients. If we decreased the parameters by their respective gradients, the new parameter values would be far to the left on both plots above - we would overshoot the bottom of the curve in both cases. Instead of using the full gradient value, it appears that the parameters should be updated as follows:\n\\[\n\\omega_{new} = (\\omega_{old}) - (\\omega_{gradient})(\\alpha)\n\\]\nWhere \\(\\alpha\\) is a multiplier in the range [0, 1], and is referred to as the learning rate. For our example, an α of 0.01 will suffice, but keep in mind that α is a hyperparameter that often must be tuned. The code below selects \\(\\alpha\\), updates the parameter values, and recalculates the loss. Notice that the loss has decreased as expected.\n\nalpha = 0.001\nw0 = w0 - Bpartial * alpha\nw1 = w1 - Wpartial * alpha\nmse(get_estimate(w0, w1))\n\n[1] 0.6816571\n\n\nNext we’ll put all this in a loop, iterate through a number of times, and see what we get for parameter estimates. I’ll start from the beginning and capture the parameters and loss as training progresses through 5000 iterations. Below, the parameter estimates are plotted for each iteration and compared to the linear model coefficients.\n\nset.seed(42)\nw0 = runif(1, -1, 1)\nw1 = runif(1, -1, 1)\nalpha = 0.001\nw0s = w0\nw1s = w1\nfor (j in 1:500){\n  for (i in 1:nrow(nn_reg)){\n    y_hat = get_estimate(w0, w1)\n    Bgrad = sum(-(nn_reg$y - y_hat))\n    Wgrad = sum(-(nn_reg$y - y_hat) * nn_reg$x)\n    w0 = w0 - Bgrad * 0.001\n    w1 = w1 - Wgrad * 0.001\n    w0s = c(w0s, w0)\n    w1s = c(w1s, w1)\n  }\n}\n\nwHistory = tibble(\n  iter = 1:length(w0s),\n  b = w0s,\n  w = w1s\n)\n\nlibrary(gganimate)\n\nggplot(wHistory) +\n  geom_point(aes(x=iter, y=w0s, color='w_0 Estimate', group=seq_along(iter))) +\n  geom_hline(yintercept=coef(nn.lm)[1]) + \n  annotate(\"text\", x=1200, y=0.43, label=\"Linear Model Intercept Coefficient\") +\n  geom_point(aes(x=iter, y=w1s, color='w_1 Estimate', group=seq_along(iter))) +\n  geom_hline(yintercept=coef(nn.lm)[2]) + \n  annotate(\"text\", x=1300, y=0.785, label=\"Linear Model Slope Coefficient\") +\n  scale_color_manual(name=\"Legend\", values=c('blue', 'red')) + \n  xlab(\"Iteration\") + ylab(\"Parameter Value\") +\n  theme_bw() +\n  transition_reveal(iter)\n\n\n\n\nTo visualize the gradient descent methodology, calculate the loss for a range of \\(\\omega_0\\) and \\(\\omega_1\\) values and plot the loss function as a surface.\n\nloss_fn = expand_grid(bs=seq(0.3,0.9,length.out=20), ws=seq(0.5,1,length.out=20))\n  \nloss_fn$Loss = 1:nrow(loss_fn) %>% \n  map(function(x) get_estimate(loss_fn[x,'bs'] %>% .$bs, loss_fn[x,'ws']%>% .$ws)) %>% \n  map_dbl(function(x) mse(x))\n\nggplot(wHistory) +\n  geom_raster(data=loss_fn, aes(x=bs, y=ws, fill=Loss)) +\n  geom_contour(data=loss_fn, aes(x=bs, y=ws, z=Loss), color='white', bins=28) +\n  geom_point(aes(x=b, y=w, group=seq_along(iter)), color='yellow', size=5) +\n  xlab(\"Intercept\") + ylab(\"Slope\") +\n  theme_bw() +\n  transition_time(iter) +\n  labs(title = paste(\"Gradient Descent Iteration:\", \"{round(frame_time, 0)}\")) +\n  shadow_wake(wake_length = 0.2)\n\n\n\n\nWe can also see the regression line update as training progresses.\n\nggplot(wHistory) +\n  geom_smooth(data=nn_reg, aes(x=x, y=y), formula='y~x', method='lm', se=FALSE) +\n  geom_abline(aes(intercept=b, slope=w), color='red') +\n  geom_point(data=nn_reg, aes(x=x, y=y)) +\n  coord_fixed(xlim=c(0,5), ylim=c(0,5)) +\n  theme_bw() +\n  transition_time(iter) +\n  labs(title = paste(\"Gradient Descent Iteration:\", \"{round(frame_time, 0)}\")) \n\n\n\n\nLet’s compare the final parameter estimates from the neural network model to the linear model coefficients.\n\ntibble(\n  Model = c(\"Linear\", \"Neural Network\"),\n  w_0 = c(coef(nn.lm)[1], tail(w0s, 1)),\n  w_1 = c(coef(nn.lm)[2], tail(w1s, 1))\n)\n\n\n\n  \n\n\n\nThere are a variety of R packages to simplify the process of fitting a neural network model. Since we’re doing regression and not something for complicated like image classification, natural language processing, or reinforcement learning, a package like nnet provides everything we need.\n\nnnModel = nnet::nnet(y ~ x, data = nn_reg,   # formula notation is the same as lm()\n                     linout = TRUE,          # specifies linear output (instead of logistic)\n                     decay = 0.001,          # weight decay\n                     maxit = 100,            # stop training after 100 iterations\n                     size = 0, skip = TRUE)  # no hidden layer (covered later)\n\n# weights:  2\ninitial  value 31.444863 \nfinal  value 2.134476 \nconverged\n\nsummary(nnModel)\n\na 1-0-1 network with 2 weights\noptions were - skip-layer connections  linear output units  decay=0.001\n b->o i1->o \n 0.41  0.76 \n\n\nFrom the model summary, we see that has two weights: one for \\(\\omega_0\\) and one for \\(\\omega_1\\). The initial and final values are model error terms. Converged means that training stopped before it reached maxit. The model takes the form 1-0-1, which means it has one input node (the bias, or intercept, node is automatically included) in the input layer, 0 nodes in the hidden layer (we’ll cover hidden layers later), and one node in the output layer. The b->o term is our \\(\\omega_0\\) (intercept) parameter value, and i1->o is our \\(\\omega_1\\) (slope) parameter value. We can extract the coefficients the usual way.\n\ncoef(nnModel)\n\n     b->o     i1->o \n0.4125070 0.7641276 \n\n\n\n\nMultiple Linear Regression\nThe neural network model can be easily expanded to accommodate additional predictors by adding a node to the input layer for each additional predictor and connecting it to the output node. Below is a comparison of coefficients obtained from a linear model and a neural network model for a data set with three predictors.\n\n# make up data\nset.seed(42)\nmlr = tibble(\n  x1 = runif(10, 0, 5),\n  x2 = runif(10, 0, 5),\n  x3 = runif(10, 0, 5),\n  y = 1 + 0.5*x1 - 0.5*x2 + x3 + rnorm(10, sd=0.5)\n)\n\n# linear model coefficients\ncoef(lm(y ~ ., data=mlr))\n\n(Intercept)          x1          x2          x3 \n  2.0447549   0.5355719  -0.7312496   0.8035532 \n\n\n\n# neural network coefficients\ncoef(nnet::nnet(y ~ ., data = mlr, linout = TRUE, decay = 0.001, maxit = 100,\n                     size = 0, skip = TRUE))\n\n# weights:  4\ninitial  value 195.927838 \nfinal  value 4.149504 \nconverged\n\n\n      b->o      i1->o      i2->o      i3->o \n 2.0399505  0.5361400 -0.7307887  0.8040190 \n\n\nIf you think this seems like overkill just to model a linear relationship between two variables, I’d agree with you. But consider this:\n\nWhat if the relationship between two variables isn’t linear?\nWhat if there are dozens of predictor variables and dozens of response variables and the underlying relationships are highly complex?\n\nIn cases like these, neural networks models can be very beneficial. To make that leap, however, we need to give our neural network model more power by giving it the ability to model these complexities. We do that by adding one or more hidden layers to the model.\n\n\nHidden Layer\nNeural network models become universal function approximators with the addition of one or more hidden layers. Hidden layers fall between the input layer and the output layer. Adding more predictor variables and one hidden layer, we get the following network.\n\n\n\n\n\nWe’ve introduced a new variable ν, and a new function u. The ν variables are trainable weights just like the w variables. The u functions are activation functions as described earlier. Typically, all nodes in a hidden layer share a common type of activation function. A variety of activation functions have been developed, a few of which are shown below. For many applications, a rectified linear, activation function is a good choice for hidden layers.\n\n\n\n\n\n\n\n\nActivation Function\nActivation Function Formula\nOutput Type\n\n\n\n\nThreshold\n\\(f(u) = \\begin{Bmatrix} 1, u\\gt0 \\\\ 0, u\\le0 \\end{Bmatrix}\\)\nBinary\n\n\nLinear\n\\(f(u) = u\\)\nNumeric\n\n\nLogistic\n\\(f(u) = \\frac{e^u}{1+e^u}\\)\nNumeric Between 0 & 1\n\n\nRectified Linear\n\\(f(u) = \\begin{Bmatrix} u, u\\gt0 \\\\ 0, u\\le0 \\end{Bmatrix}\\)\nNumeric Positive\n\n\n\nAs stated, adding a hidden layer turns the neural network model into a universal function approximator. For the case of regression, we can think of this as giving the neural network the ability to model non-linear functions without knowing what the nature of the nonlinear relationship is. Compare that to linear regression. If we had the relationship \\(y=x^2\\), we would need to transform either the response or predictor variable in a linear regression model, and this requires knowing the order of the polynomial to get a good fit. With neural network regression, we don’t need this knowledge. Instead, we allow the hidden layer to learn the nature of the relationship through the model training process. To demonstrate, we’ll use the exa data set from the faraway package, which looks like this.\n\nexa = faraway::exa\n\nggplot(exa) +\n  geom_line(aes(x=x, y=m), color='red', size=1.5) +\n  geom_point(aes(x=x, y=y)) +\n  ggtitle(\"Simulated Data (Black) and True Function (Red)\") +\n  theme_bw()\n\n\n\n\nTo fit a linear model with a 10-node hidden layer, we specify size = 10, and since 100 iterations might not be enough to converge, we’ll increase maxit to 500. Otherwise, everything else is the same. With 2 nodes in the input layer (1 for the bias, 1 for x) and 11 nodes in the hidden layer (1 for the bias, and 10 for those we specified), the model will have 31 weights to train.\n\nnnModel2 = nnet::nnet(y ~ x, data = exa, linout = TRUE, decay = 10^(-4), maxit = 500, size = 10)\n\n# weights:  31\ninitial  value 499.220789 \niter  10 value 78.378021\niter  20 value 48.290911\niter  30 value 40.726235\niter  40 value 27.840931\niter  50 value 26.694405\niter  60 value 25.771768\niter  70 value 25.636428\niter  80 value 25.582253\niter  90 value 25.475199\niter 100 value 25.237757\niter 110 value 24.502290\niter 120 value 24.125822\niter 130 value 23.715466\niter 140 value 23.644963\niter 150 value 23.592656\niter 160 value 23.541215\niter 170 value 23.460968\niter 180 value 23.394390\niter 190 value 23.326002\niter 200 value 23.299889\niter 210 value 23.294752\niter 220 value 23.253858\niter 230 value 23.176888\niter 240 value 23.124385\niter 250 value 23.104052\niter 260 value 23.084055\niter 270 value 23.080214\niter 280 value 23.078985\niter 290 value 23.076738\niter 300 value 23.074305\niter 310 value 23.073038\niter 320 value 23.072321\nfinal  value 23.071986 \nconverged\n\n\n\npreds = predict(nnModel2, newdata=data.frame(x=exa$x))\n\nggplot() +\n  geom_line(data=exa, aes(x=x, y=m, color='True Function'), size=1.5) +\n  geom_line(aes(x=exa$x, y=preds, color='Model Fit'), size=1.5) +\n  geom_point(data=exa, aes(x=x, y=y)) +\n  scale_color_manual(name=\"Legend\", values=c('blue', 'red')) + \n  theme_bw()\n\n\n\n\nThe plot above indicates the model over fit the data somewhat, although we only know this because we have the benefit of knowing the true function. We could reduce the amount of over fitting by choosing different hyperparameter values (decay, or the number of hidden layer nodes) or by changing the default training stopping criteria.\nThe trained model’s weights are saved with the model, and all 31 are shown below. This highlights a significant drawback of neural network regression. Earlier when we used a neural network model to estimate the slope and intercept, the model weights had meaning: we could directly interpret the weights as slope and intercept. How do we interpret the weights below? I have no idea! The point is that neural network models can be trained to make highly accurate predictions…but at the cost of interpretability.\n\nnnModel2$wts\n\n [1] -16.5006608  20.3693099 -29.4278236  34.4051715 -20.7471248  28.4091270\n [7] -29.1180265  40.7327613   2.3060868   1.9215748  -1.0624319   2.9527926\n[13]   2.8259165 -15.1652645   1.6957639   0.5958946   3.7441769 -21.0803998\n[19]   6.3092685  -8.3407380   1.3998851  19.2726000 -11.2056916 -23.2575100\n[25]   9.5932587   2.2295197   3.1400221   6.4367782   0.4575623  -4.5860998\n[31]  -6.2028427"
  },
  {
    "objectID": "tutorial/nn_regression.html#neural-network-classification",
    "href": "tutorial/nn_regression.html#neural-network-classification",
    "title": "Neural Network Regression",
    "section": "Neural Network Classification",
    "text": "Neural Network Classification\nNeural network models have been extremely successful when applied to classification tasks such as image classification and natural language processing. These models are highly complex and are built using sophisticated packages such as TensorFlow (developed by Google) and PyTorch (developed by Facebook). Building complex models for those kinds of classification tasks are beyond the scope of this tutorial. Instead, this section provides a high-level overview of classification using the nnet package and the iris data set.\nThe neural network training algorithm for classification is the same as for regression, but for classification, we need to change some of the attributes of the model itself. Instead of a linear activation function in the output layer, we need to use the softmax function. Doing so will cause the output layer to produce probabilities for each of the three flower species (this is accomplished by simply removing linout = TRUE from the nnet() function. Additionally, we use a categorical cross entropy loss function instead of mean squared error. Below, I also set rang = 0.1 to scale the predictors to be in the range recommended in the function help. We’ll also create the same training/test split as in the non-parametric regression chapter so we can directly compare results.\n\n# create a training and a test set\nset.seed(0)\ntrain = caTools::sample.split(iris, SplitRatio = 0.8)\niris_train = subset(iris, train == TRUE)\niris_test = subset(iris, train == FALSE)\n\n# train the model\nirisModel = nnet::nnet(Species ~ ., data = iris_train, \n                       size=2,       # only two nodes in the hidden layer\n                       maxit=200,    # stopping criteria\n                       entropy=TRUE, # switch for entropy\n                       decay=5e-4,   # weight decay hyperparameter\n                       rang=0.1)     # scale input values\n\n# weights:  19\ninitial  value 131.916499 \niter  10 value 75.933787\niter  20 value 56.873818\niter  30 value 55.958531\niter  40 value 55.852468\niter  50 value 55.797951\niter  60 value 51.382669\niter  70 value 11.286807\niter  80 value 7.667108\niter  90 value 7.657444\niter 100 value 7.643547\niter 110 value 7.642436\niter 120 value 7.641672\niter 130 value 7.640995\niter 140 value 7.640338\niter 150 value 7.627436\niter 160 value 7.377487\niter 170 value 5.671710\niter 180 value 4.882362\niter 190 value 4.810287\niter 200 value 4.793128\nfinal  value 4.793128 \nstopped after 200 iterations\n\n# make predictions on test data\niris_preds = predict(irisModel, newdata=iris_test)\nhead(iris_preds)\n\n      setosa  versicolor    virginica\n5  0.9956710 0.004329043 6.376970e-16\n10 0.9911190 0.008881042 2.881866e-15\n15 0.9976175 0.002382547 1.824318e-16\n20 0.9957198 0.004280170 6.226948e-16\n25 0.9739313 0.026068739 2.794827e-14\n30 0.9901521 0.009847885 3.581197e-15\n\n\nThis first six predictions for the test set are shown above, and notice that the values are in fact probabilities. The model is highly confident that each one of these first six predictions are setosa. Recall from the SVM and CART sections of the non-parametric regression chapter that both of those models misclassified test set observations #24 and #27 as versicolor that are actually virginica. Below we see that the neural network model correctly predicts both observations but is less confident about observation #24.\n\niris_preds[c(24,27), ]\n\n          setosa versicolor virginica\n120 7.349090e-11 0.18967040 0.8103296\n135 5.460005e-13 0.03105806 0.9689419\n\n\nThe confusion matrix for the entire test set reveals that the model has an accuracy of 100%.\n\niris_cm = cvms::confusion_matrix(\n  targets = iris_test[, 5], \n  predictions = colnames(iris_preds)[max.col(iris_preds)])\n\ncvms::plot_confusion_matrix(iris_cm$`Confusion Matrix`[[1]], add_zero_shading = FALSE) + \n  ggtitle(\"Neural Network Confusion Matrix\")"
  },
  {
    "objectID": "tutorial/np_anova.html",
    "href": "tutorial/np_anova.html",
    "title": "Nonparametric ANOVA",
    "section": "",
    "text": "As we’ve seen in the last few posts, linear models can be successfully applied to many data sets. However, there may be times when even after transforming variables, your model clearly violates the assumptions of linear models. Alternatively, you may have evidence that there is a complex relationship between predictor and response that is difficult to capture through transformation. In these cases, non-parametric regression techniques offer alternative methods for modeling your data."
  },
  {
    "objectID": "tutorial/np_anova.html#non-parametric-anova",
    "href": "tutorial/np_anova.html#non-parametric-anova",
    "title": "Nonparametric ANOVA",
    "section": "Non-Parametric ANOVA",
    "text": "Non-Parametric ANOVA\nOne of the assumptions of parametric ANOVA is that the underlying data are normally distributed. If we have a data set that violates that assumption, as is often the case with counts (especially of relatively rare events), then we’ll need to use a non-parametric method such as the Kruskal-Wallis (KW) test.\nThe setup for the KW test is as follows:\n\nLevel 1: \\(X_{11}, X_{12}, ...,X_{1J_{1}} \\sim F_{1}\\)\nLevel 2: \\(X_{21}, X_{22}, ...,X_{2J_{2}} \\sim F_{2}\\)\n…\nLevel I: \\(X_{I1}, X_{I2}, ...,X_{IJ_{I}} \\sim F_{I}\\)\n\nNull hypothesis \\(H_{o}: F_{1} = F_{2} = ... = F_{I}\\)\nAlternative hypothesis \\(H_{a}:\\) not \\(H{o}\\) (i.e., at least two of the distributions are different).\nThe idea behind the KW test is to sort the data and use an observation’s rank instead of the value itself. If we have a sample size \\(N = J_{1}, J_{2}, ... J_{I}\\), we first rank the observations (the lowest value gets a 1, second lowest a 2, etc.). If there are ties, then use the mid-rank (the mean of the two ranks). Then, separate the samples into their respective levels and sum the ranks for each level. For example, if we have three levels:\n\nLevel 1: \\(R_{11} + R_{12} + ... + R_{1J_{1}}=\\) sum of ranks\nLevel 2: \\(R_{21} + R_{22} + ... + R_{2J_{1}}=\\) sum of ranks\nLevel 3: \\(R_{31} + R_{32} + ... + R_{3J_{1}}=\\) sum of ranks\n\nNow we do the non-parametric equivalent of a sum of squares for treatment (SSTr) using these ranks. The KW test statistic takes the form:\n\\[K=\\frac{12}{N(N+1)}\\sum\\limits_{i=1}^{I}{\\frac{R^2_{i}}{J_{i}}-3(N+1)}\\]\nFor hypothesis testing, we calculate a p-value where large values of K signal rejection. We do this by approximating K with a chi-square distribution having \\(I-1\\) degrees of freedom. According to Devore (2015), chi-square is a good approximation for K if:\n\nIf there are I = 3 treatments and each sample size is >= 6, or\nIf there are I > 3 treatments and each sample size is >= 5\n\nLet’s look at an example. Say we are a weapon manufacturer that produces rifles on three different assembly lines, and we want to know if there’s a difference in the number of times a weapon jams. We select 10 random weapons from each assembly line and perform our weapon jamming test identically on all 30 weapons. I’ll make up some dummy data for this situation by drawing random numbers from Poisson distributions with two different \\(\\lambda\\)s.\n\nlibrary(tidyverse)\nset.seed(42)\n\nj = c(rpois(10, 10), rpois(10, 10), rpois(10, 15))\nr = rank(j, ties.method=\"average\")\nl = paste(\"line\", rep(1:3, each = 10), sep=\"\")\n\ntibble(\n  jams1 = j[1:10],\n  rank1 = r[1:10],\n  jams2 = j[11:20],\n  rank2 = r[11:20],\n  jams3 = j[21:30],\n  rank3 = r[21:30])\n\n\n\n  \n\n\n\nFor this data,\n\n\\(I = 3\\)\n\\(J_{1} = J_{2} = J_{3} = 10\\)\n\\(N = 10 + 10 + 10 = 30\\)\n\\(R_{1} =\\) 120\n\\(R_{2} =\\) 131.5\n\\(R_{3} =\\) 213.5\n\nWhich gives the K statistic:\n\\[K=\\frac{12}{30(31)} \\left[\\frac{120^2}{10} + \\frac{131.5^2}{10} + \\frac{213.5^2}{10}\\right] - 3(31)\\]\n\nK = 12/(30*31) * (sum(r[1:10])^2/10 + sum(r[11:20])^2/10 + sum(r[21:30])^2/10) - 3*31\nprint(paste(\"K =\", K), quote=FALSE)\n\n[1] K = 6.70903225806452\n\n\nThen, compute an approximate p-value using the chi-square test with two degrees of freedom.\n\n1 - pchisq(K, df=2)\n\n[1] 0.03492627\n\n\nWe therefore reject the null hypothesis at the \\(\\alpha = 0.5\\) test level. Of course, there’s an R function kruskall.test() so we don’t have to do all that by hand.\n\nrifles = tibble(jams = j, line = l)\n\nkruskal.test(jams ~ line, data = rifles)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  jams by line\nKruskal-Wallis chi-squared = 6.7845, df = 2, p-value = 0.03363\n\n\n\nMultiple Comparisons\nThe KW test can also be used for multiple comparisons. Recall that we applied the Tukey Test to the parametric case, and it took into account that there is an increase in the probability of a Type I error when conducting multiple comparisons. The non-parametric equivalent is to combine the Bonferroni Method with the KW test. Consider the case where we conduct \\(m\\) tests of the null hypothesis. We calculate the probability that at least one of the null hypotheses is rejected (\\(P(\\bar{A})\\)) as follows:\n\\[P(\\bar{A}) = 1-P(A) = 1-P(A_{1} \\cap A_{2} \\cap...\\cap A_{m})\\]\n\\[=1-P(A_{1})P(A_{2})\\cdot\\cdot\\cdot P(A_{m})\\]\n\\[=1-(1-\\alpha)^{m}\\]\nSo if our individual test level target is \\(\\alpha=0.05\\) and we conduct \\(m=10\\) tests, then \\(P(\\bar{A})=1-(1-0.05)^{10}=\\) 0.4012631. If we want to establish a family-wide Type I error rate, \\(\\Psi=P(\\bar{A})=0.05\\), then the individual test levels should be:\n\\[\\alpha=P(\\bar{A}_{i})=1-(1-\\Psi)^{1/m}\\]\nFor example, if \\(\\Psi=0.05\\) and we again conduct \\(m=10\\) tests, then \\(\\alpha=1-(1-0.05)^{1/10}=\\) 0.0051162. The downfall of this approach is that the same data are used to test the collection of hypotheses, which violates the independence assumption. The Bonferroni Inequality saves us from this situation because it doesn’t rely on the assumption of independence. Using the Bonferroni method, we simply calculate \\(\\alpha=\\Psi/m = 0.005\\). Note that this is almost identical to the result when we assumed independence. Unfortunately, the method isn’t perfect. As noted in Section 2 of this paper, the Bonferroni method tends to be overly conservative, which increases the chance of a false negative.\nUsing the rifles data from earlier, if we wanted to conduct all three pair-wise hypothesis tests, then \\(\\alpha=0.05/3=0.01667\\) for the individual KW tests.\nThe dunn.test() function from the aptly-named dunn.test package performs the multiple pair-wise tests and offers several methods for accounting for performing multiple tests, including Bonferroni. For example (and notice we get the KW test results also):\n\ndunn.test::dunn.test(rifles$jams, rifles$line, method=\"bonferroni\")\n\n  Kruskal-Wallis rank sum test\n\ndata: x and group\nKruskal-Wallis chi-squared = 6.7845, df = 2, p-value = 0.03\n\n                           Comparison of x by group                            \n                                 (Bonferroni)                                  \nCol Mean-|\nRow Mean |      line1      line2\n---------+----------------------\n   line2 |  -0.293738\n         |     1.0000\n         |\n   line3 |  -2.388222  -2.094483\n         |     0.0254     0.0543\n\nalpha = 0.05\nReject Ho if p <= alpha/2\n\n\nAccording to the dunn.test documentation, the null hypothesis for the Dunn test is:\n\nThe null hypothesis for each pairwise comparison is that the probability of observing a randomly selected value from the first group that is larger than a randomly selected value from the second group equals one half; this null hypothesis corresponds to that of the Wilcoxon-Mann-Whitney rank-sum test. Like the ranksum test, if the data can be assumed to be continuous, and the distributions are assumed identical except for a difference in location, Dunn’s test may be understood as a test for median difference. ‘dunn.test’ accounts for tied ranks."
  },
  {
    "objectID": "tutorial/random_forest.html",
    "href": "tutorial/random_forest.html",
    "title": "Random Forests",
    "section": "",
    "text": "As with support vector machines, and as the name implies, classification and regression trees (CART) can be used for either classification or regression tasks. Again, we’ll start with regression and then move to classification.\n\nRegression Trees\nThe algorithm is best explained as we walk through an example, and we’ll continue to use the airquality data set. The basic machine learning algorithm used in tree-based methods follows these steps:\n\nConsider the entire data set including all predictors and the response. We call this the root node, and it is represented by the top center node in the figure below. The information displayed in the node includes the mean response for that node (42.1 is the mean of Ozone for the whole data set), the number of observations in the node (n=116), and the percent of the overall observations in the node.\nIterate through each predictor, \\(k\\), and split the data into two subsets (referred to as the left and right child nodes) using some threshold, \\(t_k\\). For example, with the airquality data set, the predictor and threshold could be Temp >= 83. The choice of \\(k\\) and \\(t_k\\) for a given split is the pair that increases the “purity” of the child nodes (weighted by their size) the most. We’ll explicitly define purity shortly. If you equate a data split with a decision, then at this point, we have a basic decision tree.\n\n\n\n\n\n\n\nEach child node in turn becomes the new parent node and the process is repeated. Below is the decision tree produced by the first two splits. Notice that the first split is on the Temp predictor, and the second split is on the Wind predictor. Although we don’t have coefficients for these two predictors like we would in a linear model, we can still interpret the order of the splits as the predictor’s relative significance. In this case, Temp is the most significant predictor of Ozone followed by Wind. After two splits, the decision tree has three leaf nodes, which are those in the bottom row. We can also define the depth of the tree as the number rows in the tree below the root node (in this case depth = 2). Note that the sum of the observations in the leaf nodes equals the total number of observations (69 + 10 + 37 = 116), and so the percentages shown in the leaf nodes sum to 100%.\n\n\n\n\n\n\n\nContinuing the process once more, we see that the third split is again on Temp but at a different \\(t_k\\).\n\n\n\n\n\n\nIf we continued to repeat the process until each observation was in its own node, then we would have drastically over-fit the model. To control over-fitting, we stop the splitting process when some user-defined condition (or set of conditions) is met. Example stopping conditions include a minimum number of observations in a node or a maximum depth of the tree. We can also use cross validation with a 1 standard error rule to limit the complexity of the final model.\nWe’ll stop at this point and visually represent this model as a scatter plot. The above leaves from left to right are labeled as Leaf 1 - 4 on the scatter plot.\n\n\n\n\n\nPlotting predicted Ozone on the z-axis produces the following response surface, which highlights the step-like characteristic of regression tree predictions.\n\n\n\n\n\n\nPlotting just Temp versus Ozone in two dimensions further highlights a difference between this method and linear regression. From this plot we can infer that linear regression may outperform CART if there is a smooth trend in the relationship between the predictors and response because CART does not produce smooth estimates.\n\n\n\n\n\n\nImpurity Measure\nPreviously, it was stated that the predictor-threshold pair chosen for a split was the pair that most increased the purity (or, decreased the impurity) of the child nodes. A node with all identical response values will have an impurity of 0, so that as a node becomes more impure, it’s impurity value increases. We will then define a node’s impurity to be proportional to the residual deviance, which for a continuous response variable like Ozone, is the residual sum of squares (RSS).\n\\[RSS = \\sum\\limits_{i\\:in\\: Node}{(y_{i} - \\bar{y})^2}\\]\nwhere \\(\\bar{y}\\) is the mean of the y’s in the node.\nWe’ll start with the first split. To determine which predictor-threshold pair decreases impurity the most, start with the first factor, send the lowest Ozone value to the left node and the remainder to the right node, and calculate RSS for each child node (\\(RSS_{left}\\) and \\(RSS_{right}\\)). The decrease in impurity for this split is \\(RSS_{root} - (RSS_{left} + RSS_{right})\\). Then send the lowest two Ozone values to the left node and the remainder to the right. Repeat this process for each predictor-threshold pair, and split the data based using the pair that decreased impurity the most. Any regression tree package will iterate through all of these combinations for you, but to demonstrate the process explicitly, We’ll just consider the Temp predictor for the first split.\n\n# we'll do a lot of filtering, so convert dataframe to tibble for convenience\n# we'll also drop the NA's for the calculations (but the regression tree\n# methodology itself doesn't care if there are NA's or not)\naq  = as_tibble(airquality) %>% drop_na(Ozone)\n\n# root node deviance\nroot_dev = sum((aq$Ozone - mean(aq$Ozone))^2) \n\n# keep track of the highest decrease\nbest_split = 0\n\n# iterate through all the unique Temp values\nfor(s in sort(unique(aq$Temp))){\n  left_node = aq %>% filter(Temp <= s) %>% .$Ozone\n  left_dev = sum((left_node - mean(left_node))^2)\n  right_node = aq %>% filter(Temp > s) %>% .$Ozone\n  right_dev = sum((right_node - mean(right_node))^2)\n  split_dev = root_dev - (left_dev + right_dev)\n  if(split_dev > best_split){\n    best_split = split_dev\n    temp = s + 1}  # + 1 because we filtered Temp <= s and Temp is integer\n}\n\nprint(paste(\"Best split at Temp <\", temp), quote=FALSE)\n\n[1] Best split at Temp < 83\n\n\n\n\nTree Deviance\nArmed with our impurity measure, we can also calculate the tree deviance, which we’ll use to calculate the regression tree equivalent of \\(R^2\\). For the tree with the four leaf nodes, we calculate the deviance for each leaf.\n\n# leaf 1\nleaf_1 = aq %>% filter(Temp < 83 & Wind >= 7.15) %>% .$Ozone\nleaf_1_dev = sum((leaf_1 - mean(leaf_1))^2)\n# leaf 2\nleaf_2 = aq %>% filter(Temp < 83 & Wind < 7.15) %>% .$Ozone\nleaf_2_dev = sum((leaf_2 - mean(leaf_2))^2)\n# leaf 3\nleaf_3 = aq %>% filter(Temp >= 83 & Temp < 88) %>% drop_na(Ozone) %>% .$Ozone\nleaf_3_dev = sum((leaf_3 - mean(leaf_3))^2)\n# leaf 4\nleaf_4 = aq %>% filter(Temp >= 88) %>% drop_na(Ozone) %>% .$Ozone\nleaf_4_dev = sum((leaf_4 - mean(leaf_4))^2)\n\nThe tree deviance is the sum of the leaf node deviances, which we use to determine how much the entire tree decreases the root deviance.\n\ntree_dev = sum(leaf_1_dev, leaf_2_dev, leaf_3_dev, leaf_4_dev)\n\n(root_dev - tree_dev) / root_dev\n\n[1] 0.6119192\n\n\nThe tree decreases the root deviance by 61.2%, which also means that 61.2% of the variability in Ozone is explained by the tree.\n\n\nPrediction\nMaking a prediction with a new value is easy as following the logic of the decision tree until you end up in a leaf node. The mean of the response values for that leaf node is the prediction for the new value.\n\n\nPros And Cons\nRegression trees have a lot of good things going for them:\n\nThey are easy to explain combined with an intuitive graphic output\nThey can handle categorical and numeric predictor and response variables\nThey easily handle missing data\nThey are robust to outliers\nThey make no assumptions about normality\nThey can accommodate “wide” data (more predictors than observations)\nThey automatically include interactions\n\nRegression trees by themselves and as presented so far have two major drawbacks:\n\nThey do not tend to perform as well as other methods (but there’s a plan for this that makes them one of the best prediction methods around)\nThey do not capture simple additive structure (there’s a plan for this, too)\n\n\n\nRegression Trees in R\nThe regression trees shown above were grown using the rpart and rpart.plot packages. I didn’t show the code so that we could focus on the algorithm first. Growing a regression tree is as easy as a linear model. The object created by rpart() contains some useful information.\n\nlibrary(rpart)\nlibrary(rpart.plot)\n\naq.tree = rpart(Ozone ~ ., data=airquality)\n\naq.tree\n\nn=116 (37 observations deleted due to missingness)\n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 116 125143.1000 42.12931  \n   2) Temp< 82.5 79  42531.5900 26.54430  \n     4) Wind>=7.15 69  10919.3300 22.33333  \n       8) Solar.R< 79.5 18    777.1111 12.22222 *\n       9) Solar.R>=79.5 51   7652.5100 25.90196  \n        18) Temp< 77.5 33   2460.9090 21.18182 *\n        19) Temp>=77.5 18   3108.4440 34.55556 *\n     5) Wind< 7.15 10  21946.4000 55.60000 *\n   3) Temp>=82.5 37  22452.9200 75.40541  \n     6) Temp< 87.5 20  12046.9500 62.95000  \n      12) Wind>=8.9 7    617.7143 45.57143 *\n      13) Wind< 8.9 13   8176.7690 72.30769 *\n     7) Temp>=87.5 17   3652.9410 90.05882 *\n\n\nFirst, we see that the NAs were deleted, and then we see the tree structure in a text format that includes the node number, how the node was split, the number of observations in the node, the deviance, and the mean response. To plot the tree, use rpart.plot() or prp().\n\nrpart.plot(aq.tree)\n\n\n\n\nrpart.plot() provides several options for customizing the plot, among them are digits, type, and extra, which I invoked to produce the earlier plots. Refer to the help to see all of the options.\n\nrpart.plot(aq.tree, digits = 3, type=4, extra=101)\n\n\n\n\nAnother useful function is printcp(), which provides a deeper glimpse into what’s going on in the algorithm. Here we see that just three predictors were used to grow the tree (Solar.R, Temp, and Wind). This means that the other predictors did not significantly contribute to increasing node purity, which is equivalent to a predictor in a linear model with a high p-value. We also see the root node error (weighted by the number of observations in the root node).\nIn the table, printcp() provides optimal tuning based on a complexity parameter (CP), which we can manipulate to manually “prune” the tree, if desired. The relative error column is the amount of reduction in root deviance for each split. For example, in our earlier example with three splits and four leaf nodes, we had a 61.2% reduction in root deviance, and below we see that at an nsplit of 3, we also get \\(1.000 - 0.388 = 61.2\\)%.1 xerror and xstd are cross-validation error and standard deviation, respectfully, so we get cross validation built-in for free!\n\nprintcp(aq.tree)\n\n\nRegression tree:\nrpart(formula = Ozone ~ ., data = airquality)\n\nVariables actually used in tree construction:\n[1] Solar.R Temp    Wind   \n\nRoot node error: 125143/116 = 1078.8\n\nn=116 (37 observations deleted due to missingness)\n\n        CP nsplit rel error  xerror    xstd\n1 0.480718      0   1.00000 1.01137 0.16829\n2 0.077238      1   0.51928 0.56189 0.17935\n3 0.053962      2   0.44204 0.60667 0.18095\n4 0.025990      3   0.38808 0.52541 0.15202\n5 0.019895      4   0.36209 0.50832 0.14892\n6 0.016646      5   0.34220 0.48282 0.13541\n7 0.010000      6   0.32555 0.47948 0.13535\n\n\nWith plotcp() we can see the 1 standard error rule implemented in the same manner we’ve seen before to identify the best fit model. At the top of the plot, the number of splits is displayed so that we can choose two splits when defining the best fit model.\n\nplotcp(aq.tree, upper = \"splits\")\n\n\n\n\nSpecify the best fit model using the cp parameter with a value slightly greater than shown in the table.\n\nbest_aq.tree = rpart(Ozone ~ ., cp=0.055, data=airquality)\n\nrpart.plot(best_aq.tree)\n\n\n\n\nAs with lm() objects, the summary() function provides a wealth of information. Note the results following variable importance. Earlier we opined that the first split on Temp indicated that is was the most significant predictor followed by Wind. The rpart documentation provides a detailed description of variable importance:\n\nAn overall measure of variable importance is the sum of the goodness of split measures for each split for which it was the primary variable, plus goodness * (adjusted agreement) for all splits in which it was a surrogate.\n\nNote that the results are scaled so that they sum to 100, which is useful for directly comparing each predictor’s relative contribution.\n\nsummary(aq.tree)\n\nCall:\nrpart(formula = Ozone ~ ., data = airquality)\n  n=116 (37 observations deleted due to missingness)\n\n          CP nsplit rel error    xerror      xstd\n1 0.48071820      0 1.0000000 1.0113676 0.1682859\n2 0.07723849      1 0.5192818 0.5618853 0.1793542\n3 0.05396246      2 0.4420433 0.6066727 0.1809490\n4 0.02598999      3 0.3880808 0.5254143 0.1520220\n5 0.01989493      4 0.3620909 0.5083200 0.1489209\n6 0.01664620      5 0.3421959 0.4828189 0.1354091\n7 0.01000000      6 0.3255497 0.4794807 0.1353517\n\nVariable importance\n   Temp    Wind     Day Solar.R   Month \n     60      28       8       2       2 \n\nNode number 1: 116 observations,    complexity param=0.4807182\n  mean=42.12931, MSE=1078.819 \n  left son=2 (79 obs) right son=3 (37 obs)\n  Primary splits:\n      Temp    < 82.5  to the left,  improve=0.48071820, (0 missing)\n      Wind    < 6.6   to the right, improve=0.40426690, (0 missing)\n      Solar.R < 153   to the left,  improve=0.21080020, (5 missing)\n      Month   < 6.5   to the left,  improve=0.11595770, (0 missing)\n      Day     < 24.5  to the left,  improve=0.08216807, (0 missing)\n  Surrogate splits:\n      Wind < 6.6   to the right, agree=0.776, adj=0.297, (0 split)\n      Day  < 10.5  to the right, agree=0.724, adj=0.135, (0 split)\n\nNode number 2: 79 observations,    complexity param=0.07723849\n  mean=26.5443, MSE=538.3746 \n  left son=4 (69 obs) right son=5 (10 obs)\n  Primary splits:\n      Wind    < 7.15  to the right, improve=0.22726310, (0 missing)\n      Temp    < 77.5  to the left,  improve=0.22489660, (0 missing)\n      Day     < 24.5  to the left,  improve=0.13807170, (0 missing)\n      Solar.R < 153   to the left,  improve=0.10449720, (2 missing)\n      Month   < 8.5   to the right, improve=0.01924449, (0 missing)\n\nNode number 3: 37 observations,    complexity param=0.05396246\n  mean=75.40541, MSE=606.8356 \n  left son=6 (20 obs) right son=7 (17 obs)\n  Primary splits:\n      Temp    < 87.5  to the left,  improve=0.300763900, (0 missing)\n      Wind    < 10.6  to the right, improve=0.273929800, (0 missing)\n      Solar.R < 273.5 to the right, improve=0.114526900, (3 missing)\n      Day     < 6.5   to the left,  improve=0.048950680, (0 missing)\n      Month   < 7.5   to the left,  improve=0.007595265, (0 missing)\n  Surrogate splits:\n      Wind  < 6.6   to the right, agree=0.676, adj=0.294, (0 split)\n      Month < 7.5   to the left,  agree=0.649, adj=0.235, (0 split)\n      Day   < 27.5  to the left,  agree=0.622, adj=0.176, (0 split)\n\nNode number 4: 69 observations,    complexity param=0.01989493\n  mean=22.33333, MSE=158.2512 \n  left son=8 (18 obs) right son=9 (51 obs)\n  Primary splits:\n      Solar.R < 79.5  to the left,  improve=0.22543670, (1 missing)\n      Temp    < 77.5  to the left,  improve=0.21455360, (0 missing)\n      Day     < 27    to the left,  improve=0.05183544, (0 missing)\n      Wind    < 10.6  to the right, improve=0.04850548, (0 missing)\n      Month   < 8.5   to the right, improve=0.01998100, (0 missing)\n  Surrogate splits:\n      Temp < 63.5  to the left,  agree=0.794, adj=0.222, (1 split)\n      Wind < 16.05 to the right, agree=0.750, adj=0.056, (0 split)\n\nNode number 5: 10 observations\n  mean=55.6, MSE=2194.64 \n\nNode number 6: 20 observations,    complexity param=0.02598999\n  mean=62.95, MSE=602.3475 \n  left son=12 (7 obs) right son=13 (13 obs)\n  Primary splits:\n      Wind    < 8.9   to the right, improve=0.269982600, (0 missing)\n      Month   < 7.5   to the right, improve=0.078628670, (0 missing)\n      Day     < 18.5  to the left,  improve=0.073966850, (0 missing)\n      Solar.R < 217.5 to the left,  improve=0.058145680, (3 missing)\n      Temp    < 85.5  to the right, improve=0.007674142, (0 missing)\n\nNode number 7: 17 observations\n  mean=90.05882, MSE=214.8789 \n\nNode number 8: 18 observations\n  mean=12.22222, MSE=43.17284 \n\nNode number 9: 51 observations,    complexity param=0.0166462\n  mean=25.90196, MSE=150.0492 \n  left son=18 (33 obs) right son=19 (18 obs)\n  Primary splits:\n      Temp    < 77.5  to the left,  improve=0.27221870, (0 missing)\n      Wind    < 10.6  to the right, improve=0.09788213, (0 missing)\n      Day     < 22.5  to the left,  improve=0.07292523, (0 missing)\n      Month   < 8.5   to the right, improve=0.04981065, (0 missing)\n      Solar.R < 255   to the right, improve=0.03603008, (1 missing)\n  Surrogate splits:\n      Month < 6.5   to the left,  agree=0.686, adj=0.111, (0 split)\n      Wind  < 10.6  to the right, agree=0.667, adj=0.056, (0 split)\n\nNode number 12: 7 observations\n  mean=45.57143, MSE=88.2449 \n\nNode number 13: 13 observations\n  mean=72.30769, MSE=628.9822 \n\nNode number 18: 33 observations\n  mean=21.18182, MSE=74.573 \n\nNode number 19: 18 observations\n  mean=34.55556, MSE=172.6914 \n\n\nThe best fit model contains two predictors and explains 55.8% of the variance in Ozone as shown below.\n\nprintcp(best_aq.tree)\n\n\nRegression tree:\nrpart(formula = Ozone ~ ., data = airquality, cp = 0.055)\n\nVariables actually used in tree construction:\n[1] Temp Wind\n\nRoot node error: 125143/116 = 1078.8\n\nn=116 (37 observations deleted due to missingness)\n\n        CP nsplit rel error  xerror    xstd\n1 0.480718      0   1.00000 1.01076 0.16854\n2 0.077238      1   0.51928 0.59931 0.18107\n3 0.055000      2   0.44204 0.66824 0.18089\n\n\nHow does it compare to a linear model with the same two predictors? The linear model explains 56.1% of the variance in Ozone, which is only slightly more than the regression tree. Earlier I claimed there was a plan for improving the performance of regression trees. That plan is revealed in the next section on Random Forests.\n\nsummary(lm(Ozone~Wind + Temp, data=airquality))\n\n\nCall:\nlm(formula = Ozone ~ Wind + Temp, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-41.251 -13.695  -2.856  11.390 100.367 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -71.0332    23.5780  -3.013   0.0032 ** \nWind         -3.0555     0.6633  -4.607 1.08e-05 ***\nTemp          1.8402     0.2500   7.362 3.15e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.85 on 113 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.5687,    Adjusted R-squared:  0.5611 \nF-statistic:  74.5 on 2 and 113 DF,  p-value: < 2.2e-16\n\n\n\n\n\nRandom Forest Regression\nIn 1994, Leo Breiman at UC, Berkeley published this paper in which he presented a method he called Bootstrap AGGregation (or BAGGing) that improves the predictive power of regression trees by growing many trees (a forest) using bootstrapping techniques (thereby making it a random forest). The details are explained in the link to the paper above, but in short, we grow many trees, each on a bootstrapped sample of the training set (i.e., sample \\(n\\) times with replacement from a data set of size \\(n\\)). Then, to make a prediction, we either let each tree “vote” and predict based on the most votes, or we use the average of the estimated responses. Cross-validation isn’t necessary with this method because each bootstrapped tree has an internal error, referred to as the out-of-bag (OOB) error. With this method, about a third of the samples are left out of the bootstrapped sample, a prediction is made, and the OOB error calculated. The algorithm stops when the OOB error begins to increase.\nA drawback of the method is that larger trees tend to be correlated with each other, and so in a 2001 paper, Breiman developed a method to lower the correlation between trees. For each bootstrapped sample, his idea was to use a random selection of predictors to split each node. The number of randomly selected predictors, mtry, is a function of the total number of predictors in the data set. For regression, the randomForest() function from the randomForest package uses \\(1/k\\) as the default mtry value, but this can be manually specified. The following code chunks demonstrate the use of some of the randomForest functions. First, we fit a random forest model and specify that we want to assess the importance of predictors, omit NAs, and randomly sample two predictors at each split (mtry). There are a host of other parameters that can be specified, but we’ll keep them all at their default settings for this example.\n\nlibrary(randomForest)\n\nset.seed(42)\n\naq.rf<- randomForest(Ozone~., importance=TRUE, na.action=na.omit, mtry=2, data=airquality)\naq.rf\n\n\nCall:\n randomForest(formula = Ozone ~ ., data = airquality, importance = TRUE,      mtry = 2, na.action = na.omit) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n          Mean of squared residuals: 301.7377\n                    % Var explained: 72.5\n\n\nThis random forest model consists of 500 trees and explains 72.% of the variance in Ozone, which is a nice improvement over the 55.8% we got with the single regression tree. Plotting the aq.rf object shows the error as a function of the size of the forest. We want to see the error stabilize as the number of trees increases, which it does in the plot below.\n\nplot(aq.rf)\n\n\n\n\n\nInterpretation\nWhen the relationships between predictors and response are non-linear and complex, random forest models generally perform better than standard linear models. However, the increase in predictive power comes with a corresponding decrease in interpretability. For this reason, random forests and some other machine learning-based models such as neural networks are sometimes referred to as “black box” models. If you are applying machine learning techniques to build a model that performs optical character recognition, you might not be terribly concerned about the interpretability of your model. However, if your model will be used to inform a decision maker, interpretability is much more important - especially if you are asked to explain the model to the decision maker. In fact, some machine learning practitioners argue against using black box models for all high stakes decision making. For example, read this paper by Cynthia Rudin, a computer scientist at Duke University. Recently, advancements have been made in improving the interpretability of some types of machine learning models (for example, download and read this paper from h2o.ai or this e-book by Christoph Molnar, a Ph.D. candidate at the University of Munich), and we will explore these techniques below.\nLinear models have coefficients (the \\(\\beta\\)s) that explain the nature of the relationship between predictors and the response. Classification and regression trees have an analogous concept of variable importance, which can be extended to random forest models. The documentation for importance() from the randomForest package provides the following definitions of two variable importance measures:\n\nThe first measure is computed from permuting OOB data: For each tree, the prediction error on the out-of-bag portion of the data is recorded (error rate for classification, MSE for regression). Then the same is done after permuting each predictor variable. The difference between the two are then averaged over all trees, and normalized by the standard deviation of the differences. If the standard deviation of the differences is equal to 0 for a variable, the division is not done (but the average is almost always equal to 0 in that case).\n\n\nThe second measure is the total decrease in node impurities from splitting on the variable, averaged over all trees. For classification, the node impurity is measured by the Gini index. For regression, it is measured by residual sum of squares.\n\nThese two measures can be accessed with:\n\nimportance(aq.rf)\n\n          %IncMSE IncNodePurity\nSolar.R 13.495267     15939.238\nWind    19.989633     39498.922\nTemp    37.489127     48112.583\nMonth    4.053344      4160.278\nDay      3.052987      9651.722\n\n\nAlternatively, we can plot variable importance with varImpPlot().\n\nvarImpPlot(aq.rf)\n\n\n\n\nVariable importance can be related to a linear model coefficient in that a large variable importance value is akin to a large coefficient value. However, it doesn’t indicate whether the coefficient is positive or negative. For example, from the above plot, we see that Temp is an important predictor of Ozone, but we don’t know if increasing temperatures result in increasing or decreasing ozone measurements (or if it’s a non-linear relationship). Partial dependence plots (PDP) were developed to solve this problem, and they can be interpreted in the same way as a loess or spline smoother.\nFor the airquality data, one would expect that increasing temperatures would increase ozone concentrations, and that increasing wind speed would decrease ozone concentrations. The partialPlot() function provided with the randomForest package produces PDPs, but they are basic and difficult to customize. Instead, we’ll use the pdp package, which works nicely with ggplot2 and includes a loess smoother (another option is the iml package - for interpretable machine learning - which we’ll also explore).\n\n#library(pdp)\n\np3 = aq.rf %>%\n  pdp::partial(pred.var = \"Temp\") %>%   # from the pdp package\n  autoplot(smooth = TRUE, ylab = expression(f(Temp))) +\n  theme_bw() +\n  ggtitle(\"Partial Dependence of Temp\")\n\np4 = aq.rf %>%\n  pdp::partial(pred.var = \"Wind\") %>%\n  autoplot(smooth = TRUE, ylab = expression(f(Temp))) +\n  theme_bw() +\n  ggtitle(\"Partial Dependence of Wind\")\n\ngridExtra::grid.arrange(p3, p4, ncol=2)\n\n\n\n\nEarlier, we produced a response surface plot based on a regression tree. Now we can produce a response surface based on the random forest model, which looks similar but more detailed. Specifying chull = TRUE (chull stands for convex hull) limits the plot to the range of values in the training data set, which prevents predictions being shown for regions in which there is no data. A 2D heat map and a 3D mesh are shown below.\n\n# Compute partial dependence data for Wind and Temp\npd = pdp::partial(aq.rf, pred.var = c(\"Wind\", \"Temp\"), chull = TRUE)\n\n# Default PDP\npdp1 = pdp::plotPartial(pd)\n\n# 3-D surface\npdp2 = pdp::plotPartial(pd, levelplot = FALSE, zlab = \"Ozone\",\n                    screen = list(z = -20, x = -60))\n\ngridExtra::grid.arrange(pdp1, pdp2, ncol=2)\n\n\n\n\nThe iml package was developed by Christoph Molnar, the Ph.D. candidate referred to earlier, and contains a number of useful functions to aid in model interpretation. In machine learning vernacular, predictors are commonly called features, so instead of variable importance, we’ll get feature importance. With this package, we can calculate feature importance and produce PDPs as well, and a grid of partial dependence plots are shown below. Note the addition of a rug plot at the bottom of each subplot, which helps identify regions where observations are sparse and where the model might not perform as well.\n\n# library(iml) # for interpretable machine learning\n# library(patchwork) # for arranging plots - similar to gridExtra\n\n# iml doesn't like NAs, so we'll drop them from the data and re-fit the model\naq = airquality %>% drop_na()\naq.rf2 = randomForest(Ozone~., importance=TRUE, na.action=na.omit, mtry=2, data=aq)\n\n# provide the random forest model, the features, and the response\npredictor = iml::Predictor$new(aq.rf2, data = aq[, 2:6], y = aq$Ozone)\n\nPDP = iml::FeatureEffects$new(predictor, method='pdp')\nPDP$plot() & theme_bw()\n\n\n\n\nPDPs show the average feature effect, but if we’re interested in the effect for one or more individual observations, then an Individual Conditional Expectation (ICE) plot is useful. In the following plot, each black line represents one of the 111 observations in the data set, and the global partial dependence is shown in yellow. Since the individual lines are generally parallel, we can see that each individual observation follows the same general trend: increasing temperatures have little effect on ozone until around 76 degrees, at which point all observations increase. In the mid 80s, there are a few observations that have a decreasing trend while the majority continue to increase, which indicates temperature may be interacting with one or more other features. Generally speaking, however, since the individual lines are largely parallel, we can conclude that the partial dependence measure is a good representation of the whole data set.\n\nice = iml::FeatureEffect$new(predictor, feature = \"Temp\", method='pdp+ice') \n\nice$plot() + theme_bw()\n\n\n\n\nOne of the nice attributes of tree-based models is their ability to capture interactions. The interaction effects can be explicitly measured and plotted as shown below. The x-axis scale is the percent of variance explained by interaction for each feature, so Wind, Temp, and Solar.R all have more than 10% of their variance explained by an interaction.\n\ninteract = iml::Interaction$new(predictor)\nplot(interact) + theme_bw()\n\n\n\n\nTo identify what the feature is interacting with, just specify the feature name. For example, Temp interactions are shown below.\n\ninteract = iml::Interaction$new(predictor, feature='Temp')\nplot(interact) + theme_bw()\n\n\n\n\n\n\nPredictions\nPredictions for new data are made the usual way with predict(), which is demonstrated below using the first two rows of the airquality data set.\n\npredict(aq.rf, airquality[1:2, c(2:6)])\n\n       1        2 \n38.50243 31.39827 \n\n\n\n\n\nRandom Forest Classification\nFor a classification example, we’ll skip over simple classification trees and jump straight to random forests. There is very little difference in syntax with the randomForest() function when performing classification instead of regression. For this demonstration, we’ll use the iris data set so we can compare results with the SVC results. We’ll use the same training and test sets as earlier.\n\nset.seed(0)\n\ntrain = caTools::sample.split(iris, SplitRatio = 0.8)\niris_train = subset(iris, train == TRUE)\niris_test = subset(iris, train == FALSE)\n\niris.rf <- randomForest(Species ~ ., data=iris_train, importance=TRUE)\nprint(iris.rf)\n\n\nCall:\n randomForest(formula = Species ~ ., data = iris_train, importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 5%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa         40          0         0       0.000\nversicolor      0         37         3       0.075\nvirginica       0          3        37       0.075\n\n\nThe model seems to have a little trouble distinguishing virginica from versicolor. The linear SVC misclassified two observations in the test set, and the radial SVC misclassified one. Before we see how the random forest does, let’s make sure we grew enough trees. We can make a visual check by plotting the random forest object.\n\nplot(iris.rf)\n\n\n\n\nNo issue there! Looks like 500 trees was plenty. Taking a look at variable importance shows that petal width and length are far more important than sepal width and length.\n\nvarImpPlot(iris.rf)\n\n\n\n\nSince the response variable is categorical with three levels, a little work is required to get partial dependence plots for each predictor-response combination. Below are the partial dependence plots for Petal.Width for each species. The relationship between petal width and species varies significantly based on the species, which is what makes petal width have a high variable importance.\n\nas_tibble(iris.rf %>%\n  # which.class refers to the factor level\n  pdp::partial(pred.var = \"Petal.Width\", which.class=1) %>% \n  mutate(Species = levels(iris$Species)[1])) %>%\n  bind_rows(as_tibble(iris.rf %>%\n  pdp::partial(pred.var = \"Petal.Width\", which.class=2) %>%\n  mutate(Species = levels(iris$Species)[2]))) %>%\n  bind_rows(as_tibble(iris.rf %>%\n  pdp::partial(pred.var = \"Petal.Width\", which.class=3) %>%\n  mutate(Species = levels(iris$Species)[3]))) %>%\n  ggplot() +\n  geom_line(aes(x=Petal.Width, y=yhat, col=Species), size=1.5) +\n  ggtitle(\"Partial Dependence of Petal.Width\") +\n  theme_bw()\n\n\n\n\nEnough visualizing. Time to get the confusion matrix for the random forest model using the test set.\n\n# get the confusion matrix\nrf_conf_mat = cvms::confusion_matrix(\n  targets = iris_test[, 5],\n  predictions = predict(iris.rf, newdata = iris_test[-5]))\n\n# plot the confusion matrix\ncvms::plot_confusion_matrix(rf_conf_mat$`Confusion Matrix`[[1]]) + \n  ggtitle(\"Random Forest\")\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\nIt’s always nice to see that I didn’t mess up the manual calculations.↩︎"
  },
  {
    "objectID": "tutorial/slr.html",
    "href": "tutorial/slr.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "The purpose of regression is to describe a relationship that explains one variable (the response, or the “y” variable) based on one or more other variables (the predictors, or the “x” variables). The simplest deterministic mathematical relationship between two variables \\(x\\) and \\(y\\) is a linear relationship, which we define as:\n\\[y = \\beta_{0} + \\beta_{1}x + \\varepsilon\\]\nwhere,\n\n\\(\\beta_{0}\\) is the y-intercept\n\\(\\beta_{1}\\) is the slope\n\\(\\varepsilon\\) is the error in \\(y\\) not explained by \\(\\beta_{0}\\) and \\(\\beta_{1}\\).\n\nIf there is no error in the model and a linear relationship exists, we could predict the true value of y given any value of x. With error, however, we can only estimate y, which we annotate by \\(\\hat{y}\\). The regression line itself is determined using the least squares method, which involves drawing a line through the centroid of the data, and adjusting the slope until the squared distance between the straight line and each observed value (the residual) is minimized. For example, assume we have the following observations of height in inches (predictor) and weight in pounds (response).\n\nlibrary(tidyverse)\nlibrary(GGally)\n\ndf = tibble(\n  height = c(66, 54, 50, 74, 59, 53),\n  weight = c(141, 128, 123, 160, 136, 139)\n)\n\n\nLeast Squares Method Manually\nThe centroid coordinates \\((\\bar{x},\\bar{y})\\) are calculated simply by \\(\\bar{x}\\) = mean(df$height) = 59.33, and \\(\\bar{y}\\) = mean(df$weight) = 137.83. Plotting the data with the centroid, we get:\n\n\n\n\n\nTo find the slope, \\(\\beta_{1}\\), we calculate how much each height and weight observation deviate from the centroid, multiply those paired deviations, sum them, and divide that by the sums of the squared height deviations. With the height and weight data, we find:\n\ndf = df %>%\n  mutate(\n    h_dev = height - mean(height),    # height deviation from centroid\n    w_dev = weight - mean(weight),    # weight deviation from centroid\n    dev_prod = h_dev * w_dev,         # the product of the deviations\n    h_dev_squared = h_dev^2           # the squared products\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nheight\nweight\nheight deviance\nweight deviance\ndeviation products\nheight deviance squared\n\n\n\n\nx\ny\n\\(x_{i}-\\bar{x}\\)\n\\(y_{i}-\\bar{y}\\)\n\\((x_{i}-\\bar{x})(y_{i}-\\bar{y})\\)\n\\((x_{i}-\\bar{x})^{2}\\)\n\n\n66\n141\n6.67\n3.17\n21.14\n44.49\n\n\n54\n128\n-5.33\n-9.83\n52.39\n28.41\n\n\n50\n123\n-9.33\n-14.83\n138.36\n87.05\n\n\n74\n160\n14.67\n22.17\n325.23\n215.21\n\n\n59\n136\n-0.33\n-1.83\n0.60\n0.11\n\n\n53\n139\n-6.33\n1.17\n-7.41\n40.07\n\n\n\\(\\bar{x} = 59.33\\)\n\\(\\bar{y} = 137.83\\)\n-\n-\n\\(\\Sigma = 530.33\\)\n\\(\\Sigma = 415.33\\)\n\n\n\nThe slope is found by \\(\\beta_{1} = \\frac{\\Sigma (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{\\Sigma (x_{i}-\\bar{x})^{2}} = \\frac{530.33}{415.33} = 1.28\\). For our dataset in R, that translates to:\n\nbeta1 = sum(df$dev_prod) / sum(df$h_dev_squared)\nbeta1\n\n[1] 1.276886\n\n\nWe have now have what we need to calculate the y-intercept, \\(\\beta_{0} =\\bar{y}-\\beta{_1}\\bar{x}\\). Equivalently in R:\n\nbeta0 = mean(df$weight) - beta1 * mean(df$height)\nbeta0\n\n[1] 62.07143\n\n\nWhen we plot the line defined by our beta values, we find that it does, in fact, pass through the centroid and visually appears to fit the data.\n\nggplot(data=df, aes(x=height, y=weight)) +\n  geom_point() +\n  geom_abline(intercept=beta0, slope=beta1, color='blue') + # note the explicit use of our betas\n  #geom_smooth(formula=y~x, method=\"lm\", se=FALSE) + # how it's normally done in practice\n  geom_point(data = centroid, aes(x=height, y=weight), color='red', size=5) +\n  annotate(\"text\", x=61, y=138, label=\"centroid\", color='red') +\n  annotate(\"text\", x=67.5, y=143.5, label=\"residual\", color='blue') +\n  annotate(\"segment\", x=50, xend=50, y=123, yend=125.5, color='blue', linetype=2, size=0.5) +\n  annotate(\"segment\", x=53, xend=53, y=139, yend=129.5, color='blue', linetype=2, size=0.5) +\n  annotate(\"segment\", x=54, xend=54, y=128, yend=131.5, color='blue', linetype=2, size=0.5) +\n  annotate(\"segment\", x=59, xend=59, y=136, yend=137.5, color='blue', linetype=2, size=0.5) +\n  annotate(\"segment\", x=66, xend=66, y=141, yend=146.5, color='blue', linetype=2, size=0.5) +\n  annotate(\"segment\", x=74, xend=74, y=160, yend=156.5, color='blue', linetype=2, size=0.5) +\n  theme_bw()\n\n\n\n\nThis method minimizes the residual sum of squares (RSS), which is represented mathematically by:\n\\[RSS = \\sum\\limits_{i=1}^{n}{(y_{i} - \\hat{y}_{i})^2} = \\sum\\limits_{i=1}^{n}{\\hat\\varepsilon_{i}^{2}}\\]\nand is calculated as follows.\n\ndf = df %>% mutate(\n  y_hat = beta0 + beta1 * height,\n  error = weight - y_hat,\n  error_squared = error^2\n)\nprint(paste(\"RSS =\", round(sum(df$error_squared), 2)))\n\n[1] \"RSS = 145.66\"\n\n\n\n\n\n\n\n\n\n\n\n\nheight\nweight\npredicted weight\nerror\nsquared error\n\n\n\n\nx\ny\n\\(\\hat{y}_{i}\\)\n\\(y-\\hat{y}_{i}\\)\n\\((y-\\hat{y}_{i})^2\\)\n\n\n66\n141\n146.35\n-5.35\n28.58\n\n\n54\n128\n131.02\n-3.02\n9.14\n\n\n50\n123\n125.92\n-2.92\n8.50\n\n\n74\n160\n156.56\n3.44\n11.83\n\n\n59\n136\n137.41\n-1.41\n1.98\n\n\n53\n139\n129.75\n9.25\n85.63\n\n\n\\(\\bar{x} = 59.33\\)\n\\(\\bar{y} = 137.83\\)\n-\n-\n\\(RSS: \\Sigma = 145.66\\)\n\n\n\n\n\nGoodness of Fit\nWhile RSS gives us an idea of how well the regression prediction (\\(\\hat{y}\\)) can approximate the response (\\(y\\)), it does not tell us how well the model fits the data because it has the same units as \\(y\\). To obtain a unitless measure of fit, \\(R^2\\) (also called the coefficient of determination), RSS is divided by the total sum of squares (TSS), and that ratio is subtracted from 1.\n\\[R^2 = 1- \\frac{RSS}{TSS} = 1 - \\frac{\\Sigma(y_{i} - \\hat{y}_{i})^2}{\\Sigma(y_{i} - \\bar{y}_{i})^2}\\]\nWe calculate \\(R^2\\) for the height/weight data as follows:\n\n1 - sum(df$error_squared) / sum(df$w_dev^2)\n\n[1] 0.8229798\n\n\nWe interpret \\(R^2\\) as the proportion of weight variation explained by the linear model. As a proportion, \\(R^2\\) varies from 0 to 1, and ideally we seek models with a high \\(R^2\\). A graphical depiction of RSS and TSS for one of the residuals illustrates their relationship.\n\n\n\n\n\n\n\nLeast Squares Method In R\nThat was a fair amount of work, and of course R simplifies the process. Fortunately, the syntax for creating a linear model is very similar to ANOVA.\n\ndf.lm = lm(weight ~ height, data=df)\nsummary(df.lm)\n\n\nCall:\nlm(formula = weight ~ height, data = df)\n\nResiduals:\n     1      2      3      4      5      6 \n-5.346 -3.023 -2.916  3.439 -1.408  9.254 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  62.0714    17.7405   3.499   0.0249 *\nheight        1.2769     0.2961   4.312   0.0125 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.034 on 4 degrees of freedom\nMultiple R-squared:  0.823, Adjusted R-squared:  0.7787 \nF-statistic:  18.6 on 1 and 4 DF,  p-value: 0.01252\n\n\nGoing through each section of the output from top to bottom:\nCall: This is simply the formula we gave the lm() function.\nResiduals: The residuals in order of observation.\nCoefficients:\n\nEstimate These are the \\(\\beta\\)s, and we see that they match our values calculated above.\nStd. Error is the standard deviation of each Estimate (or \\(\\beta\\)) and can be used to determine the 95% confidence interval (CI). For example, the 95% CI for height is \\(1.28 \\pm 1.96(0.296)\\).\nt value is Estimate / Std. Error.\nPr(>|t|) is the probability of observing a value at least as extreme as \\(\\beta\\) using a t-distribution and \\((n-p-1)\\) degrees of freedom. The null hypothesis is \\(H_{o}: \\beta_{i}=0\\). Notice that this is a test of each individual predictor.\n\n\nResidual standard error: The square root of RSS divided by the difference of number of observations and the number of predictors. Or, \\(RSE = \\sqrt{\\frac{RSS}{n-p}}\\). Degrees of freedom is \\((n-p-1)\\).\nMultiple R-squared: The \\(R^2\\) we calculated above.\nAdjusted R-squared:Normalizes \\(R^2\\) by accounting for the number of observations and predictors in the model. When conducting multiple linear regression, this is the appropriate method of measuring goodness of fit. Adjusted r-squared is calculated by: \\(\\bar{R}^{2} = 1 - (1 - R^{2}) \\frac{n-1}{n-p-1}\\).\nF-statistic: The global test of significance where we wish to determine if at least one predictor is sginificant. The null hypothesis is \\(H_{o}: \\beta_{1}=...=\\beta_{p-1}=0\\) under the F-distribution with \\(p-1\\) and \\(n-p-1\\) degrees of freedom.\nWe interpret the linear model in the following manner: for every inch increase in height, we predict a person’s weight increases by 1.28 pounds.\nR allows us to do several things with a model. We can use R to give us a confidence interval on our coefficients using confint:\n\nconfint(df.lm)\n\n                 2.5 %     97.5 %\n(Intercept) 12.8158877 111.326969\nheight       0.4547796   2.098992\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is more precise than the Z estimation shown above as it accounts for our sample size and uses a t test.\n\n\nWe can also predict results using predict Predict can either give you a point estimate, or an interval based on either the mean predictions (using 'confidence') or a single point (using 'prediction'). You can read more about these options [here](http://www.sthda.com/english/articles/40-regression-analysis/166-predict-in-r-model-predictions-and-confidence-intervals/).\n\npredict(df.lm, list(height = 66))\n\n       1 \n146.3459 \n\npredict(df.lm, list(height = 66), interval = 'confidence')\n\n       fit      lwr      upr\n1 146.3459 137.5811 155.1108\n\npredict(df.lm, list(height = 66), interval = 'prediction')\n\n       fit      lwr      upr\n1 146.3459 127.4375 165.2544\n\n\nNote, R will not prevent one from extrapolating beyond the data. Predicting a result on values outside the observed data is bad practice and should generally be avoided.\nFinally, one can plot the results and a regression quite simply with ggplot() using stat_smooth():\n\nggplot(data = df, aes(x = height, y = weight)) + # Provide your data and aesthetics as usual\n  geom_point() +  # plot the observations\n  # geom_smooth creates a linear regression based on your given x and y values (i.e. lm(y~x))\n  # you can also plot the standard error\n  geom_smooth(method = 'lm', se = T, formula = \"y ~ x\")"
  },
  {
    "objectID": "tutorial/svm.html",
    "href": "tutorial/svm.html",
    "title": "Support Vector Machines",
    "section": "",
    "text": "SVR attempts to include as many data points as possible in the area between two lines. The following figure demonstrates this using dummy data with a linear relationship. The two parallel lines are the margin, and it’s width is a hyperparameter \\(\\varepsilon\\) that we can tune. If you draw a line through one of the points that fall outside the margin so that it is perpendicular to the margin, you have a support vector. A cost is applied to each point that falls outside the margin, and minimizing the cost determines the slope of the margin. Cost is another tunable hyperparameter, which is sometimes represented as \\(1/\\lambda\\). Notice that unlike linear regression, if we were to add more points inside the margin, it would have no impact on the slope. SVR is also much less influence by outliers than linear regression. For the mathematical details behind SVR, refer to Section 12.3.6 in Trevor Hastie and Friedman (2008).\n\n\n\n\n\nChoosing values for the hyperparameters \\(\\varepsilon\\) and \\(\\lambda\\) is once again done through cross validation. To do this in R, we’ll use some functions from the e1071 package (another option is the LiblineaR package). Before we get to cross validation, let’s just look at how to build an SVR model. The syntax is the same as for linear models, we just replace lm() with svm(). Note that the function is not svr() because the function can do both regression and classification. To make this more interesting, we’ll switch back to the airquality data. From the model summary below, SVM-type:  eps-regression tells us that the function is performing regression and not classification, then we see the hyperparameter values and the number of support vectors used to fit the model.\nFor the kernel, we have four choices: linear, polynomial, radial basis, and sigmoid. Selecting a linear kernel will force a straight line fit, and the other three kernels are different methods for adding curvature to the regression line1. The theory behind SVR kernels is beyond the scope of this tutorial, but if you want to dig deeper:\n\nHere are some slides titled SVM dual, kernels and regression from The University of Oxford.\nHere’s An Idiot’s Guide to Support Vector Machines, a catchy title from MIT.\nHere’s post titled Support Vector Machine: Kernel Trick; Mercer’s Theorem at towardsdatascience.com.\n\nFor our purposes, we just need to know that the three non-linear kernels have gamma as a hyperparameter that controls curvature.\nTo force a straight regression line, specify kernel='linear'. Also, the svm() by default scales all variables in the data set to have a mean of zero and equal variance. Scaling the variables will improve the model’s performance, but we’ll turn that off in this example so we can directly compare the coefficients to those produced by lm().\n\nlibrary(e1071)\n\naq = airquality %>% drop_na()\n\naq.svm = svm(Ozone ~ Solar.R, data=aq, kernel='linear', scale=FALSE)\nsummary(aq.svm)\n\n\nCall:\nsvm(formula = Ozone ~ Solar.R, data = aq, kernel = \"linear\", scale = FALSE)\n\n\nParameters:\n   SVM-Type:  eps-regression \n SVM-Kernel:  linear \n       cost:  1 \n      gamma:  1 \n    epsilon:  0.1 \n\n\nNumber of Support Vectors:  110\n\n\nWe can then extract the coefficients with coef().\n\n(coeffs = coef(aq.svm))\n\n(Intercept)     Solar.R \n12.52321429  0.09107143 \n\n\nUsing lm(), we get the following coefficients.\n\naq.lm = lm(Ozone ~ Solar.R, data=aq)\nsummary(aq.lm)\n\n\nCall:\nlm(formula = Ozone ~ Solar.R, data = aq)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.292 -21.361  -8.864  16.373 119.136 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 18.59873    6.74790   2.756 0.006856 ** \nSolar.R      0.12717    0.03278   3.880 0.000179 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 31.33 on 109 degrees of freedom\nMultiple R-squared:  0.1213,    Adjusted R-squared:  0.1133 \nF-statistic: 15.05 on 1 and 109 DF,  p-value: 0.0001793\n\n\nThe coefficients produced by the two models might seem fairly different. The following plot shows the data with the two regression lines for comparison. Notice how the linear model is more influenced by the extreme high ozone values (possible outliers).\n\nggplot() +\n  geom_point(data = aq, aes(x=Solar.R, y=Ozone)) +\n  geom_abline(slope=coeffs[2], intercept=coeffs[1], color='red') + \n  annotate(\"text\", x=315, y=50, label=\"svm()\", color='red') +\n  geom_abline(slope=aq.lm$coefficients[2], \n              intercept=aq.lm$coefficients[1], \n              color='blue') +\n  annotate(\"text\", x=315, y=70, label=\"lm()\", color='blue') +\n  theme_bw()\n\n\n\n\nNow we’ll re-fit the model with a non-linear regression line and invoking scaling. To extract the predicted response, we use the predict() function just like with linear models. Plotting the predicted response gives is the following.\n\naq.svm2 = svm(Ozone ~ Solar.R, data=aq)\n\naq = aq %>% mutate(svrY = predict(aq.svm2, data=aq))\n\nggplot(aq) +\n  geom_point(aes(Solar.R, Ozone), color='black') +\n  geom_line(aes(Solar.R, svrY), color='red') +\n  ggtitle(\"SVR With Default Hyperparameters\") +\n  coord_fixed() +\n  theme_bw()\n\n\n\n\nTo tune the hyperparameters with cross validation, we can use the tune function from the e1017 package. If we give the tune function a range of values for the hyperparameters, it will perform a grid search of those values. In the following example, we’re therefore fitting 100 different models. If we print the object returned from tune, we see that it performed 10-fold cross validation, the best hyperparameter values, and the mean squared error of the best performing model.\n\nset.seed(42)\naq.tune = tune.svm(Ozone ~ Solar.R, \n                   data = aq, gamma=seq(0.1, 1, 0.1), \n                   cost = seq(1, 100, 10))\nprint(aq.tune)\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n gamma cost\n   0.1   91\n\n- best performance: 909.1502 \n\n\nWe can visualize the tune results as well by printing the aq.tune object. Here we see the range of cost and epsilon values with their associated mean squared error. The lower the error, the better, and those are indicated by the darkest blue regions.\n\nplot(aq.tune)\n\n\n\n\nI prefer to choose a wide range of tuning parameter values initially, and then do a finer search in the area with the lowest error. It looks like we need a low gamma and a high cost.\n\nset.seed(42)\naq.tune = tune.svm(Ozone ~ Solar.R, \n                   data = aq, \n                   gamma=seq(0.02, 0.22, 0.05), \n                   cost = seq(80, 100, 2))\nprint(aq.tune)\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n gamma cost\n  0.22   96\n\n- best performance: 907.4115 \n\n\nThe best model from the tuning call can be obtained with aq.tune$best.model, and we can then apply the predict function to get the best fit regression.\n\naq$svrY = predict(aq.tune$best.model, data=aq)\n\nggplot(aq) +\n  geom_point(aes(Solar.R, Ozone), color='black') +\n  geom_line(aes(Solar.R, svrY), color='red') +\n  ggtitle(\"SVR With Tuned Hyperparameters\") +\n  coord_fixed() +\n  theme_bw()\n\n\n\n\n\nSupport Vector Classification\nClassification problems have either a binary or categorical response variable. To demonstrate how SVC works, we’ll start with the iris data set, which contains four predictors and one categorical response variable. Plotting petal length versus petal width for the setosa and versicolor species shows that the two species are linearly separable, meaning we can draw a straight line on the plot that completely separates the two species. If we want to train an SVC to make predictions on new data, the question becomes: how do we draw the line that separates the data? There are infinitely many options, three of which are shown on the plot.\n\n\n\n\n\nSupport vector classification uses margins, but in a different way than SVR, to find a line that separates the data. If you think of the two parallel margin lines as a street, the idea is that we want to fit the widest possible street between the species because doing so results in the rest of the data points being as far off the street as possible. The two points below that fall on the margin determine the location of the support vectors.\n\n\n\n\n\nWhat happens when two categories aren’t linearly separable, as is the case when we look at versicolor and virginica below?\n\n\n\n\n\nWe still want to draw two parallel lines through the data sets, but the only way to do it is to have some observations in the middle of the street, or even on the wrong side of the line (called margin violations). We still want to fit as wide of a street as possible through the data points, but now we must also limit the number of margin violations. As with SVR, we can assign a cost for each margin violation. Since margin violations are generally bad, we might be tempted to apply a large cost; however, we must also consider how well the model will generalize. Below are the linear boundaries for two choices of cost. Support vectors are based on the points surrounded by black.\n\n\n\n\n\nInterestingly, the margins (and therefore the decision boundary) don’t have to be straight lines. SVC also accommodates a curved boundary as in the example below. With a polynomial kernel, the curvature is controlled by the degree of the polynomial. In the plot, note that the support vectors are the X points.\n\n\n\n\n\n\nExample In R\nIn this section, we’ll walk through an example using the full iris data set. First, we’ll split the data set into a training set that includes 80% of the data, and a test set with the remaining 20% using the caTools package.\n\nset.seed(0)\ntrain = caTools::sample.split(iris, SplitRatio = 0.8)\niris_train = subset(iris, train == TRUE)\niris_test = subset(iris, train == FALSE)\n\nNext, we’ll tune two models using a linear kernel and a radial basis function (which allows for curvature). We’ll tune both models over a range of gamma and cost values.\n\niris.lin = tune.svm(Species~., data=iris_train, \n                    kernel=\"linear\", \n                    gamma = seq(0.1, 1, 0.1), \n                    cost = seq(1, 100, 10))\n\niris.rbf = tune.svm(Species~., data=iris_train, \n                    kernel=\"radial\", \n                    gamma = seq(0.1, 1, 0.1), \n                    cost = seq(1, 100, 10))\n\niris.lin$best.model\n\n\nCall:\nbest.svm(x = Species ~ ., data = iris_train, gamma = seq(0.1, 1, \n    0.1), cost = seq(1, 100, 10), kernel = \"linear\")\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  1 \n\nNumber of Support Vectors:  25\n\niris.rbf$best.model\n\n\nCall:\nbest.svm(x = Species ~ ., data = iris_train, gamma = seq(0.1, 1, \n    0.1), cost = seq(1, 100, 10), kernel = \"radial\")\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  48\n\n\nBoth models are using a low cost, but the radial basis function model has twice as many support vectors. To compare model performance, we’ll make predictions using the test set and display each model’s confusion matrix using the cvms package (note: we could also create a simple confusion matrix with table(iris_test[, 5], predictions)).\n\n# get the confusion matrix for the linear kernel\nlin_conf_mat = cvms::confusion_matrix(\n  targets = iris_test[, 5], \n  predictions = predict(iris.lin$best.model, type = 'response', newdata = iris_test[-5]))\n\n# get the confusion matrix for the radial kernel\nrbf_conf_mat = cvms::confusion_matrix(\n  targets = iris_test[, 5],\n  predictions = predict(iris.rbf$best.model, type = 'response', newdata = iris_test[-5]))\n\n# plot the confusion matrix for the linear kernel (it's a ggplot2 object!)\ncvms::plot_confusion_matrix(lin_conf_mat$`Confusion Matrix`[[1]]) + \n  ggtitle(\"Linear Kernel\")\n\n\n\n\nThe SVC model with the linear kernel did a great job! Of the 30 observations in the test set, only two were incorrectly classified. If this is the first time you’ve seen a confusion matrix, then what you see are the target (or actual) species by column and the species predictions from the SVC by row. In each cell, we see the percent and count of the total observations that fell into that cell. From this plot, we can identify true positives, false positives, etc. using the following guide.\n\n\n\nConfusion Matrix\n\nTarget\n\n\n\n\n\nYes\nNo\n\n\nPrediction\nYes\nTrue Positive\nFalse Positive\n\n\n\nNo\nFalse Negative\nTrue Positive\n\n\n\nA perfect classifier will have zeros everywhere in the table except the diagonal. In our case, it’s close to perfect. We just have two false negatives because two flowers that were actually virginica, were predicted to be versicolor. Now let’s look at the radial kernel results.\n\ncvms::plot_confusion_matrix(rbf_conf_mat$`Confusion Matrix`[[1]]) + \n  ggtitle(\"Radial Kernel\")\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman. 2008. The Elements of Statistical Learning. Springer.\n\nFootnotes\n\n\nChanging the kernel to specify the type of fit is known as the kernel trick.↩︎"
  },
  {
    "objectID": "tutorial/transform.html",
    "href": "tutorial/transform.html",
    "title": "Variable Transformation",
    "section": "",
    "text": "If we have data that includes factors with more than two levels, we have the ability to evaluate non-linear relationships between predictor and response variables. Incorporating non-linear terms in a linear model is accomplished by transforming either the response or predictors. For an overview of transformation, read Transformations: an introduction by Nicholas J. Cox at Durham University. Skip the section titled “How to do transformations in Stata”; we’ll replace that with “How to do transformations in R” below.\n\nIdentifying Non-Linear Relationships\nI think the simplest way to screen your data for potential non-linear relationships is with a pairs plot that includes a smoother. To demonstrate, I’ll create data using a central composite design and add a response, y, that has a non-linear relationship with the speed and stealth factors. I also subtracted 2 from the stealth factor to center it at 0.\n\nlibrary(tidyverse)\nlibrary(GGally)\n\nset.seed(42)\n\nccdGrid = tibble(\n  x1 = c(c(1,3,2,2,2,2,2), rep(c(1,3), 4)),\n  x2 = c(c(2,2,1,3,2,2,2), rep(c(1,3,1,3), each = 2)),\n  x3 = c(c(2,2,2,2,1,3,2), rep(1,4), rep(3,4)),\n  star = c(rep('y', 7), rep('n',8)),\n  line = c(rep('line1',2), rep('line2',2), rep('line3',2), rep('none',9))\n)\n\n\nccd = tibble(speed = ccdGrid$x1, \n             stealth = ccdGrid$x2 - 2, \n             surv = ccdGrid$x3, \n             y = log(speed) - stealth^2 + surv)\n\nsmooth_fn <- function(data, mapping, ...){\n  ggplot(data = data, mapping = mapping) + \n    geom_point() + \n    geom_smooth(formula = y~x, method=loess, fill=\"red\", color=\"red\", se=FALSE, ...)\n}\n\nggpairs(ccd, lower=list(continuous=smooth_fn), progress=FALSE) + theme_bw()\n\n\n\n\nA visual inspection of the last row of plots is enough to identify the non-linear relationships that speed and stealth have with the response. We can also look at the density plot for the response (the lower right curve) and see some skewness away from a normal distribution.\n\n\nChecking Model Structure\nGenerating a pairs plot is a screening process only. For a more complete analysis, we need to check the model structure. Recall that one of the key assumptions of the linear regression model is that the regression errors are independent and identically distributed. If that assumption is not true, then the non-linear portion of the relationship between predictor and response will be contained in the (estimated) residuals, \\(\\hat{\\varepsilon}\\). Plotting the residuals versus the individual predictors is one method of checking model structure. In R, we can do this with termplot.\n\n# first, we need a linear model\nccd.lm = lm(y ~ ., data=ccd)\npar(mfrow=c(1,3))\ntermplot(ccd.lm, partial.resid = TRUE, col.res='blue', main=\"Residuals vs. Predictor\")\n\n\n\n\nIn these plots, we’re checking for whether there is a non-linear shape to the data by looking for trends in the blue circles. The red lines are the coefficients from the linear model for reference. The non-linear shape to stealth is clearly visible, but we’re missing the non-linearity in speed. Unfortunately, partial residual plots only suggest transformations for the predictors because they are influenced by other predictor variables, and (if present) influential observations and multicollinearity. The process is done manually in R. First, since stealth appears to be an inverse square, we’ll transform that variable, then re-fit the model, and check the partial residuals again. Before we do that, we need to know how to transform variables.\n\n\nHow To Transform Variables In R\nTo account for non-linear relationships in a linear model, we need to transform the variables in the lm function. From the partial residuals plot, we know we should try a quadratic term for stealth. Summaries of linear models without and then with the transformation are shown below for the ccd data.\nWithout transformation:\n\nccd.lm = lm(y ~ speed + stealth + surv, data = ccd)\nsummary(ccd.lm)\n\n\nCall:\nlm(formula = y ~ speed + stealth + surv, data = ccd)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.3813 -0.3813 -0.3813  0.6187  0.7626 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.168e+00  5.462e-01  -2.139  0.05574 .  \nspeed        5.493e-01  1.855e-01   2.961  0.01295 *  \nstealth     -2.708e-18  1.855e-01   0.000  1.00000    \nsurv         1.000e+00  1.855e-01   5.390  0.00022 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5867 on 11 degrees of freedom\nMultiple R-squared:  0.7747,    Adjusted R-squared:  0.7132 \nF-statistic: 12.61 on 3 and 11 DF,  p-value: 0.0006996\n\n\nWith transformation:\n\nccd_t.lm = lm(y ~ speed + I(stealth^2) + surv, data = ccd)\nsummary(ccd_t.lm)\n\n\nCall:\nlm(formula = y ~ speed + I(stealth^2) + surv, data = ccd)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.08631 -0.02877 -0.02877  0.05754  0.11507 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.46300    0.07257   -6.38 5.22e-05 ***\nspeed         0.54931    0.02295   23.94 7.72e-11 ***\nI(stealth^2) -1.05754    0.03975  -26.61 2.46e-11 ***\nsurv          1.00000    0.02295   43.58 1.13e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07257 on 11 degrees of freedom\nMultiple R-squared:  0.9966,    Adjusted R-squared:  0.9956 \nF-statistic:  1060 on 3 and 11 DF,  p-value: 8.06e-14\n\n\nNotice in the call to lm with the transformed variables, the polynomial term is surrounded by I(). This is to avoid confusion between arithmetic and symbolic uses of + in the formula function (see ?formula for more details). Let’s take another look at the partial residual plots with stealth transformed.\n\npar(mfrow=c(1,3))\n\ntermplot(ccd_t.lm, \n         partial.resid = TRUE, \n         col.res='blue', \n         main=\"Residuals vs. Predictor\")\n\n\n\n\nStealth looks much better, and now we’re able to see the non-linear relationship with speed. Re-fit and plot again with a log transformation on speed.\n\nccd_t2.lm = lm(y ~ log(speed) + I(stealth^2) + surv, data = ccd)\n\npar(mfrow=c(1,3))\n\ntermplot(ccd_t2.lm, \n         partial.resid = TRUE, \n         col.res='blue', \n         main=\"Residuals vs. Predictor\")\n\n\n\n\nI didn’t add any error to the data, so we now have a perfect fit. With real-world data, there will be noise, and the best transformation isn’t known in advance. The following chart from Stats With Cats is a useful guide when determining what transformations to try.\n\nNow consider results from the 17-point NOLH we created earlier. I added a response, y, with a non-linear relationship to one of the factors, and I added some noise to make this example more realistic. Plotting just these two variables with a linear regression line reveals a little curvature to the trend, but it’s not extreme.\n\nnolh.lm1 = lm(y~x, data=nolh)\ntermplot(nolh.lm1, partial.resid = TRUE, col.res='blue')\n\n\n\n\nWe’ve picked up the non-linear shape, and it looks like we need some degree of polynomial as a transformation. For reference, let’s look at the linear model summary.\n\nsummary(nolh.lm1)\n\n\nCall:\nlm(formula = y ~ x, data = nolh)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-665.4 -497.3 -108.8  364.6 1245.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -1193.3      314.6  -3.793  0.00177 ** \nx              286.8       30.7   9.343 1.21e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 620.1 on 15 degrees of freedom\nMultiple R-squared:  0.8534,    Adjusted R-squared:  0.8436 \nF-statistic: 87.29 on 1 and 15 DF,  p-value: 1.212e-07\n\n\nThat’s not a bad fit, but we can probably improve it by trying a transformation. The curvature suggests a polynomial might be better, so let’s try a second degree polynomial fit. First the plot, then the linear model summary.\n\n\n\n\n\n\nCall:\nlm(formula = y ~ I(x^2), data = nolh)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-304.4 -243.1  -25.1  228.5  495.9 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -349.0775    97.0449  -3.597  0.00264 ** \nI(x^2)        16.5459     0.6993  23.660 2.73e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 261.6 on 15 degrees of freedom\nMultiple R-squared:  0.9739,    Adjusted R-squared:  0.9722 \nF-statistic: 559.8 on 1 and 15 DF,  p-value: 2.73e-13\n\n\nThe plot looks better than the first one. This model also has a higher \\(R^{2}\\) than the first one, so perhaps it is a better fit. What happens if we continue to add higher order polynomials? The poly() function is useful for this purpose.\n\nnolh.poly = lm(y~poly(x, 15), data = nolh)\nsummary(nolh.poly)\n\n\nCall:\nlm(formula = y ~ poly(x, 15), data = nolh)\n\nResiduals:\n         1          2          3          4          5          6          7 \n 0.0003231 -0.0051699  0.0387741 -0.1809457  0.5880735 -1.4113764  2.5875234 \n         8          9         10         11         12         13         14 \n-3.6964620  4.1585197 -3.6964620  2.5875234 -1.4113764  0.5880735 -0.1809457 \n        15         16         17 \n 0.0387741 -0.0051699  0.0003231 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1388.2398     1.9213 722.541 0.000881 ***\npoly(x, 15)1  5793.9598     7.9218 731.390 0.000870 ***\npoly(x, 15)2  2372.8190     7.9218 299.528 0.002125 ** \npoly(x, 15)3   371.3985     7.9218  46.883 0.013577 *  \npoly(x, 15)4     6.3875     7.9218   0.806 0.568003    \npoly(x, 15)5    -1.2064     7.9218  -0.152 0.903787    \npoly(x, 15)6    -0.4781     7.9218  -0.060 0.961626    \npoly(x, 15)7    -8.2612     7.9218  -1.043 0.486653    \npoly(x, 15)8    -0.9076     7.9218  -0.115 0.927379    \npoly(x, 15)9    -2.6003     7.9218  -0.328 0.798087    \npoly(x, 15)10    4.4736     7.9218   0.565 0.672730    \npoly(x, 15)11    3.9691     7.9218   0.501 0.704304    \npoly(x, 15)12   -1.8700     7.9218  -0.236 0.852423    \npoly(x, 15)13   -5.3439     7.9218  -0.675 0.622195    \npoly(x, 15)14   -7.5554     7.9218  -0.954 0.515068    \npoly(x, 15)15   -6.5343     7.9218  -0.825 0.560919    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.922 on 1 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 4.179e+04 on 15 and 1 DF,  p-value: 0.003839\n\n\nWith a 15th order polynomial we get an \\(R^{2}\\) of 0.9999745, which is a nearly perfect fit. Notice the p-values, though. They indicate that the best model is actually the one with the second order term. Why is the best model not the one with the highest \\(R^{2}\\)? What we’ve done is over-fit the model to the data. We can see this by plotting the two polynomial fits.\n\n\n\n\n\nWhile the model with a 15th order polynomial perfectly fits these data, the model with the second order polynomial will generalize much better and is preferred."
  },
  {
    "objectID": "posts/wordle/index.html",
    "href": "posts/wordle/index.html",
    "title": "Wordle",
    "section": "",
    "text": "I was scrolling through the RStudio blog page, and a post by Arthur Holtz caught my attention. He wrote some code to identify what the optimal starting word is for Wordle. After some initial success with parsing the JavaScript code that contains the possible solution words, he ran into some difficulty when working out the logic for implementing a scoring system to rank the words. I’ve never played Wordle, but I thought what he was trying to do sounded interesting, so I decided to give it a go. The first few code chunks are either identical to or only slightly modified Arthur’s code to retrieve and parse the JavaScript file, so all credit goes to him."
  },
  {
    "objectID": "posts/wordle/index.html#gather-the-data",
    "href": "posts/wordle/index.html#gather-the-data",
    "title": "Wordle",
    "section": "Gather the Data",
    "text": "Gather the Data\n\nlibrary(tidyverse)\nlibrary(httr)\n\n# this doesn't work any more!!\n#\n# wordle <- \n#   GET(\"https://www.nytimes.com/games/wordle/main.18637ca1.js\") %>%\n#   content(as = \"text\", encoding = \"UTF-8\")\n\nwordle <- readRDS(\"wordle.Rdata\")\n\nThis retrieved the JavaScript for the game is a really long string. Here are the first 100 characters.\n\nsubstr(wordle, 1, 100)\n\n[1] \"c(\\\"cigar\\\", \\\"rebut\\\", \\\"sissy\\\", \\\"humph\\\", \\\"awake\\\", \\\"blush\\\", \\\"focal\\\", \\\"evade\\\", \\\"naval\\\", \\\"serve\\\", \\\"heath\\\",\"\n\n\nArthur discovered that the solutions are buried in the JavaScript in a list named Ma. The first word in the list is “cigar” and the last word is “shave”, so we can get the entire list of words by grabbing those words and everything in between.\n\n# the first word of the possible solutions follows Ma=\n# the character \"c\" is at the start index, r at the end index\nstr_locate(wordle, \"cigar\")\n\n     start end\n[1,]     4   8\n\n\nstr_locate() if from the stringr package, and it returns the indices of the first and last letters in “cigar”. Note that it returns a matrix.\n\n# it's a matrix, not a tibble\nclass(str_locate(wordle, \"cigar\"))\n\n[1] \"matrix\" \"array\" \n\n\nSimilarly, we get the indices for the last word.\n\n# the last word\nstr_locate(wordle, \"shave\")\n\n     start   end\n[1,] 20817 20821\n\n\nThis is where I start to deviate from Arthur’s code. I end up with the same data he did, just in a slightly different format. This grabs all of the text, parses the words, and puts them in a tibble with one column called words.\n\n# put the words in a tibble column\nma <- \n  tibble(\n    words = substr(\n      wordle,\n      str_locate(wordle, \"cigar\")[, 1],\n      str_locate(wordle, \"shave\")[, 2]) %>%\n      str_remove_all(\"\\\"\") %>%\n      str_remove_all(\"\\\\s+\") %>%\n      str_split(\",\") %>%\n      unlist()\n  )\nhead(ma)\n\n\n\n  \n\n\n\nNext, like in Arthur’s post, I create a tibble where each word is further parse into letters with one column per letter.\n\n# parse words to letters\ndf <-\n  str_split_fixed(ma$words, \"\", n=5) %>% \n  as_tibble(.name_repair = \"minimal\") %>% \n  set_names(c(\"one\", \"two\", \"three\", \"four\", \"five\"))\n\nhead(df)"
  },
  {
    "objectID": "posts/wordle/index.html#visualize-the-data",
    "href": "posts/wordle/index.html#visualize-the-data",
    "title": "Wordle",
    "section": "Visualize the Data",
    "text": "Visualize the Data\nThese plots are also in Arthur’s post, and I’m including them here because they give some insight into the characteristics of the solution words. First, a plot of the frequencies for all letters.\n\n# plot freq of all letters\ndf %>% pivot_longer(everything()) %>%\n  ggplot() +\n  geom_bar(aes(x = forcats::fct_infreq(value))) +\n  theme_bw() +\n  labs(title = \"All Letter Frequency\",\n       x = \"Letter\",\n       y = \"Count\")\n\n\n\n\nThen plots of letters by position. First the starting letter. Note that “s” is by far the most common, so we might expect the best starting word to also start with “s”.\n\nggplot(df) +\n  geom_bar(aes(x = forcats::fct_infreq(one))) +\n  theme_bw() +\n  labs(title = \"First Letter Frequency\",\n       x = \"Letter\",\n       y = \"Count\")\n\n\n\n\nThe second letter is heavier on vowels, which makes sense.\n\nggplot(df) +\n  geom_bar(aes(x = forcats::fct_infreq(two))) +\n  theme_bw() +\n  labs(title = \"Second Letter Frequency\",\n       x = \"Letter\",\n       y = \"Count\")\n\n\n\n\nThe third letter is even more vowel heavy.\n\nggplot(df) +\n  geom_bar(aes(x = forcats::fct_infreq(three))) +\n  theme_bw() +\n  labs(title = \"Third Letter Frequency\",\n       x = \"Letter\",\n       y = \"Count\")\n\n\n\n\nA lot of “e”’s for the fourth letter.\n\nggplot(df) +\n  geom_bar(aes(x = forcats::fct_infreq(four))) +\n  theme_bw() +\n  labs(title = \"Fourth Letter Frequency\",\n       x = \"Letter\",\n       y = \"Count\")\n\n\n\n\nAnd “e” and “y” for the last letter. Note that there aren’t many “s”’s, as Arthur pointed out. Apparently, they got rid of the plural form of four letter words. I also noticed they got rid of four letter words that you could add “r” or “d” to make a five letter word. For example “rate” to “rater” or “rated”.\n\nggplot(df) +\n  geom_bar(aes(x = forcats::fct_infreq(five))) +\n  theme_bw() +\n  labs(title = \"Fifth Letter Frequency\",\n       x = \"Letter\",\n       y = \"Count\")\n\n\n\n\n\nma %>% filter(words == \"rater\")\n\n\n\n  \n\n\nma %>% filter(words == \"rated\")"
  },
  {
    "objectID": "posts/wordle/index.html#scoring-words",
    "href": "posts/wordle/index.html#scoring-words",
    "title": "Wordle",
    "section": "Scoring Words",
    "text": "Scoring Words\nAs I mentioned earlier, implementing a scoring system is where Arthur got stuck. My first attempt was a brute force approach where, one at a time, I compare one word in the list to all others. Matching letters get three points, and correct letters in the wrong position get one point. Since there are 2309 words in the list, that means there are 2309 * 2309 = 5,331,481 comparisons. Even with some parallelization using the furrr package, this script took more than 20 minutes on my laptop. For that reason, I’m not going to execute the code in this post, but it’s below for reference. Using this code, the optimal starting word is “slate”.\nlibrary(progress)\nlibrary(furrr)\nplan(multisession, workers = 6)\n\npb <- progress_bar$new(total = 2309)\n\nmean_score <- rep(0, nrow(df))\nfor (j in 1:nrow(df)){\n  scores <- rep(0, nrow(df))\n  base <- as.vector(t(df[j, ]))\n  mean_score[j] <- \n    1:nrow(df) %>%\n    future_map(function (i){\n      tgt <- df[i, ]\n      score <- sum((base == tgt) * 3)\n      score + sum(tgt[base != tgt] %in% base[base != tgt])}\n    ) %>% unlist() %>% mean()\n  pb$tick()\n}\n\nma %>% slice(which.max(mean_score))\nAlthough brute force gives an answer, I could speed things up by vectorizing to work with all words simultaneously. I think this approach works for all cases except when there are two letters in the initial guess word (what I call tgt below) and the same two letters in the answer word that aren’t in the same location. For example, given a tgt of “erase” and answer word of “steed”, the “e”’s get a score of 1, not 2 like they should. I believe this is ok for a start word because my assumption is that a good start word should have all unique letters so that you get feedback on all five letters.\nI start with the first word in the list, “cigar”, and parse it into a vector of five letters called tgt. I then extract the unique letters from that vector, pad it with “#” so that it has a length of 5, and call it utgt for later use.\nThen I create a tibble df to implement the scoring. The first five mutate functions do some bookkeeping that I found helpful to visualize what was going on as I iterated on this solution. I also use the two columns they create as the basis for calculating the tgt word score for each of the other words in the list. There’s one mutate for each letter. In each case, if the letters match, I put the letter in the match column. If they don’t match, I check to see if it’s a correct letter in the wrong position, and if so, put it in the notmatch column.\nThe last mutate implements the scoring. The lines with c1 through c5 score the notmatch column. These are the cases when the correct letter is in the wrong position, and they get a score of 1, all other cases get a 0. The line with score uses the match column to give a score of 3 for the cases where the correct letter is in the correct location. The scores of 3’s and 1’s then get added together to give a score for each word. Before I iterate over all of the tgt words, I’ll show what the df tibble looks like for “cigar”, the first tgt word.\n\ntgt <-  as.vector(t(df[1, ])) # get the target word\nutgt <- unique(tgt)           # get the unique letters\n\nif (length(unique(tgt)) < 5){\n  utgt <- c(utgt, rep(\"#\", length(tgt) - length(utgt)))} # add padding\n\ndf %>%\n  mutate(\n    match = ifelse(tgt[1] == one, one, \"_\"),\n    notmatch = ifelse(tgt[1] != one & one %in% tgt, one, \"_\")) %>%\n  mutate(\n    match = paste0(match, ifelse(tgt[2] == two, two, \"_\")),\n    notmatch = paste0(notmatch, ifelse(tgt[2] != two & two %in% tgt, two, \"_\"))) %>%\n  mutate(\n    match = paste0(match, ifelse(tgt[3] == three, three, \"_\")),\n    notmatch = paste0(notmatch, ifelse(tgt[3] != three & three %in% tgt, three, \"_\"))) %>% \n  mutate(\n    match = paste0(match, ifelse(tgt[4] == four, four, \"_\")),\n    notmatch = paste0(notmatch, ifelse(tgt[4] != four & four %in% tgt, four, \"_\"))) %>% \n  mutate(\n    match = paste0(match, ifelse(tgt[5] == five, five, \"_\")),\n    notmatch = paste0(notmatch, ifelse(tgt[5] != five & five %in% tgt, five, \"_\"))) %>%\n  mutate(c1 = ifelse(!is.na(str_match(notmatch, utgt[1])), 1, 0),\n         c2 = ifelse(!is.na(str_match(notmatch, utgt[2])), 1, 0),\n         c3 = ifelse(!is.na(str_match(notmatch, utgt[3])), 1, 0),\n         c4 = ifelse(!is.na(str_match(notmatch, utgt[4])), 1, 0),\n         c5 = ifelse(!is.na(str_match(notmatch, utgt[5])), 1, 0), \n         score = str_count(match, \"[:alpha:]\") * 3 + c1 + c2 + c3 + c4 + c5) %>%\n  head()\n\n\n\n  \n\n\n\nI’ll use future_map() from the furrr package to iterate over all 2,309 starting words in parallel. Note that after creating the df tibble shown above, I calculate the mean of the score column and record that in the scores list. Once that completes, I display the top ten scoring starting words.\n\nlibrary(furrr) # since I didn't run the code above, load package\n\nplan(multisession, workers = 4) \n\nscores <-\n  1:nrow(df) %>%\n  future_map(function(x){\n    tgt <-  as.vector(t(df[x, ])) # get the target word\n    utgt <- unique(tgt)           # get the unique letters\n    \n    if (length(unique(tgt)) < 5){\n      utgt <- c(utgt, rep(\"#\", length(tgt) - length(utgt)))} # add padding\n    \n    df %>%\n      mutate(\n        match = ifelse(tgt[1] == one, one, \"_\"),\n        notmatch = ifelse(tgt[1] != one & one %in% tgt, one, \"_\")) %>%\n      mutate(\n        match = paste0(match, \n                       ifelse(tgt[2] == two, two, \"_\")),\n        notmatch = paste0(notmatch, \n                          ifelse(tgt[2] != two & two %in% tgt, two, \"_\"))) %>%\n      mutate(\n        match = paste0(match, \n                       ifelse(tgt[3] == three, three, \"_\")),\n        notmatch = paste0(notmatch, \n                          ifelse(tgt[3] != three & three %in% tgt, three, \"_\"))) %>% \n      mutate(\n        match = paste0(match, \n                       ifelse(tgt[4] == four, four, \"_\")),\n        notmatch = paste0(notmatch, \n                          ifelse(tgt[4] != four & four %in% tgt, four, \"_\"))) %>% \n      mutate(\n        match = paste0(match, \n                       ifelse(tgt[5] == five, five, \"_\")),\n        notmatch = paste0(notmatch, \n                          ifelse(tgt[5] != five & five %in% tgt, five, \"_\"))) %>%\n      mutate(c1 = ifelse(!is.na(str_match(notmatch, utgt[1])), 1, 0),\n             c2 = ifelse(!is.na(str_match(notmatch, utgt[2])), 1, 0),\n             c3 = ifelse(!is.na(str_match(notmatch, utgt[3])), 1, 0),\n             c4 = ifelse(!is.na(str_match(notmatch, utgt[4])), 1, 0),\n             c5 = ifelse(!is.na(str_match(notmatch, utgt[5])), 1, 0), \n             score = str_count(match, \"[:alpha:]\") * 3 + c1 + c2 + c3 + c4 + c5) %>%\n      summarise(mean_score = mean(score)) %>% unlist()\n    }\n  ) %>% unlist()\n\nma %>% \n  mutate(score = scores) %>% \n  arrange(desc(scores)) %>% \n  head(10)\n\n\n\n  \n\n\n\nHere we see “slate” is on top, followed by two other words that start with “s”. All but one of the words end in “e”. I didn’t include it here, but I tried a different scoring system with correct matches given a 5 instead of a 3, and I got the same top 3 words. We could also try giving more points to correct letters in the wrong position and other variations.\nLastly, I was curious about the distribution of scores and so plotted a histogram. I didn’t have an expectation of a shape, but I think it’s interesting that it’s normal looking.\n\nhist(scores, breaks=30)"
  },
  {
    "objectID": "posts/adjusted_stats/index.html",
    "href": "posts/adjusted_stats/index.html",
    "title": "Adjusted Statistics",
    "section": "",
    "text": "The idea for this post started off as essentially a replication of this post but using R and Tidymodels instead of Python. Once I got that done, I had an idea of making an animated plot instead of the static plot at the end of the post I referenced. I also wanted to get that code worked out because I noticed that a couple of animated images in a tutorial I had written weren’t rendering."
  },
  {
    "objectID": "posts/adjusted_stats/index.html#get-the-data",
    "href": "posts/adjusted_stats/index.html#get-the-data",
    "title": "Adjusted Statistics",
    "section": "Get the Data",
    "text": "Get the Data\nI’m not going to go through all the detail about why I’m going through these steps because, again, it’s just re-creating what was done in the above link. In a nutshell, I’m going to calculate adjusted offense and defense scores for FBS college football teams based on each team’s strength of schedule week by week through the 2021 season.\nI’m also not going to go over making API calls because 1) I covered that here, and 2) I’ve already downloaded that data and saved it to disk. You can see the code necessary for the API calls commented out, but really I’m just reading the data from disk that I saved earlier. The first data set games is basic information about each game of the season.\n\nlibrary(httr)\nlibrary(tidyjson)\nlibrary(tidymodels)\nlibrary(tidyr)\n\n# get game info for 2021\n# df <-\n#   httr::GET(\n#     url = \"https://api.collegefootballdata.com/games?year=2021\",\n#     httr::add_headers(\n#       Authorization = paste(\"Bearer\", Sys.getenv(\"YOUR_API_TOKEN\"))))\n# \n# games21 <- tibble(data = content(df, \"parsed\")) %>% unnest_wider(data)\n# \n# rm(df)\n# \n# games21 <-\n#   games21 %>%\n#   select(id, season, week, neutral_site, home_team, home_conference, home_pregame_elo,\n#          away_team, away_conference, away_pregame_elo)\n# \n# saveRDS(games21, \"games21.RData\")\n\ngames <- readRDS(\"games21.RData\")\n\nhead(games)\n\n\n\n  \n\n\n\nThen I get a dataset that identifies which teams are FBS teams, so I can filter out the games against non-FBS opponents.\n\n# get the ID of non-FBS games\n# fbs_teams <-\n#   httr::GET(\n#     url = \"https://api.collegefootballdata.com/teams/fbs?year=2021\",\n#     httr::add_headers(\n#       Authorization = paste(\"Bearer\", Sys.getenv(\"YOUR_API_TOKEN\"))))\n# \n# fbs <- \n#   tibble(data = content(fbs_teams, \"parsed\")) %>% \n#   unnest_wider(data) %>% \n#   unnest_wider(logos) %>%\n#   rename(\"logo\" = \"...1\", \"logo2\" = \"...2\") %>%\n#   select(-location)\n# \n# saveRDS(fbs, \"fbs_teams.RData\")\n# rm(fbs_teams)\n\nfbs <- readRDS(\"fbs_teams.RData\")\n\nfbsIDs <- \n  games %>% \n  filter(home_team %in% (fbs %>% .$school) & \n         away_team %in% (fbs %>% .$school)) %>% .$id\n\ngames <- games %>% filter(id %in% fbsIDs)\n\nThe last dataset contains play-by-play statistics for each games. I’m primarily interested in the ppa column, which is the predicted points added of each play and apparently is the same thing as EPA - expected points added. I also do some manipulation to account for home field advantage.\n\n# get PBP data\n# for (wk in 1:max(games$week)){\n#   print(wk)\n#   df <-\n#     httr::GET(\n#       url = paste0(\"https://api.collegefootballdata.com/plays?year=2021&week=\", as.character(wk)),\n#       httr::add_headers(\n#         Authorization = paste(\"Bearer\", Sys.getenv(\"YOUR_API_TOKEN\"))))\n#   \n#   if (wk == 1){pbp21 <- tibble(data = content(df, \"parsed\")) %>% unnest_wider(data)}\n#   else{pbp21 <- pbp21 %>% bind_rows(tibble(data = content(df, \"parsed\")) %>% unnest_wider(data))}\n# }\n# \n# rm(df, wk)\n# saveRDS(pbp21, \"pbp21.RData\")\n\npbp21 <- readRDS(\"pbp21.RData\")\n\npbp21 <-\n  pbp21 %>% \n  filter(game_id %in% fbsIDs) %>%\n  select(game_id, offense, defense, home, ppa) %>% \n  drop_na() %>%\n  left_join(games %>% \n              select(id, neutral_site, week), \n            by = c(\"game_id\" = \"id\")) %>%\n  mutate(\n    hfa = case_when(\n      home == offense ~ 1,\n      home == defense ~ -1),\n    hfa = ifelse(neutral_site, 0, hfa)) %>%\n  select(offense, hfa, defense, ppa, week) %>%\n  mutate_if(is.character, factor)\n\npbp21 <- pbp21 %>% mutate(ppa = as.numeric(as.character(ppa)))\n\nhead(pbp21)"
  },
  {
    "objectID": "posts/adjusted_stats/index.html#ridge-regression",
    "href": "posts/adjusted_stats/index.html#ridge-regression",
    "title": "Adjusted Statistics",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nThis block of code loops over the weeks of the season, tunes and fits a ridge regression model for each week. The regression model for week 1 includes all week 1 play-by-play data, the week 2 model includes week 1 and 2, and so on.\n\n# define the model\nlm_mod <- \n  linear_reg(penalty = tune(), mixture = 0) %>% \n  set_engine(\"glmnet\")\n\n# hyperparameter tuning grid\nlambda_grid <- tibble(penalty = c(0, 10^seq(-2, 2, length.out = 25)))\n\n# loop through the weeks\nfor (wk in 1:15){\n  \n  # define the recipe\n  lm_rec <-\n    recipe(ppa ~ ., data = pbp21 %>% filter(week <= wk) %>% select(-week)) %>%\n    step_dummy(all_nominal_predictors(), one_hot = TRUE)\n  \n  # cread cross-validation folds\n  folds <- vfold_cv(pbp21 %>% filter(week <= wk) %>% select(-week), v = 5)\n  \n  # define the workflow\n  lm_wflow <- \n    workflow() %>% \n    add_model(lm_mod) %>% \n    add_recipe(lm_rec)\n  \n  # get the tuning results\n  lm_res <- \n    lm_wflow %>% \n    tune_grid(\n      resamples = folds,\n      grid = lambda_grid,\n      control = control_grid(save_pred = TRUE)\n    )\n  \n  # get the model with the lowest root mean squared error\n  lowest_rmse <- lm_res %>% select_best(\"rmse\")\n  \n  # finalize the workflow with the best model\n  lm_final_wf <- \n    lm_wflow %>% \n    finalize_workflow(lowest_rmse)\n  \n  # get the final fit\n  lm_final_fit <- \n    lm_final_wf %>%\n    fit(pbp21 %>% filter(week <= wk) %>% select(-week)) \n  \n  # extract the coefficients\n  adjStats <- \n    broom::tidy(lm_final_fit) %>%\n    separate(term, into = c(\"side\", \"team\"), sep = \"_\", fill = \"left\") %>%\n    select(-penalty)\n  \n  # separate the intercept and home field advantage coefficients\n  otherTerms <- \n    adjStats %>% \n    slice(1:2) %>% \n    select(-side)\n  \n  # get the remaining coefficients \n  adjStats <- \n    adjStats %>% \n    slice(3:nrow(adjStats))\n  \n  # add the intercept term to the other coefficients\n  adjStats <- \n    adjStats %>% \n    mutate(estimate = estimate + otherTerms %>% \n             filter(team == \"(Intercept)\") %>% .$estimate)\n  \n  # fix team name formatting (Oregon.State to Oregon State)\n  adjStats <- \n    adjStats %>% \n    mutate(team = stringr::str_replace_all(team, \"\\\\.\", \" \"))\n  \n  # make dataframe wider\n  adjStats <- \n    adjStats %>% \n    pivot_wider(names_from = side, values_from = estimate) %>%\n    rename(\"adjOff\" = \"offense\", \"adjDef\" = \"defense\")\n  \n  # get the unadjusted (raw) stats - although I don't need this for the plot later\n  rawOff <- \n    pbp21 %>% \n    group_by(offense) %>% \n    summarize(meanPPA = mean(ppa)) %>%\n    rename(\"team\" = \"offense\", \"rawOff\" = \"meanPPA\")\n  \n  # same thing for raw defense\n  rawDef <- \n    pbp21 %>% \n    group_by(defense) %>% \n    summarize(meanPPA = mean(ppa)) %>%\n    rename(\"team\" = \"defense\", \"rawDef\" = \"meanPPA\")\n  \n  # bind everything together into one dataframe\n  if (wk == 1){\n    df <- \n      adjStats %>% \n      left_join(rawOff, by = \"team\") %>% \n      left_join(rawDef, by = \"team\") %>% \n      mutate(week = wk)}\n  else{\n    df_new <- \n      adjStats %>% \n      left_join(rawOff, by = \"team\") %>% \n      left_join(rawDef, by = \"team\") %>% \n      mutate(week = wk)\n    \n    df <- df %>% bind_rows(df_new)\n  }\n}\n\nhead(df)"
  },
  {
    "objectID": "posts/adjusted_stats/index.html#animated-plot",
    "href": "posts/adjusted_stats/index.html#animated-plot",
    "title": "Adjusted Statistics",
    "section": "Animated Plot",
    "text": "Animated Plot\nNow the visualization. I’ll use gganimate to animate the plot and ggimage to render team icons on the plot.\n\nlibrary(gganimate)\nlibrary(ggimage)\n\nPerforming ridge regression caused a team names to get messed up if they had non alphanumeric characters, so I’ll fix that. There’s also no week 0 to use as a starting point, so to get them roughly centered, I’ll just set them as the mean of week 1 stats. Lastly, I add the links to the team logos.\n\ndf <-\n  df %>% mutate(team = case_when(\n  team == \"Hawai i\" ~ \"Hawai'i\",\n  team == \"Miami  OH \" ~ \"Miami (OH)\",\n  team == \"Texas A M\" ~ \"Texas A&M\",\n  TRUE ~ team\n))\n\n# Add a week 0\ndf <-\n  tibble(team = fbs$school,\n         adjOff = df %>% filter(week == 1) %>% .$adjOff %>% mean(),\n         adjDef = df %>% filter(week == 1) %>% .$adjDef %>% mean(),\n         rawOff = 0,\n         rawDef = 0,\n         week = 0) %>% \n  bind_rows(df)\n\n# add logos\ndf <- df %>% \n  left_join(fbs %>% select(school, logo), by = c(\"team\" = \"school\"))\n\nTo generate the plots with team logos, I use geom_image(), and the last 5 lines are used by gganimate in a later code chunk. This block of code is fast to execute.\n\nstats_plot <-\n  ggplot(df, aes(x=adjOff, y=adjDef)) +\n  geom_image(aes(image = logo, group = seq_along(logo))) +\n  theme_bw() +\n  scale_y_reverse() +\n  xlab(\"Adjusted Offense\") +\n  ylab(\"Adjusted Defense\") +\n  ggtitle(\"Week {closest_state}\") +\n  transition_states(week, transition_length = 3, state_length = 3, wrap = FALSE) +\n  ease_aes(\"linear\")\n\nFinally I’ll create the animated gif. It took my laptop about 15 minutes to generate the graphic. One thing that could be improved for efficiency is that the logo column contains URLs for the team logos. There are 130 logos and 16 weeks (including week 0), so I’m getting the same 130 logos 16 times when I should only need to get them once. I haven’t worked out how to solve that yet, but for now at least I have something that works.\n\nanimate(stats_plot, \n        width = 800, height = 800, \n        fps = 10, nframes = 90, end_pause = 10)"
  },
  {
    "objectID": "posts/collatz/index.html",
    "href": "posts/collatz/index.html",
    "title": "Collatz Conjecture",
    "section": "",
    "text": "The inspiration for a series of posts on generative art was a post I read announcing the aRtsy package for producing generative art. A number of the example canvases looked nice, so I installed the package from the author’s GitHub repository. The first canvas I tried was canvas_collatz(), but for whatever reason, instead of the canvas as shown in the example below, it produced just one wavy line.\nI looked into the source code in the repo to see if I could tell what was going wrong, but the R scripts were calling functions that I couldn’t find. So then I thought, why not just write my own script to create the art?"
  },
  {
    "objectID": "posts/collatz/index.html#collatz-conjecture",
    "href": "posts/collatz/index.html#collatz-conjecture",
    "title": "Collatz Conjecture",
    "section": "Collatz Conjecture",
    "text": "Collatz Conjecture\nBetween the Wikipedia page for the Collatz conjecture and the aRtsy package description, I thought I had enough to go on. First, I started with generating just one number sequence. The process is this.\n\nRandomly choose a positive integer.\nIf it’s even, divide by two.\nIf it’s odd, multiply by three and add one.\n\nRepeat 2 and 3 and keep track of the sequence of numbers. The Collatz conjecture states that no matter what number you start with, the sequence will always reach the number 1. So, once the sequence reaches 1, stop the sequence. Let’s do it!\n\nset.seed(1)                # 1 seems appropriate for this problem \n\nn <- sample(2:1000000, 1)  # choose a random number between 2 and one million\nns <- n                    # add it to the sequence\n\nwhile (n > 1){             # stop the sequence when we reach 1 \n  if(n %% 2 == 0){         # check if the number is even\n    n <- n / 2             # divide by 2\n    ns <- c(ns, n)         # add it to the sequence\n  }else{                   # if it's odd\n    n <- 3*n + 1           # do the math\n    ns <- c(ns, n)}        # add it to the sequence\n}\n\nns\n\n  [1]  548677 1646032  823016  411508  205754  102877  308632  154316   77158\n [10]   38579  115738   57869  173608   86804   43402   21701   65104   32552\n [19]   16276    8138    4069   12208    6104    3052    1526     763    2290\n [28]    1145    3436    1718     859    2578    1289    3868    1934     967\n [37]    2902    1451    4354    2177    6532    3266    1633    4900    2450\n [46]    1225    3676    1838     919    2758    1379    4138    2069    6208\n [55]    3104    1552     776     388     194      97     292     146      73\n [64]     220     110      55     166      83     250     125     376     188\n [73]      94      47     142      71     214     107     322     161     484\n [82]     242     121     364     182      91     274     137     412     206\n [91]     103     310     155     466     233     700     350     175     526\n[100]     263     790     395    1186     593    1780     890     445    1336\n[109]     668     334     167     502     251     754     377    1132     566\n[118]     283     850     425    1276     638     319     958     479    1438\n[127]     719    2158    1079    3238    1619    4858    2429    7288    3644\n[136]    1822     911    2734    1367    4102    2051    6154    3077    9232\n[145]    4616    2308    1154     577    1732     866     433    1300     650\n[154]     325     976     488     244     122      61     184      92      46\n[163]      23      70      35     106      53     160      80      40      20\n[172]      10       5      16       8       4       2       1"
  },
  {
    "objectID": "posts/collatz/index.html#creating-art",
    "href": "posts/collatz/index.html#creating-art",
    "title": "Collatz Conjecture",
    "section": "Creating Art",
    "text": "Creating Art\nOk, now that I have a sequence of numbers, how do I turn that into a line? According to the description in the GitHub repo, by “bending the edges differently for even and odd numbers in the sequence”. I wasn’t certain exactly meant in terms of code, but my first thought was just to do a little trigonometry and follow these steps:\n\nReverse the sequence in order to start with 1.\nAlso pick a starting angle - I chose 0.\nFor the first number (1), assign it the (x, y) coordinates of (0, 0).\nLook at the next number in the sequence.\nIf it’s even, update the angle by:\n\nnew angle = old angle + 0.0075\n\nIf it’s odd, update the angle by:\n\nnew angle = old angle - 0.0145\n\nCalculate the next coordinate by:\n\nnew x = old x + cos(new angle)\nnew y = old y + sin(new angle)\n\n\nRepeat steps 3-6 for the rest of the sequence. The following code does the trick.\n\neven_angle = 0.0075 \nodd_angle = 0.0145\n\ndf <- data.frame(n = rev(ns)) # dataframe to store coordinates\n\nangle <- 0\nx <- rep(1, length(ns))       # initialize x coords with 1's\ny <- rep(1, length(ns))       # same for y coords\n\nfor (i in 2:length(ns)){\n  if (ns[i] %% 2 == 0){       # check for even number\n    angle <- angle + even_angle\n    x[i] <- x[i-1] + cos(angle)\n    y[i] <- y[i-1] + sin(angle)\n  }else{\n    angle <- angle - odd_angle\n    x[i] <- x[i-1] + cos(angle)\n    y[i] <- y[i-1] + sin(angle)}\n}\ndf$x <- x\ndf$y <- y\n\nhead(df)\n\n\n\n  \n\n\n\nLet’s see how that looks in a plot.\n\nlibrary(ggplot2)\n\ntheme_set(theme_bw())\n\nggplot(df) +\n  geom_line(aes(x=x, y=y))\n\n\n\n\nThat looks promising, so now I’ll generate 200 sequences the same way. I’ll number each sequence 1-200 as I create them and store the sequence number in column named gp.\n\nset.seed(1)\n\nfor (i in 1:200){\n  n <- sample(2:1000000, 1)\n  ns <- n\n  \n  while (n > 1){\n    if(n %% 2 == 0){\n      n <- n / 2\n      ns <- c(ns, n)\n    }else{\n      n <- 3*n + 1\n      ns <- c(ns, n)}\n  }\n  ifelse(i == 1, \n         df <- data.frame(n = rev(ns), gp = i), \n         df <- rbind(df, data.frame(n = rev(ns), gp = i)))\n}\n\nNext I generate all of the coordinates for each sequence.\n\ndf$x <- 0\ndf$y <- 0\n\nfor (j in 1:200){\n  angle <- 0\n  sq <- df[df$gp == j, \"n\"]\n  x <- rep(1, length(sq))\n  y <- rep(1, length(sq))\n  for (i in 2:length(sq)){\n    if (sq[i] %% 2 == 0){\n      angle <- angle + even_angle\n      x[i] <- x[i-1] + cos(angle)\n      y[i] <- y[i-1] + sin(angle)\n    }else{\n      angle <- angle - odd_angle\n      x[i] <- x[i-1] + cos(angle)\n      y[i] <- y[i-1] + sin(angle)}\n  }\n  df[df$gp == j, \"x\"] <- x\n  df[df$gp == j, \"y\"] <- y\n}\n\nhead(df)\n\n\n\n  \n\n\n\nThis time, instead of ggplot2, I’m going to use plotly to create the graphic because it might be interesting to zoom in on different parts of the plot. I’m going to hide all of the axis labels, grid lines, etc. so that the final plot looks more like a canvas. I’ll also apply the Spectral color palette from RColorBrewer and make the background black.\n\nlibrary(plotly)\nlibrary(RColorBrewer)\n\nnoax <- list(\n  title = \"\",\n  zeroline = FALSE,\n  showline = FALSE,\n  showticklabels = FALSE,\n  showgrid = FALSE\n)\n\ndf %>% mutate(gp = factor(gp)) %>%\n  plot_ly() %>%\n  add_lines(x=~x, y=~y, color=~gp, colors = colorRampPalette(brewer.pal(11, \"Spectral\"))(200),\n            hoverinfo = \"none\",\n            opacity = 0.5, showlegend = FALSE) %>%\n  layout(xaxis = noax,\n         yaxis = noax,\n         paper_bgcolor = \"#000000\", plot_bgcolor = \"#000000\")\n\n\n\n\n\nThe images this algorithm generates remind me of feathers, flowers, or grass. Maybe animating the plot would produce an interesting effect.\n\nIt seemed to me that there are a number of knobs one could turn to get different effects, like the seed, the number of sequences, the amount of bend in the lines, and the choice of color palettes. So, I made a Shiny App for this and other generative art algorithms."
  },
  {
    "objectID": "posts/colonize/index.html",
    "href": "posts/colonize/index.html",
    "title": "Space Colonization",
    "section": "",
    "text": "Continuing the series of generative art posts, I liked the look of the canvas_petri() images from the aRtsy package. That led me to this article that describes the algorithm behind the canvas_petri() in enough detail that I thought I could give it a shot."
  },
  {
    "objectID": "posts/colonize/index.html#space-colonization-algorithm",
    "href": "posts/colonize/index.html#space-colonization-algorithm",
    "title": "Space Colonization",
    "section": "Space Colonization Algorithm",
    "text": "Space Colonization Algorithm\nTurns out the images are generated using something called the space colonization algorithm. When I first read that, this kind of space colonization came to mind.\n\n\n\nvia GIPHY\n\n\nThat’s not it, though. They mean colonizing the space inside of, say, a circle. Borrowing an image from the article above, we’re looking for something more like this.\n\n\n\nImage from Jason Webb article above.\n\n\nJason’s article does a good job describing the algorithm and shows images to visualize the process, so I’ll just paraphrase. It goes like this:\n\nRandomly place points in some sort of bounding shape (rectangle, circle, etc.). These are called “attractors”.\nPlace one or more “nodes” in the same shape. These will serve as the starting points for the vein-like lines in the above image. In the image above, there’s just one starting node in the middle right.\nFor every attractor, find the node closest to it. If you start with one “seed” node, of course there’s only one node to choose from. We’ll add nodes in a later step, so this gets more complicated as we progress.\nFind the coordinates of the centroid of the attractors closest to each node.\nAdd another node some distance away from the original node in the direction of the centroid.\nNow consider a radius around each attractor - Jason called that area the kill zone. If any new nodes are within an attractor’s kill zone, delete the attractor.\nContinue this process until all attractors have been deleted.\nVisualize by plotting lines between all of the nodes.\n\nSeems relatively straight forward, so let’s do it step by step.\n\nPlace Attractors\nI’ll place 50 attractors in a 10 x 10 square.\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\nset.seed(1)\natt <- data.frame(x = runif(50, 0, 10), y = runif(50, 0, 10))\n\nhead(att)\n\n\n\n  \n\n\n\n\n\nPlace A Node\nEasy enough. I’ll just put it in the center of the square. I’m adding a couple other columns to this data frame. n will keep track of the node numbers, and s will be used to keep track of line segments.\n\nnodes <- data.frame(n = 1, x=5, y=5, s=1)\n\n\n\nFind The Closest Nodes\nAgain, finding the closest node to each attractor is trivial when there’s only one node, but it will get more complicated as we add nodes. First, though, as I was working through this script, I found that sometimes at this step attractors are aready within the kill zone, so let’s check for that and delete them if needed.\n\nkill_distance <- 0.25\n\natt <- att[sapply(1:nrow(att), \n                  function(x) \n                    min(sqrt((att[x, ]$x - nodes$x)^2 + (att[x, ]$y - nodes$y)^2)) > kill_distance), ]\n\nnrow(att)\n\n[1] 50\n\n\nNot this time, but we’ll re-use this bit of code after adding new nodes. I’m going to add a new column cl to record the closest node number to each attractor.\n\natt$cl <- sapply(1:nrow(att), \n                 function(x) \n                   nodes[which.min(sqrt((att[x, ]$x - nodes$x)^2 + (att[x, ]$y - nodes$y)^2)), \"n\"])\n\nhead(att)\n\n\n\n  \n\n\n\n\n\nCentroids\nSince node 1 is the closest to all attractors, there’s only one centroid to find. mx is the mean x coordinate, and my is the mean y coordinate. We also need the angle, theta from the node to the centroid.\n\n# get each centroid coords\nns <- unique(att$cl)\nmx <- sapply(ns, function(x)mean(att[att$cl==x, \"x\"]))\nmy <- sapply(ns, function(x)mean(att[att$cl==x, \"y\"]))\n\n# get the angle from the nodes to the centroids\nthetas <- sapply(\n  1:length(ns), \n  function(x)\n    ifelse(mx[x] > nodes[nodes$n==ns[x], \"x\"],\n           atan((my[x] - nodes[nodes$n==ns[x], \"y\"]) / (mx[x] - nodes[nodes$n==ns[x], \"x\"])),\n           pi + atan((my[x] - nodes[nodes$n==ns[x], \"y\"]) / (mx[x] - nodes[nodes$n==ns[x], \"x\"])))\n)\n\nprint(round(c(mx, my, thetas), 2))\n\n[1] 5.33 5.03 0.09\n\n\n\n\nAdd New Nodes\nI initialize the line segment counter I mentioned earlier. This is just to keep track of the segments for plotting purposes. I store the values in the s column. newx and newy are the coordinates for each new node. I store each new node along with the line segment information in a data frame called new_n, and then combine it with the nodes data frame. While I’m at it, I delete an attractor if a new node is within its kill zone. We should now have three rows in nodes: the original node associated with segment 1, and the original node and new node associated with segment 2.\n\nseq_count <- 1 # sequence counter\nseg_len <- 0.1 # the length of the line segment to a new node\n\nfor (i in 1:length(ns)){\n  \n  seq_count <- seq_count + 1\n  \n  newx <- seg_len * cos(thetas[i]) + nodes[nodes$n==ns[i], 'x']\n  newy <- seg_len * sin(thetas[i]) + nodes[nodes$n==ns[i], 'y']\n  \n  new_n <- data.frame(n = 1:2 + max(nodes$n),\n                      x=c(nodes[nodes$n==ns[i], 'x'], newx),\n                      y=c(nodes[nodes$n==ns[i], 'y'], newy),\n                      s=rep(seq_count, 2)\n                      )\n  \n  nodes <- rbind(nodes, new_n)\n  \n  # check if the node is within kill distance of an attractor\n  att <- att[sapply(1:nrow(att), \n                    function(x) \n                      min(sqrt((att[x, ]$x - nodes$x)^2 + (att[x, ]$y - nodes$y)^2)) > kill_distance), ]\n}\n\nnodes\n\n\n\n  \n\n\n\nLet’s plot it to see what we have so far. The red line connects the nodes, and the attractors are dark green.\n\nggplot() +\n  geom_path(data=nodes, aes(x=x, y=y, group=s), \n            color=\"red\", size=2, lineend=\"round\") +\n  geom_point(data=att, aes(x=x, y=y), \n             color=\"darkgreen\", size=2) +\n  coord_fixed() +\n  theme_bw() +\n  theme(legend.position=\"none\")\n\n\n\n\nNow it’s just a matter of grinding through it until we finish deleting the nodes. Before we do that, let’s put all that code into a get_colony() function because we’re going to want to mess around with the different values we set earlier. We’ll want to pass different shapes and arrangements of attractors, specify the seed, number of attractors, the kill zone, line segment length, and the coordinates of the first node (the trunk). The only thing new in the code below is the while loop, which checks for an empty att dataframe - our stopping condition for when all of the attractors have been deleted\n\nget_colony <- \n  function(att, seed=1, kill_distance=0.25, segment_length=0.1, trunk_x=0, trunk_y=0){\n\n  set.seed(seed)\n  \n  # where the colony starts from\n  nodes <- data.frame(n = 1, x=trunk_x, y=trunk_y, s=1)\n  \n  # delete attractors that are already within kill distance\n  att <- \n    att[sapply(1:nrow(att), \n               function(x) \n                 min(sqrt((att[x, ]$x - nodes$x)^2 + (att[x, ]$y - nodes$y)^2)) > kill_distance), ]\n  \n  seq_count <- 1\n  seg_len <- segment_length\n  \n  while (nrow(att) > 0){\n    # id the closest node to each attractor\n    att$cl <- \n      sapply(1:nrow(att), \n             function(x) \n               nodes[which.min(sqrt((att[x, ]$x - nodes$x)^2 + (att[x, ]$y - nodes$y)^2)), \"n\"])\n    \n    # get each centroid coords\n    ns <- unique(att$cl)\n    mx <- sapply(ns, function(x)mean(att[att$cl==x, \"x\"]))\n    my <- sapply(ns, function(x)mean(att[att$cl==x, \"y\"]))\n    \n    # get the angle from the nodes to the centroids\n    thetas <- sapply(\n      1:length(ns), \n      function(x)\n        ifelse(mx[x] > nodes[nodes$n==ns[x], \"x\"],\n               atan((my[x] - nodes[nodes$n==ns[x], \"y\"]) / (mx[x] - nodes[nodes$n==ns[x], \"x\"])),\n               pi + atan((my[x] - nodes[nodes$n==ns[x], \"y\"]) / (mx[x] - nodes[nodes$n==ns[x], \"x\"])))\n    )\n    \n    \n    # add a new segment in that direction\n    for (i in 1:length(ns)){\n      seq_count <- seq_count + 1\n      newx <- seg_len * cos(thetas[i]) + nodes[nodes$n==ns[i], 'x']\n      newy <- seg_len * sin(thetas[i]) + nodes[nodes$n==ns[i], 'y']\n      new_n <- data.frame(n = 1:2 + max(nodes$n),\n                          x=c(nodes[nodes$n==ns[i], 'x'], newx),\n                          y=c(nodes[nodes$n==ns[i], 'y'], newy),\n                          s=rep(seq_count, 2)\n      )\n      nodes <- rbind(nodes, new_n)\n      # check if the node is within kill distance of an attractor\n      att <- \n        att[sapply(1:nrow(att), \n                   function(x) \n                     min(sqrt((att[x, ]$x - nodes$x)^2 + (att[x, ]$y - nodes$y)^2)) > kill_distance), ]\n      \n      if (nrow(att) < 1) break\n    }\n  }\n  nodes\n}"
  },
  {
    "objectID": "posts/colonize/index.html#visualizations",
    "href": "posts/colonize/index.html#visualizations",
    "title": "Space Colonization",
    "section": "Visualizations",
    "text": "Visualizations\nLet’s finish what we started. Here I call the function with the same parameters as earlier, and the function returns a data frame with all of the nodes for plotting.\n\ndf <- \n  get_colony(att, kill_distance=0.25, segment_length=0.1, trunk_x=5, trunk_y=5)\n\nhead(df)\n\n\n\n  \n\n\n\nAnd here it is.\n\nggplot() +\n  geom_path(data=df, aes(x=x, y=y, group=s), \n            color=\"red\", size=2, lineend=\"round\") +\n  geom_point(data=att, aes(x=x, y=y), \n             color=\"darkgreen\", size=2) +\n  coord_fixed() +\n  theme_bw() +\n  theme(legend.position=\"none\")\n\n\n\n\nPretty weird looking, right? We can make a more interesting image by adding more attractors (500!), plotting only the line segments, and adding some varying thickness to the lines.\n\nset.seed(1)\natt <- data.frame(x = runif(500, 0, 10), y = runif(500, 0, 10))\n\ndf <- \n  get_colony(att, kill_distance=0.25, segment_length=0.1, trunk_x=5, trunk_y=5)\n\nggplot() +\n  geom_path(data=df, \n            aes(x=x, y=y, group=s, size=1/s), \n            color=\"black\", lineend=\"round\") +\n  coord_fixed() +\n  theme_void() +\n  theme(legend.position=\"none\")\n\n\n\n\n\nCircles\nThat’s a little better. It’s getting that organic feel to it, I think. Let’s try a bounding circle instead of a square. We’ll put the starting seed in the middle of the circle at (0, 0).\n\nset.seed(1)\n# put all the attractors in a circle\na <- runif(500) * 2 * pi\nr <- 5 * sqrt(runif(500))\natt <- data.frame(x = r*cos(a), y = r*sin(a))\n\ndf <- \n  get_colony(att, kill_distance=0.25, segment_length=0.1, trunk_x=0, trunk_y=0)\n\nggplot() +\n  geom_path(data=df, \n            aes(x=x, y=y, group=s, size=1/s), \n            color=\"black\", lineend=\"round\") +\n  coord_fixed() +\n  theme_void() +\n  theme(legend.position=\"none\")\n\n\n\n\nHow about moving the starting node to the right?\n\ndf <- \n  get_colony(att, kill_distance=0.25, segment_length=0.1, trunk_x=4.5, trunk_y=0)\n\nggplot() +\n  geom_path(data=df, \n            aes(x=x, y=y, group=s, size=1/s), \n            color=\"black\", lineend=\"round\") +\n  coord_fixed() +\n  theme_void() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\nTrees\nSeeing the image above made me think of a sideways bush, and then it occurred to me that I can make a tree-like image by placing the starting node below the bounding circle. While I’m at it, I’ll plot the attractors as green circles like I did earlier to give a leaf-like effect.\n\ndf <- \n  get_colony(att, kill_distance=0.25, segment_length=0.1, trunk_x=0, trunk_y=-8)\n\nggplot() +\n  geom_path(data=df, \n            aes(x=x, y=y, group=s, size=1/s), \n            color=\"black\", lineend=\"round\") +\n  geom_point(data=att, aes(x=x, y=y), \n             color=\"darkgreen\", size=2) +\n  coord_fixed() +\n  theme_void() +\n  theme(legend.position=\"none\")\n\n\n\n\nWe could continue by exploring different bounding shapes, color combinations, and adjusting all of the function parameters to make a variety of shapes.\n\na <- runif(250) * pi\nr <- 5 * sqrt(runif(250, 0.9, 1.0))\natt <- data.frame(x = r*cos(a), y = r*sin(a))\n\ndf <- \n  get_colony(seed=2, att, kill_distance=0.05, segment_length=0.01, trunk_x=0, trunk_y=-0.02)\n\nggplot() +\n  geom_path(data=df, \n            aes(x=x, y=y, group=s, size=1/s), \n            color=\"black\", lineend=\"round\") +\n  geom_point(data=att, aes(x=x, y=y), \n             color=\"darkgreen\", size=2) +\n  coord_fixed() +\n  theme_void() +\n  theme(legend.position=\"none\")\n\n\n\n\n\na <- runif(250) * pi\nr <- 5 * sqrt(runif(250))\natt <- data.frame(x = r*cos(a), y = r*sin(a))\n\ndf <- \n  get_colony(seed=2, att, kill_distance=0.25, segment_length=0.1, trunk_x=0, trunk_y=-4)\n\nggplot() +\n  geom_path(data=df, \n            aes(x=x, y=y, group=s, size=1/s), \n            color=\"black\", lineend=\"round\") +\n  geom_point(data=att, aes(x=x, y=y), \n             color=\"darkgreen\", size=2) +\n  coord_fixed() +\n  theme_void() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\nAnimated Tree\nOne last idea. Let’s animate the tree “growing”!\n\n\n\n\n\nYou can click here for my Shiny app to generate your own art using this and other algorithms. Enjoy!"
  },
  {
    "objectID": "posts/decipher/index.html",
    "href": "posts/decipher/index.html",
    "title": "Substitution Cyphers",
    "section": "",
    "text": "Stanford professor Persi Diaconis described an event when a psychologist from the California prison system brought encoded messages obtained from inmates to the Statistics Department’s drop-in consulting service and asked if the messages could be decrypted(Diaconis 2008). The problem was tackled by Stanford student Marc Coram, who guessed that the message was encoded using a simple substitution cipher. Using the Markov Chain Monte Carlo (MCMC) method, Coram’s algorithm was able to correctly decipher the message without enumerating through all of the possible letter combinations. This paper presents an algorithm in Python that replicates Coram’s approach and evaluates some minimum conditions required for the technique to be successful. The Python code used for this project is included as an appendix to this report."
  },
  {
    "objectID": "posts/decipher/index.html#cryptography-background",
    "href": "posts/decipher/index.html#cryptography-background",
    "title": "Substitution Cyphers",
    "section": "Cryptography Background",
    "text": "Cryptography Background\nText encryption is the process of turning intelligible text into unintelligible text, and text decryption is the opposite process. A cipher is a pair of algorithms the create the encrypted and decrypted texts. The operation of the cipher is controlled by the both the algorithm and a key. The cipher used in the inmates’ messages substituted a different symbol for each letter of the alphabet, punctuation, and spaces between words. Instead of using symbols in this project, letters were randomly chosen to substitute for other letters in a string of text."
  },
  {
    "objectID": "posts/decipher/index.html#monte-carlo-markov-chain-method",
    "href": "posts/decipher/index.html#monte-carlo-markov-chain-method",
    "title": "Substitution Cyphers",
    "section": "Monte Carlo Markov Chain Method",
    "text": "Monte Carlo Markov Chain Method\nWhen faced with the challenge of deciphering text encoded with a substitution cipher, an initial approach might be to attempt to match letter frequencies in the encoded text with average letter frequencies for the English language. Chen tried this approach and found that it had limited success, correctly decoding just 16 out of 26 letters. Because of this, Coram instead used two letter transition frequencies.\n\nTwo Letter Transitions\nFor this project, I obtained a text file of War and Peace from www.textfiles.com and counted the number of times each of the two letter combinations appeared. All numbers, punctuation, and spaces were combined into a single category of a non-letter. I used the space character to represent these non-letters in order to have the resulting decrypted text be more readable. I chose War and Peace as the reference text because of its length. A longer text produces a larger sample size for calculating statistics on two-letter frequencies. Additionally, a novel is a better choice than a dictionary of the English language because the resulting two-letter frequencies will represent what would be expected in natural language. For example, since the word “the” is the most common word in written English, the two-letter pairs of TH and HE should be relatively frequent.\n\nThis is a plot of the normalized transition probabilities that I used to check the accuracy of the Python function. The QU transition stands out, and the TH and HE frequencies are high as expected from the common use of the word “the”. T is the most common letter to start a word, and D and Y are the most common letters to end a word. These results match expectations, so the function appears to be correct.\n\n\nMetropolis-Hastings Algorithm\nA state space of the Markov Chain in this application is represented by a unique sequence of each of the 26 letters. For example, if the original text was encoded with a key where only the first two letters were swapped, that would be represented by the state BACDEF…XYZ. Since the order of substitution for encrypted text is unknown, the correct decryption key is one of 26! possible states. Iterating through each of the 26! possibilities is infeasible, and that is where the MCMC approach comes to the rescue.\nThe Metropolis-Hastings Algorithm is a technique for randomly sampling from the possible state spaces in such a way as to efficiently converge to the correct decryption. As described in Chen, for each successive pair of characters \\(\\beta_{1}\\), \\(\\beta_{2}\\), the expression \\(r(\\beta_{1},\\beta_{2})\\) records the number of times each particular pair of characters appears in the reference text War and Peace. Similarly, \\(f_{x}(\\beta_{1},\\beta_{2})\\) records the number of times each two-letter pair occurs in the target text after it was decrypted with key x from the state space. For a particular decryption key x, its score function is as follows:(Chen and Rosenthal 2011)\n\\[\n\\pi(x)=\\prod_{\\beta_{1},\\beta_{2}}r(\\beta_{1},\\beta_{2})^{f_{x}(\\beta_{1},\\beta_{2})}\n\\]\nWhen I implemented this expression in Python, there were significant numerical errors due to multiplying large numbers. Therefore, I modified the expression so that the target text letter count is multiplied by the logarithm of the reference text count. I then added the products of all of the two-letter pairs since adding logarithms is equivalent to successive multiplication. For example, if the number of times AB appeared in the decrypted text was 12 and the number of times AB appeared in War and Peace was 3,437, then 12 was multiplied by log(3437). This product was added to the product of every other possible two-letter pair for a total score for that particular decryption key. This logic is implemented in the following code where in the last line, key_score represents the score for the decryption key, v represents a two-letter count in the target text, and trans_counts[k] represents the same two-letter pair count in War and Peace.\n```{python}\nfor k,v in target_counts.items():\n        if k in trans_counts:\n            key_score += v * math.log(trans_counts[k])\n```\nAfter calculating a score for the decryption key, a proposal key is generated by randomly choosing two letters from the key and swapping their positions. So if A and G are randomly chosen and A mapped to S and G mapped to Y, then in the proposed key, A maps to Y and G maps to S. A score for the proposed key is calculated and compared to the current key. If the proposed key’s score is greater than the current key’s score, the score ratio is greater than one, and it produces a decrypted text with letter transitions that more closely match the reference text, so it is accepted as the new decryption key. If the score ratio is less than one, then the proposed key is accepted with a probability equal to the score ratio. This allows lower probability keys to be explored during the random walk. In Python, it is implemented by drawing a random number from a uniform distribution ranging from 0 to 1 and comparing it to the score ratio. The Python implementation of this logic is below. Note that the exponent of the difference of the scores is required to negate the logarithm of the two-letter count described earlier.\n```{python}\n#generate a proposed decryption key and get scores\nproposed_decrypt_key = get_proposed_key(current_decrypt_key)\ncurrent_score = score(current_decrypt_key, encoded_text, trans_counts)\nproposed_score = score(proposed_decrypt_key, encoded_text, trans_counts)\n\n#calculate the acceptance probability\nap = min(1, math.exp(proposed_score - current_score))\n\n#generate a random number between 0 and 1\nrunif = np.random.uniform()\n\n#accept the proposed key if the random number is less than the acceptance probability\nif runif <= ap: accept_proposed_key = True\nelse: accept_proposed_key = False\nif accept_proposed_key: current_decrypt_key = proposed_decrypt_key\n```"
  },
  {
    "objectID": "posts/decipher/index.html#results",
    "href": "posts/decipher/index.html#results",
    "title": "Substitution Cyphers",
    "section": "Results",
    "text": "Results\nIn this section, I will test the algorithm on two different short texts, which I refer to as the target texts, using War and Peace as the reference text for letter transition probabilities. The first target text is taken from a passage in the movie Strange Brew. I chose this text to represent typical spoken English, and it contains 12 sentences, 202 words, and 944 characters. I will repeat the test on the movie passage using increasingly shorter segments of the passage to determine the approximate minimum length of text needed. I will also test the algorithm on the movie massage by using a shorter reference text Alice’s Adventures in Wonderland. The other target text is a portion of the decrypted text obtained from the prison psychologist. Because this text contains slang, mis-spellings, and several words in Spanish, it should provide a challenge to the decryption algorithm. For this target text, I will again use War and Peace as the reference text.\n\n\nCode\nimport numpy as np\nimport math\nimport pandas as pd\n\n\n\n\nCode\ndef get_trans_counts(plaintext):\n    '''takes .txt file and creates a dictionary with two-letter combinations as keys and their counts as values.\n    returns a dictionary of the counts in the form {AB:343, AC:112, etc}'''\n    trans_counts = {}\n    chars = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\n    f = open(plaintext)\n    wp = []\n    for line in f:\n        wp.append(line)\n    f.close()\n    for line in wp:\n        #convert to upper case\n        line = str.upper(line)\n        #remove the \\n newline characters at the end of the line\n        line = line[0:len(line)-1]\n        #count the number of two letter pairs\n        for i in range(len(line)-1):\n            twoletter_key = line[i] + line[i+1]\n            #two consecutive letters\n            if (line[i] in chars) & (line[i+1] in chars):\n                if twoletter_key not in trans_counts: trans_counts[twoletter_key] = 1\n                else: trans_counts[twoletter_key] += 1\n            #non-letter followed by a letter\n            elif (line[i] not in chars) & (line[i+1] in chars):\n                twoletter_key = \" \" + line[i+1]\n                if twoletter_key not in trans_counts: trans_counts[twoletter_key] = 1\n                else: trans_counts[twoletter_key] += 1\n            #letter followed by a non-letter\n            elif (line[i] in chars) & (line[i+1] not in chars):\n                twoletter_key = line[i] + \" \"\n                if twoletter_key not in trans_counts: trans_counts[twoletter_key] = 1\n                else: trans_counts[twoletter_key] += 1\n            #two non-letters\n            elif (line[i] not in chars) & (line[i+1] not in chars):\n                twoletter_key = \" \" + \" \"\n                if twoletter_key not in trans_counts: trans_counts[twoletter_key] = 1\n                else: trans_counts[twoletter_key] += 1\n    return trans_counts\n\ndef map_key(key):\n    '''given a encryption or decryption key, returns a dictionary where the dictionary key is the letter from the key\n    and the value is the letter of the alphabet it maps to in the form {A:R, B:W, C:O, etc}'''\n    alphabet = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\n    mapping = {}\n    for i in range(len(key)): mapping[alphabet[i]] = key[i]\n    return mapping\n\ndef apply_key(key, text):\n    '''encodes/decodes text given a de/encryption key (a sequence of 26 letters) and a string of text\n    returns a new text (string)'''\n    mapped_text = \"\"\n    #convert the text to a list\n    text = list(text)\n    #get the mapping from map_key\n    mapping = map_key(key)\n    #apply the mapping based on the key\n    for letter in text:\n        #convert to upper case\n        letter = str.upper(letter)\n        if letter in mapping: mapped_text += mapping[letter]\n        else: mapped_text += \" \"\n    return mapped_text\n\ndef score(key, text, trans_counts):\n    '''calculates the score of a decryption key based on it's log-likliness to the referece text transition counts.\n    Returns a score value (float)'''\n    #get the current mapping\n    mapping = map_key(key)\n    #decode the text based on the mapping\n    decoded = apply_key(key, text)\n    key_score = 0\n    target_counts = {}\n    chars = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\n    #strip the text\n    stripped_text = decoded.strip()\n    #convert the target text into a list of characters\n    text_list = list(stripped_text)\n    #count the number of two letter pairs in the target text\n    for i in range(len(text_list)-1):\n        twoletter_key = text_list[i] + text_list[i+1]\n        #two consecutive letters\n        if (text_list[i] in chars) & (text_list[i+1] in chars):\n            if twoletter_key not in target_counts: target_counts[twoletter_key] = 1\n            else: target_counts[twoletter_key] += 1\n        #non-letter followed by a letter\n        elif (text_list[i] not in chars) & (text_list[i+1] in chars):\n            twoletter_key = \" \" + text_list[i+1]\n            if twoletter_key not in target_counts: target_counts[twoletter_key] = 1\n            else: target_counts[twoletter_key] += 1\n        #letter followed by a non-letter\n        elif (text_list[i] in chars) & (text_list[i+1] not in chars):\n            twoletter_key = text_list[i] + \" \"\n            if twoletter_key not in target_counts: target_counts[twoletter_key] = 1\n            else: target_counts[twoletter_key] += 1\n        #two non-letters\n        elif (text_list[i] not in chars) & (text_list[i+1] not in chars):\n            twoletter_key = \" \" + \" \"\n            if twoletter_key not in target_counts: target_counts[twoletter_key] = 1\n            else: target_counts[twoletter_key] += 1\n    for k,v in target_counts.items():\n        if k in trans_counts:\n            key_score += v*math.log(trans_counts[k])\n    return key_score\n\ndef get_proposed_key(key):\n    '''Takes a decryption key, randomly selects two letters, and swaps they keys for those letters. So if A mapped to X\n    and B mapped to Q, then A maps to Q and B to X.\n    Returns a new decryption key (string)'''\n    proposed_key = \"\"\n    chars = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\n    #randomly select two letters to swap\n    char1, char2 = np.random.choice(chars, size=2, replace=False)\n    #create a new key, swapping letters in the old key\n    new_key = list(key)\n    for i in range(len(new_key)):\n        if new_key[i] == char1: index1 = i\n        if new_key[i] == char2: index2 = i\n    new_key[index1] = char2\n    new_key[index2] = char1\n    for letter in new_key:\n        proposed_key += letter\n    return proposed_key\n\n\n\n\nCode\n### MAIN PROGRAM\n\n#get letter transition counts for reference text\ntrans_counts = get_trans_counts('wp_full.txt')  # war and peace\n\ntarget_text = \"to bat—rb. con todo mi respeto. i was sitting down playing chess with \\\ndanny de emf and boxer de el centro was sitting next to us. boxer was \\\nmaking loud and loud voices so i tell him por favor can you kick back \\\nhomie cause im playing chess. a minute later the vato starts back up again \\\nso this time i tell him con respecto homie can you kick back. the vato \\\nstop for a minute and he starts up again so i tell him check this out shut \\\nthe fuck up cause im tired of your voice and if you got a problem with it \\\nwe can go to celda and handle it. i really felt disrespected thats why i \\\ntold him. anyways after i tell him that the next thing I know that vato \\\nslashes me and leaves. by the time i figure im hit i try to get away but \\\nthe c.o. is walking in my direction and he gets me right by a celda. so i \\\ngo to the hole. when im in the hole my home boys hit boxer so now b is \\\nalso in the hole. while im in the hole im getting schoold wrong. \" \n\nalphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n\n#generate a random encryption key\nchars = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\nencrypt_list = list(np.random.choice(chars, size=26, replace=False))\nencrypt_key = \"\"\nfor i in encrypt_list:\n    encrypt_key = encrypt_key + i\nencrypt_test =\"abcdefghijklmnopqrstuvwxyz\"\ntest_key = apply_key(encrypt_key, encrypt_test)\n\n#encrypt the target text\nencoded_text = apply_key(encrypt_key, target_text)\n\n#generate a random decryption key\ndecrypt_list = list(np.random.choice(chars, size=26, replace=False))\ncurrent_decrypt_key = \"\"\nfor i in decrypt_list:\n    current_decrypt_key = current_decrypt_key + i\n\n#Results!\n\nprint('Unencrypted text:\\n', target_text)\nprint('\\nEncrypted text:\\n', encoded_text)\nprint('\\n')\n\nfor iters in range(20001):\n    proposed_decrypt_key = get_proposed_key(current_decrypt_key)\n    current_score = score(current_decrypt_key, encoded_text, trans_counts)\n    proposed_score = score(proposed_decrypt_key, encoded_text, trans_counts)\n\n    #calculate the acceptance probability based on the ratio of the proposed and current scores\n    ap = min(1, math.exp(proposed_score - current_score))\n\n    #generate a random number between 0 and 1\n    runif = np.random.uniform()\n\n    #accept the proposed key only if the random number is less than the probability of acceptance\n    if runif >= ap: accept_proposed_key = False\n    else: accept_proposed_key = True\n    if accept_proposed_key: current_decrypt_key = proposed_decrypt_key\n\n    #print every 1000th iteration\n    if iters%500 == 0:\n        print('Iter:', iters, apply_key(current_decrypt_key, encoded_text)[0:99])\n        test = apply_key(encrypt_key, alphabet)\n        check = apply_key(current_decrypt_key, test)\n        correct = 0\n        for i in range(len(alphabet)):\n            if alphabet[i] == check[i]: correct += 1\n        #print('Number of correctly decoded letters:', correct)\n\nprint('\\nDecrypted text:')\nprint(apply_key(current_decrypt_key, encoded_text))\n\n#get number of correct letters\ntest = apply_key(encrypt_key, alphabet)\ncheck = apply_key(current_decrypt_key, test)\ncorrect = 0\nfor i in range(len(alphabet)):\n    if alphabet[i] == check[i]: correct += 1\nprint('\\n')\n#print(alphabet)\n#print(check)\nprint('Number of correctly decoded letters:', correct)\n\n\nUnencrypted text:\n to bat—rb. con todo mi respeto. i was sitting down playing chess with danny de emf and boxer de el centro was sitting next to us. boxer was making loud and loud voices so i tell him por favor can you kick back homie cause im playing chess. a minute later the vato starts back up again so this time i tell him con respecto homie can you kick back. the vato stop for a minute and he starts up again so i tell him check this out shut the fuck up cause im tired of your voice and if you got a problem with it we can go to celda and handle it. i really felt disrespected thats why i told him. anyways after i tell him that the next thing I know that vato slashes me and leaves. by the time i figure im hit i try to get away but the c.o. is walking in my direction and he gets me right by a celda. so i go to the hole. when im in the hole my home boys hit boxer so now b is also in the hole. while im in the hole im getting schoold wrong. \n\nEncrypted text:\n IV ACI MA  TVJ IVYV RU MLDXLIV  U PCD DUIIUJS YVPJ XZCHUJS TOLDD PUIO YCJJH YL LRB CJY AVELM YL LZ TLJIMV PCD DUIIUJS JLEI IV QD  AVELM PCD RCNUJS ZVQY CJY ZVQY FVUTLD DV U ILZZ OUR XVM BCFVM TCJ HVQ NUTN ACTN OVRUL TCQDL UR XZCHUJS TOLDD  C RUJQIL ZCILM IOL FCIV DICMID ACTN QX CSCUJ DV IOUD IURL U ILZZ OUR TVJ MLDXLTIV OVRUL TCJ HVQ NUTN ACTN  IOL FCIV DIVX BVM C RUJQIL CJY OL DICMID QX CSCUJ DV U ILZZ OUR TOLTN IOUD VQI DOQI IOL BQTN QX TCQDL UR IUMLY VB HVQM FVUTL CJY UB HVQ SVI C XMVAZLR PUIO UI PL TCJ SV IV TLZYC CJY OCJYZL UI  U MLCZZH BLZI YUDMLDXLTILY IOCID POH U IVZY OUR  CJHPCHD CBILM U ILZZ OUR IOCI IOL JLEI IOUJS U NJVP IOCI FCIV DZCDOLD RL CJY ZLCFLD  AH IOL IURL U BUSQML UR OUI U IMH IV SLI CPCH AQI IOL T V  UD PCZNUJS UJ RH YUMLTIUVJ CJY OL SLID RL MUSOI AH C TLZYC  DV U SV IV IOL OVZL  POLJ UR UJ IOL OVZL RH OVRL AVHD OUI AVELM DV JVP A UD CZDV UJ IOL OVZL  POUZL UR UJ IOL OVZL UR SLIIUJS DTOVVZY PMVJS  \n\n\nIter: 0 ND CBN GC  KDY NDED LM GXJSXND  M OBJ JMNNMYH EDOY SPBFMYH KAXJJ OMNA EBYYF EX XLR BYE CDTXG EX XP \n\n\nIter: 500 LI WUL MW  HIR LIDI FO MESVELI  O CUS SOLLORG DICR VNUKORG HTESS COLT DURRK DE EFP URD WIZEM DE EN \n\n\nIter: 1000 TA WUT SW  PAN TADA FO SELVETA  O CUL LOTTONG DACN VRUMONG PHELL COTH DUNNM DE EFK UND WAXES DE ER \n\n\nIter: 1500 TO WUT SW  CON TODO FA SELVETO  A PUL LATTANG DOPN VRUMANG CHELL PATH DUNNM DE EFK UND WOXES DE ER \n\n\nIter: 2000 TO WIT MW  CON TODO RA MESPETO  A KIS SATTANG DOKN PLIFANG CHESS KATH DINNF DE ERB IND WOXEM DE EL \n\n\nIter: 2500 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAKING CHESS WITH DANNK DE EMF AND BOXER DE EL \n\n\nIter: 3000 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 3500 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 4000 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 4500 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 5000 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 5500 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 6000 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 6500 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 7000 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 7500 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 8000 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 8500 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 9000 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 9500 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 10000 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 10500 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 11000 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 11500 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 12000 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 12500 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 13000 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 13500 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 14000 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 14500 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 15000 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 15500 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 16000 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 16500 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 17000 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 17500 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 18000 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 18500 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 19000 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 19500 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\n\nIter: 20000 TO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL \n\nDecrypted text:\nTO BAT RB  CON TODO MI RESPETO  I WAS SITTING DOWN PLAYING CHESS WITH DANNY DE EMF AND BOXER DE EL CENTRO WAS SITTING NEXT TO US  BOXER WAS MAKING LOUD AND LOUD VOICES SO I TELL HIM POR FAVOR CAN YOU KICK BACK HOMIE CAUSE IM PLAYING CHESS  A MINUTE LATER THE VATO STARTS BACK UP AGAIN SO THIS TIME I TELL HIM CON RESPECTO HOMIE CAN YOU KICK BACK  THE VATO STOP FOR A MINUTE AND HE STARTS UP AGAIN SO I TELL HIM CHECK THIS OUT SHUT THE FUCK UP CAUSE IM TIRED OF YOUR VOICE AND IF YOU GOT A PROBLEM WITH IT WE CAN GO TO CELDA AND HANDLE IT  I REALLY FELT DISRESPECTED THATS WHY I TOLD HIM  ANYWAYS AFTER I TELL HIM THAT THE NEXT THING I KNOW THAT VATO SLASHES ME AND LEAVES  BY THE TIME I FIGURE IM HIT I TRY TO GET AWAY BUT THE C O  IS WALKING IN MY DIRECTION AND HE GETS ME RIGHT BY A CELDA  SO I GO TO THE HOLE  WHEN IM IN THE HOLE MY HOME BOYS HIT BOXER SO NOW B IS ALSO IN THE HOLE  WHILE IM IN THE HOLE IM GETTING SCHOOLD WRONG  \n\n\nNumber of correctly decoded letters: 23\n\n\nNot bad! I think it’s fascinating that the algorithm was also successful on text that contained slang, mis-spellings, and occasional Spanish as in the above example."
  },
  {
    "objectID": "posts/dqn_ttt/index.html",
    "href": "posts/dqn_ttt/index.html",
    "title": "Deep Q-Networks",
    "section": "",
    "text": "In a previous post on Q-learning, I demonstrated how to train an agent to play tic-tac-toe. One of the challenges with Q-learning is that it doesn’t scale well to environments with a large state space. Even after experiencing 200,000 tic-tac-toe games, the agent had visited only a fraction of the total state space. After training and when playing against an opponent, if the agent encountered a board that it didn’t experience in training, the Q-values for all available moves were 0, and so the agent played randomly. This becomes even more problematic if we try to apply Q-learning to a game like chess or Go. We need a method that doesn’t rely on a training agent visiting every state. Neural networks have been very successful in solving this problem. DeepMind, for example, has not only mastered 49 different Atari games, but has also defeated the raining Go champion. I’m not that ambitious. I’ll just stick with tic-tac-toe. The most basic application of neural networks to this problem is a deep Q-Network, which I’ll demonstrate here."
  },
  {
    "objectID": "posts/dqn_ttt/index.html#tic-tac-toe-environment",
    "href": "posts/dqn_ttt/index.html#tic-tac-toe-environment",
    "title": "Deep Q-Networks",
    "section": "Tic-Tac-Toe Environment",
    "text": "Tic-Tac-Toe Environment\nAs with Q-learning, I needed to define an environment that contains the game fundamentals. I started with the same environment I set up for Q-learning and then stripped out the Q-learning algorithm so I just have the basics of the game. The env class below does just three things: 1. It resets the game board to begin a new game. I define the board state as a list with nine elements to represent each of the nine game board positions. A 0 represents an empty position, 1 is an X, and -1 is an O. 2. In the step function, it makes a move for either player X or O. 3. In the game_over function, it checks the state of the board and returns a reward. The reward system is the same as before. An X win gets a reward of 1, an O win gets a reward of -1, and everything else gets a reward of 0.\n\n\nCode\nimport tensorflow as tf\nimport numpy as np\nimport copy\nimport random\n\n\n\n\nCode\nclass env:\n\n    def __init__(self):\n        self.state = self.reset()\n\n    def reset(self):\n        return [0 for i in range(9)]\n\n    def game_over(self, s):\n        done = False\n        reward = 0\n        if (s[0] + s[1] + s[3]  == 3 or s[3] + s[4] + s[5]  == 3 or s[6] + s[7] + s[8]  == 3 or\n            s[0] + s[3] + s[6]  == 3 or s[1] + s[4] + s[7]  == 3 or s[2] + s[5] + s[8]  == 3 or\n            s[0] + s[4] + s[8]  == 3 or s[2] + s[4] + s[6]  == 3):\n            done = True\n            reward = 1\n        if (s[0] + s[1] + s[3]  == -3 or s[3] + s[4] + s[5]  == -3 or s[6] + s[7] + s[8]  == -3 or\n            s[0] + s[3] + s[6]  == -3 or s[1] + s[4] + s[7]  == -3 or s[2] + s[5] + s[8]  == -3 or\n            s[0] + s[4] + s[8]  == -3 or s[2] + s[4] + s[6]  == -3):\n            done = True\n            reward = -1\n        if sum(1 for i in s if i != 0)==9 and not done:\n            done = True\n        return done, reward\n\n    def step(self, state, action, player):\n        next_state = state.copy()\n        if player == 0: next_state[action] = 1\n        else: next_state[action] = -1\n        done, reward = self.game_over(next_state)\n        return next_state, done, reward"
  },
  {
    "objectID": "posts/dqn_ttt/index.html#deep-q-learning",
    "href": "posts/dqn_ttt/index.html#deep-q-learning",
    "title": "Deep Q-Networks",
    "section": "Deep Q-Learning",
    "text": "Deep Q-Learning\nNext, I create a class DQNagent where the real work is done. I’ll go through the three main functions in detail.\nbuild_model Function. This is where I create the neural network itself. It’s a simple sequential model with two hidden layers that I built some flexibility into. The input shape, the number of nodes, and the output size are all variables because I intend to apply this basic set-up to other problems. In the tic-tac-toe problem, the neural network takes the board state as input, so in this case the input_shape is nine. The two hidden layers have 81 nodes each with the relu activation function. The output also has a length of nine that are the Q-values for each of the nine actions (i.e., moves on the game board). In other words, the network takes the game board as input, and provides the best move as output.\nplay_ttt Function. It might make more sense to explain this function next. Basically, there’s an inner loop to play one game of tic-tac-toe, and an outer loop to play the game multiple times. Most of the function just controls the mechanics of the game, but there are two aspects I want to point out. 1. The function uses an epsilon-greedy policy to determine whether to make a move based on the neural network Q-values or make a random move. The probability of playing randomly is initially high during training to encourage the algorithm to explore the state space. As training progresses, player X moves are more and more likely to be based on the neural network. Note that the neural network is set up to learn the game from player X’s perspective. Player O always plays randomly. 2. During the course of playing one game, the game board states, player moves (actions), and rewards are recorded. These are passed to the train_model function at the end of a game.\ntrain_model Function. This is where the neural network gets trained and things get complicated. I’m going to let Aurélien Géron do the ’splainin from his book Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow.\n\nConsider the approximate Q-Value computed by the DQN for a given state-action pair (s, a). Thanks to Bellman, we know we want this approximate Q-Value to be as close as possible to the reward r that we actually observe after playing action a in state s, plus the discounted value of playing optimally from then on. To estimate this sum of future discounted rewards, we can simply execute the DQN on the next state s′ and for all possible actions a′. We get an approximate future Q-Value for each possible action. We then pick the highest (since we assume we will be playing optimally) and discount it, and this gives us an estimate of the sum of future discounted rewards. By summing the reward r and the future discounted value estimate, we get a target Q-Value y(s, a) for the state-action pair (s, a), as shown in the equation:\n\n\\[\nQ_{target}(s,a)=r\\gamma\\cdot \\underset{a'}{max}Q_{\\theta}\\left(s', a'\\right)\n\\]\n\nWith this target Q-Value, we can run a training step using any Gradient Descent algorithm. Specifically, we generally try to minimize the squared error between the estimated Q-Value Q(s, a) and the target Q-Value (or the Huber loss to reduce the algorithm’s sensitivity to large errors). And that’s all for the basic Deep Q-Learning algorithm!\n\nRight. So that’s what happening in the train_model function.\n\n\nCode\nclass DQNagent:\n\n    def __init__(self, state_size, action_size, iterations):\n        self.alpha0 = 0.05           # learning rate\n        self.decay = 0.005           # learning rate decay\n        self.gamma = 0.95            # discount factor\n        self.state_size = state_size\n        self.action_size = action_size\n        self.iterations = iterations\n        self.model = self.build_model()\n        self.optimizer = tf.keras.optimizers.SGD(learning_rate=0.02)\n        self.loss_fn = tf.keras.losses.mean_squared_error\n\n    def build_model(self):\n        model = tf.keras.models.Sequential([\n            tf.keras.layers.Dense(self.state_size**2, activation=\"relu\", input_shape=[self.state_size]),\n            tf.keras.layers.Dense(self.state_size**2, activation=\"relu\"),\n            tf.keras.layers.Dense(self.action_size)\n        ])\n        return model\n\n    def train_model(self, state_history, action_history, next_state_history, rewards, dones):\n        next_Q_values = self.model.predict(np.array(next_state_history), verbose=0)\n        max_next_Q_values = np.max(next_Q_values, axis=1)\n        target_Q_values = rewards + (1 - 1*np.array(dones)) * self.gamma * max_next_Q_values\n        target_Q_values = tf.reshape(target_Q_values, [len(rewards), 1])\n        mask = tf.one_hot(action_history, 9)\n        with tf.GradientTape() as tape:\n            all_Q_values = self.model(np.array(state_history))\n            Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n            loss = tf.reduce_mean(self.loss_fn(target_Q_values, Q_values))\n        grads = tape.gradient(loss, self.model.trainable_variables)\n        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n\n    def play_ttt(self):\n        for iteration in range(self.iterations):    # outer loop to play the game a bunch of times\n            state = env().reset()\n            next_state = state.copy()\n            done = False\n            dones = []\n            state_history = []\n            state_history.append(state)\n            action_history = []\n            rewards = []\n            epsilon = max(1 - iteration/(self.iterations*0.8), 0.01)\n            while not done:                          # inner loop to play one game\n                if random.random() < epsilon:        # epsilon-greedy policy\n                    action = random.choice([i for i in range(len(state)) if state[i] == 0])\n                else:\n                    action = np.argmax(self.model.predict(np.array(state)[np.newaxis], verbose=0)[0])\n                action_history.append(action)\n                next_state, done, reward = env().step(state, action, 0)\n                if done:\n                    state_history.append(next_state)\n                    dones.append(done)\n                    rewards.append(reward)\n                if not done:\n                    omove = random.choice([i for i in range(len(next_state)) if next_state[i] == 0])\n                    next_state, done, reward = env().step(next_state, omove, 1)\n                    state = next_state.copy()\n                    state_history.append(next_state)\n                    dones.append(done)\n                    rewards.append(reward)\n            next_state_history = state_history[1:len(state_history)]\n            state_history = state_history[0:len(action_history)]\n            self.train_model(state_history, action_history, next_state_history, rewards, dones)\n        return self.model"
  },
  {
    "objectID": "posts/dqn_ttt/index.html#training",
    "href": "posts/dqn_ttt/index.html#training",
    "title": "Deep Q-Networks",
    "section": "Training",
    "text": "Training\nHere I just provide the DQNagent with the number of input and output nodes for the neural network (9 each) and how many games to play (1000). Compare the number of games here with the 200,000 games I used for the Q-learning method - a huge difference!"
  },
  {
    "objectID": "posts/dqn_ttt/index.html#results",
    "href": "posts/dqn_ttt/index.html#results",
    "title": "Deep Q-Networks",
    "section": "Results",
    "text": "Results\nI want to see real quick if the neural net has learned to play in the center of the board on the opening move, so I have the model give me the Q-values for an empty board. The fifth number returned represents the center of the board, and it is the highest Q-value, so the neural net has already learned it!\n\n\nCode\nm.predict(np.zeros(9)[np.newaxis], verbose=0)\n\n\narray([[0.5948092 , 0.49544078, 0.51444966, 0.56917584, 0.5867796 ,\n        0.5812461 , 0.89313275, 0.67149806, 0.6291183 ]], dtype=float32)\n\n\nNext, the play_v_random function pits the AI-enabled player against an opponent that plays randomly.\n\n\nCode\ndef play_v_random (games):\n    results = [0 for i in range(games)]\n    for i in range(games):\n        board = env().reset()\n        done = False\n        while not done:\n            #print(board[0:3])\n            #print(board[3:6])\n            #print(board[6:9])\n            xmoves = m.predict(np.array(board)[np.newaxis], verbose=0)[0]\n            xmoves[np.where(np.array(board)!=0)[0]] = -1\n            xmove = np.argmax(xmoves)\n            #print(\"move\", xmove)\n            board[xmove] = 1\n            done, reward = env().game_over(board)\n            if not done:\n                omove = random.choice([i for i in range(len(board)) if board[i] == 0])\n                board[omove] = -1\n                done, reward = env().game_over(board)\n        #print(board[0:3])\n        #print(board[3:6])\n        #print(board[6:9])\n        results[i] = reward\n    return results\n\n\nSo here we have it. The AI-enabled player won 90% of the games, lost 8%, and 2% were ties. For comparison, the best performance using Q-learning was 81.2% AI wins, 2.3% losses, and 16.5% ties. The deep Q-network performed significantly better after being trained on 1,000 games than Q-learning did after being trained on 200,000 games. Remarkable!\n\n\nCode\nsum(1 for i in results if i == 1), sum(1 for i in results if i == -1), sum(1 for i in results if i == 0)\n\n\n(822, 122, 56)\n\n\nAs I mentioned at the beginning of the post, a deep Q-network is the most basic application of neural networks to reinforcement learning. More advanced applications include double deep Q-networks and dueling deep Q networks, and each of these can be enhanced with techniques like actor-critic algorithms, curiosity-based exploration, proximal policy optimization, and so on. There’s a lot left to explore."
  },
  {
    "objectID": "posts/knn_art/index.html",
    "href": "posts/knn_art/index.html",
    "title": "KNN Art",
    "section": "",
    "text": "The generative art journey continues with images produced from machine learning models. In this post, I’ll use k-nearest neighbors to produce two types of art: one that sometimes resembles gemstones, and another that resembles a mosaic."
  },
  {
    "objectID": "posts/knn_art/index.html#generating-the-data",
    "href": "posts/knn_art/index.html#generating-the-data",
    "title": "KNN Art",
    "section": "Generating the Data",
    "text": "Generating the Data\nThe k-nearest neighbors (KNN) algorithm has been around since the 1950’s and can be used for both classification and regression tasks. Describing the algorithm is beyond the scope of this post, and a simple Google search will produce many detailed explanations. Besides, for art purposes, I’m really not interested in the underlying math, the model accuracy, or any of that. I just need any model that makes predictions about my response variable z based on my predictors x and y.\nThere are two model hyperparameters that effect the art: k (the k from k-nearest neighbors) and the type of kernel. It actually doesn’t even really matter what those are and how they impact the model. I will just include them as something to experiment with when generating KNN art.\nI mentioned there are two types of art - gemstones and mosaics. The only change needed to produce one type versus the other is by specifying a continuous response variable for gemstones and a categorical response variable for mosaics."
  },
  {
    "objectID": "posts/knn_art/index.html#gemstones",
    "href": "posts/knn_art/index.html#gemstones",
    "title": "KNN Art",
    "section": "Gemstones",
    "text": "Gemstones\nIt’s surprisingly simple to generate the data for either type of art. I first create a train data set that I’ll use to train the model. It consists of 100 random values for the x and y predictor variables and 100 random, continuous values for the response variable z. The test data set is a regular grid of x and y values from 1 to 500. The model will make predictions for each of these 500 x 500 = 250,000 observations that will be visualized in the art.\nWith these two data sets, I fit the KNN model with the two hyperparameters I mentioned. All that’s relevant is that k can be any integer greater than 0, and the kernel can be “rectangular”, “triangular”, “epanechnikov”, “biweight”, “triweight”, “cos”, “inv”, “gaussian”, “rank”, or “optimal”. I’ll start with k = 3 and a triangular kernel.\n\nlibrary(kknn)\nset.seed(1)\n\ntrain <- data.frame(x=runif(100, 0, 500), y=runif(100, 0, 500), z=runif(100))\ntest <- expand.grid(x=1:500, y=1:500)\n\nknn <- kknn(z~., train, test, k=3, kernel=\"triangular\")\n\nNow that the model is fit, I just need to extract the predictions (aka, the “fitted” values). Since the predictions are a grid of x and y values from 1 to 500, I can just think of them as an image with 500x500 pixels, which I’ll visualize with geom_tile() from ggplot2. I’ll also use color palettes from RColorBrewer.\n\nlibrary(ggplot2)\nlibrary(RColorBrewer)\n\ntest$z <- fitted(knn) # get the predictions\n\nggplot(test) +\n  geom_tile(aes(x=x, y=y, fill=z)) +\n  scale_fill_distiller(palette = \"Set1\") +\n  coord_fixed() +\n  theme_void() +\n  theme(legend.position=\"none\") \n\n\n\n\nDoes that look gemstone-ish? The color palette has a fair amount to do with it. Choosing a brown-blue-green palette produces a turquoise-looking image, I think.\n\nggplot(test) +\n  geom_tile(aes(x=x, y=y, fill=fitted(knn))) +\n  scale_fill_distiller(palette = \"BrBG\") +\n  coord_fixed() +\n  theme_void() +\n  theme(legend.position=\"none\") \n\n\n\n\nNow I’ll try different values for k, some different kernel choices, and a different color palette. A convenient way of dealing with this kind of situation in which k is numeric and the kernel choices are strings is to combine all combinations of the two with the cross2() function from purrr. I then pipe that into the future_map2() function from the furrr package to run the processes in parallel and create a list of plots. The cowplot package is useful for arranging non-faceted plots in a grid and can accept the list of plots with the plotlist argument.\n\nlibrary(furrr)\n\nLoading required package: future\n\nplan(multisession, workers = 4)\n\nlibrary(magrittr)\n\nx <- c(3, 9, 18)\ny <- c(\"rectangular\", \"triangular\", \"gaussian\")\n\nplots <- \n  purrr::cross2(x, y) %>%\n  furrr::future_map(function(l){\n    knn <- kknn(z~., train, test, k=l[[1]], kernel=l[[2]])\n    \n    ggplot(test) +\n      geom_tile(aes(x=x, y=y, fill=fitted(knn))) +\n      scale_fill_distiller(palette = \"PuOr\") +\n      coord_fixed() +\n      theme_void() +\n      ggtitle(paste(\"k =\", l[[1]], \"kernel =\", l[[2]])) +\n      theme(legend.position=\"none\", plot.title = element_text(size=10))\n  })\n\ncowplot::plot_grid(plotlist = plots, ncol=3)\n\n\n\n\nGenerally speaking, a higher k seems to produce a more gradual transition between colors, and the choice of kernel also has a profound impact. Personally, I find that a low k with a triangular kernel tends to produce more gemstone-like images, but it ultimately comes down to the look you’re going for."
  },
  {
    "objectID": "posts/knn_art/index.html#mosaics",
    "href": "posts/knn_art/index.html#mosaics",
    "title": "KNN Art",
    "section": "Mosaics",
    "text": "Mosaics\nI just need one simple change for mosaic-looking art: specify a categorical response variable. I’ll create a six-level categorical z variable in the train data that I’ll randomly sample. Except for the use of scale_fill_brewer() and a different palette, the rest of the code is identical to the above.\n\ntrain <- data.frame(\n  x = runif(100, 0, 500), \n  y = runif(100, 0, 500), \n  z = factor(sample(1:6, 100, replace=TRUE))\n  )\n\nknn <- kknn(z~., train, test, k=1, kernel=\"triangular\")\n\nggplot(test) +\n  geom_tile(aes(x=x, y=y, fill=fitted(knn))) +\n  scale_fill_brewer(palette = \"Set1\") +\n  coord_fixed() +\n  theme_void() +\n  theme(legend.position=\"none\") \n\n\n\n\nThrough experimenting, I found that more training observations result in smaller mosaic “tiles”, which makes sense when you plot the training data observations. More dots produce more tiles.\n\ntrain <- data.frame(\n  x=runif(200, 0, 500), \n  y=runif(200, 0, 500), \n  z=factor(sample(1:6, 200, replace=TRUE))\n  )\n\nknn <- kknn(z~., train, test, k=1, kernel=\"triangular\")\n\nggplot(test) +\n  geom_tile(aes(x=x, y=y, fill=fitted(knn))) +\n  geom_point(data=train, aes(x=x, y=y)) +\n  scale_fill_brewer(palette = \"Set1\") +\n  coord_fixed() +\n  theme_void() +\n  theme(legend.position=\"none\") \n\n\n\n\nIn the above examples, I used a k of 1, which produces nice straight mosaic boundaries (because it’s only trained on the k=1 nearest neighbor). A higher value of k results in rounded and sometimes jagged boundaries, which isn’t very mosaic-y.\n\nknn <- kknn(z~., train, test, k=7, kernel=\"triangular\")\n\nggplot(test) +\n  geom_tile(aes(x=x, y=y, fill=fitted(knn))) +\n  scale_fill_brewer(palette = \"Set1\") +\n  coord_fixed() +\n  theme_void() +\n  theme(legend.position=\"none\") \n\n\n\n\nYou can click here for my Shiny app to generate your own art using this and other algorithms. Enjoy!"
  },
  {
    "objectID": "posts/maze/index.html",
    "href": "posts/maze/index.html",
    "title": "Maze Generator",
    "section": "",
    "text": "This generative art seems less artsy to me, but the idea of generating a maze was interesting. I found the mazegenerator GitHub repo that described the algorithm simply as a random walk and displayed a .gif of the maze being incrementally created. It’s straight forward enough that that bit of information and a visual is all you need to figure it out."
  },
  {
    "objectID": "posts/maze/index.html#generate-the-data",
    "href": "posts/maze/index.html#generate-the-data",
    "title": "Maze Generator",
    "section": "Generate the Data",
    "text": "Generate the Data\nThe way this algorithm works is that you choose the dimensions of the maze and a random starting spot for the maze path to begin. I started with a 20x20 grid, which I represented as a 20x20 matrix of 0s (m) and a starting point of row 1, column 2 (r_start and c_start below). When the maze path is “drawn” on a grid coordinate, I change the corresponding matrix value of 0 to a 1. This is how I keep track of where in the grid I have and have not visited. For plotting purposes, I know I’ll have to keep track of each path segment, so I create a counter s_start.\n\n# crate the maze matrix\nmaze_size <- 20\nm <- matrix(0, nrow=maze_size, ncol=maze_size)\n\n# choose a starting position\nr_start <- 1; c_start <- 2\n# segment counter\ns_count <- 1\n\nm[1, 2] <- 1 # change starting position value\n\nset.seed(2)\n\nFrom the starting point, I’m going to randomly pick a direction to move in. Valid moves are either up, down, left, or right - no diagonal moves are allowed in a maze. At each step, I know I’ll need to check what are the valid next moves. The get_valid_moves() function returns a valid vector of length 4. I initialize the vector with all FALSE values. If a move up is valid (doesn’t take me off the board or to a position that’s already been visited), I change the first index to TRUE. Same thing with down, left, and right which are stored in the 2nd, 3rd, and 4th index positions.\nNext is the get_segment() function. I keep track of all of the row and column pairs visited with vectors ro and co. For convenience, the idx variable just keeps track of the length of these vectors, so I can save some typing. After getting length, I check for valid moves from that position. If there aren’t any, the get_valid_moves() function returns a vector of all FALSE values, and that’s the criteria to break out of the repeat loop. Otherwise, I randomly sample from the available valid moves using the index of the TRUE values in the valid_moves vector. For example, if up and left are the only valid moves, valid_moves will contain (TRUE, FALSE, TRUE, FALSE), and so I randomly sample either index 1 or 3.\nI discovered that the sample() function doesn’t behave as I expected if there’s only one TRUE index. The help for sample() confirms this:\n\n\n\n\n\n\nNote\n\n\n\nIf x has length 1, is numeric (in the sense of is.numeric) and x >= 1, sampling via sample takes place from 1:x. Note that this convenience feature may lead to undesired behaviour when x is of varying length in calls such as sample(x).\n\n\nIn other words if valid_moves is (FALSE, FALSE, FALSE, TRUE), then which(valid_moves) = 4 and then sample(4) will sample the vector 1:4 instead of just returning 4. Not what I want, so that’s the reason for the ifelse() statement. The rest of the function just adds the new row and column values to the ro and co vectors and updates the maze matrix value for the corresponding new move. Once the segment gets stuck somewhere because a lack of a valid move, the ro and co vectors and the segment number are put in a dataframe and returned.\n\nget_valid_moves <- function(rw, cl, maze){\n    valid <- rep(FALSE, 4) #up, down, left, right\n    if (rw > 1) if(m[rw-1, cl] != 1) valid[1] <- TRUE     # up\n    if (rw < maze) if(m[rw+1, cl] != 1) valid[2] <- TRUE  # down\n    if (cl > 1) if(m[rw, cl-1] != 1) valid[3] <- TRUE     # left\n    if (cl < maze) if(m[rw, cl+1] != 1) valid[4] <- TRUE  # right\n    valid\n}\n\nget_segment <- function(ro, co, sg, maze){\n  \n  repeat{\n    idx <- length(ro)\n    # check for valid moves\n    valid_moves <- get_valid_moves(ro[idx], co[idx], maze)\n    \n    if (sum(valid_moves) == 0) break # end of the sequence\n    \n    # note: if only one valid move, sample misbehaves\n    move <- ifelse(sum(valid_moves) == 1, which(valid_moves), sample(which(valid_moves), 1))\n    \n    if (move == 1){\n      ro <- c(ro, ro[idx] - 1)\n      co <- c(co, co[idx])}\n    if (move == 2){\n      ro <- c(ro, ro[idx] + 1)\n      co <- c(co, co[idx])}\n    if (move == 3){\n      ro <- c(ro, ro[idx])\n      co <- c(co, co[idx] - 1)}\n    if (move == 4){\n      ro <- c(ro, ro[idx])\n      co <- c(co, co[idx] + 1)}\n    \n    m[ro[idx+1], co[idx+1]] <<- 1\n  }\n  \n  return(data.frame(x=co, y=ro, seg=sg))\n}\n\ndf <- get_segment(r_start, c_start, s_count, maze_size)\n\nhead(df)\n\n\n\n  \n\n\n\nAt this point I have a matrix with one maze segment that got stuck somewhere. Let’s visualize it to make sure nothing’s wonky.\n\nlibrary(ggplot2)\ntheme_set(theme_bw())\n\nggplot(df) +\n  geom_path(aes(x=x, y=y, group=seg), size=5, linejoin = \"mitre\", lineend = \"square\") +\n  coord_fixed() +\n  xlim(1, maze_size) + ylim(1, maze_size)\n\n\n\n\nRecall the segment started at row 1 and column 2, so that translates into plotting at x=2, y=1 in the lower left. Following the segment from there, we see that it wandered around, stayed in bounds (yay!), and eventually trapped itself.\nThat seemed to work as expected, so now what? Now I need to pick a starting point for the next path segment. After watching the .gif I mentioned, I noticed that the next segment always started somewhere along the black line where there is a valid adjacent move. Seems like I ought to be able to pick a spot randomly to start from - again, as long as there’s an adjacent valid move.\nThe while loop below just needs a few lines to finish the maze. I get valid next starting positions by finding which indices of the m matrix are 1, and then I check which of those indices have a valid adjacent move. I randomly sample from these and pass the row and column number to the get_segment() function. Once all matrix values are 1, the loop stops.\n\nlibrary(purrr)\nlibrary(dplyr)\n\nwhile (sum(m) < maze_size^2){\n  # get the indices of 1 values from the m matrix\n  ones <- data.frame(which(m==1, arr.ind=TRUE)) \n  # get the indices that have an open adjacent position\n  ones$open <- map2(ones[, 1], ones[, 2], ~sum(get_valid_moves(.x, .y, maze_size))>0) %>% unlist()\n  # filter out the blocked positions\n  ones <- ones[ones$open, ]\n  # get a random new starting position\n  if (nrow(ones) > 1) ones <- ones[sample(nrow(ones), 1), ]\n  \n  s_count <- s_count + 1\n  df <- bind_rows(df, get_segment(ones$row, ones$col, s_count, maze_size))\n}"
  },
  {
    "objectID": "posts/maze/index.html#visualizations",
    "href": "posts/maze/index.html#visualizations",
    "title": "Maze Generator",
    "section": "Visualizations",
    "text": "Visualizations\nHere’s the complete maze. I switched color schemes to be more like what you’d see on a piece of paper - a maze where you follow the white paths. Looks great, I think!\n\nggplot(df) +\n  geom_path(aes(x=x, y=y, group=seg), \n            color = \"white\", size=5, linejoin = \"mitre\", lineend = \"square\") +\n  coord_fixed() +\n  theme_void() +\n  theme(panel.background = element_rect(fill = 'black', color = 'black'))\n\n\n\n\nBefore I continue, this is a good spot to recreate the .gif I saw. I’ll use the gganimate package to display the maze as it’s generated.\n\nlibrary(gganimate)\n\nanim <- \n  df %>% mutate(frame = row_number()) %>%\n  ggplot() +\n  geom_path(aes(x=x, y=y, group=seg), \n            color = \"white\", size=2, linejoin = \"mitre\", lineend = \"square\") +\n  coord_fixed() +\n  theme_void() +\n  theme(panel.background = element_rect(fill = 'black', color = 'black')) +\n  transition_reveal(frame)\n\nanimate(anim, end_pause=20, res=300)\n\n\nHow about a bigger maze? Let’s try 50x50.\n\nmaze_size <- 50\nm <- matrix(0, nrow=maze_size, ncol=maze_size)\nr_start <- 1; c_start <- 2; s_count <- 1\nm[1, 2] <- 1\ndf <- get_segment(r_start, c_start, s_count, maze_size)\n\nwhile (sum(m) < maze_size^2){\n  ones <- data.frame(which(m==1, arr.ind=TRUE)) \n  ones$open <- map2(ones[, 1], ones[, 2], ~sum(get_valid_moves(.x, .y, maze_size))>0) %>% unlist()\n  ones <- ones[ones$open, ]\n  if (nrow(ones) > 1) ones <- ones[sample(nrow(ones), 1), ]\n  s_count <- s_count + 1\n  df <- bind_rows(df, get_segment(ones$row, ones$col, s_count, maze_size))\n}\n\nggplot(df) +\n  geom_path(aes(x=x, y=y, group=seg), \n            color = \"white\", size=2, linejoin = \"mitre\", lineend = \"square\") +\n  coord_fixed() +\n  theme_void() +\n  theme(panel.background = element_rect(fill = 'black', color = 'black'))\n\n\n\n\nThe aRtsy package shows a maze plotted in polar coordinates, which is a cool effect. I noticed that the author made the “spokes” of the maze increase in size as they extended toward the perimeter. I’ll try to recreate that here.\n\nlibrary(tidyr)\n\ndf <- df %>% \n  mutate(s = ifelse(y != lead(y), y/20, 0.5))\n\nggplot(df) +\n  geom_path(aes(x=x, y=y, group=seg), \n            size = df$s, \n            color = \"black\", linejoin = \"mitre\", lineend = \"square\") +\n  coord_polar() +\n  theme_void() +\n  theme(panel.background = element_rect(fill = 'white', color = 'white'))\n\n\n\n\nYou can click here for my Shiny app to generate your own art using this and other algorithms. Enjoy!"
  },
  {
    "objectID": "posts/predict_football/index.html",
    "href": "posts/predict_football/index.html",
    "title": "Predictive Modeling",
    "section": "",
    "text": "Continuing with the streak of posts about college football posts, here I finally get into developing some models to predict the winning team and margin of victory of college football games. For this analysis, I was inspired by this post that mentioned a leader board for people who picked winners from the 2021-2022 season. Reminds me of Kaggle competitions, which I did once for fun and wrote about it.\nI also found this published paper both extremely interesting and helpful because it lays out in detail the author’s analytic methodology for building their predictive model. The first part of this post is my attempt at replicating their technique using a different data source (collegefootball.com’s API) and using data from the 2017-2020 seasons to predict games of the 2021 season. Spoiler, my initial tree-based models don’t perform nearly as well as theirs, and I eventually try a neural network with much better success."
  },
  {
    "objectID": "posts/predict_football/index.html#get-the-data",
    "href": "posts/predict_football/index.html#get-the-data",
    "title": "Predictive Modeling",
    "section": "Get the Data",
    "text": "Get the Data\nFirst, I’ll load data I previously obtained via the API I mentioned above. If you want to see how to make those API calls, I explain it in this post. The data in /games/teams has a bunch of game result data that are similar to the data used in the article above.\n\ngs <- readRDS(\"gameStats.RData\")\ngs <- type.convert(gs, as.is = TRUE)\nhead(gs)\n\n\n\n  \n\n\n\nI also want to get the list of all of the FBS teams because I want to filter out the games played against non-FBS teams. There’s a bunch of data in that table, but I’m really only interested in the school column.\n\nfbs <- readRDS(\"fbs_teams.RData\")\nhead(fbs)\n\n\n\n  \n\n\n\nNow I’ll get a vector of all of the game IDs from the first table in which a non-FBS team played. Here are the first three.\n\nlibrary(dplyr) # I'm going to need to do some data wrangling\n\nfcsIDs <- gs %>% filter(!school %in% (fbs %>% .$school)) %>% .$id\nfcsIDs[1:3]\n\n[1] 400944827 400944827 400933835\n\n\nNow I apply the filter and separate out some of the columns. For example, the totalPenaltiesYards column is formatted as penalties-yards, so for example 3-25. I’ll then create a new column that divides one of those values by the other so that I get a single yardsPerPenalty value. I’ll also convert the time of possession into minutes and get rid of columns I no longer need.\n\nlibrary(tidyr) # needed for the separate function below\n\ngs <- \n  gs %>%\n  filter(!id %in% fcsIDs) %>%\n  separate(totalPenaltiesYards, \n           into = c(\"penalties\", \"penaltiesYards\"), convert = TRUE) %>%\n  separate(completionAttempts, \n           into = c(\"passCompletions\", \"passAttempts\"), convert = TRUE) %>%\n  separate(possessionTime, into = c(\"posMM\", \"posSS\"), convert = TRUE) %>%\n  mutate(posTime = posMM * 60 + posSS,\n         yardsPerPenalty = penaltiesYards / penalties,\n         passEfficiency = passCompletions / passAttempts,\n         yardsPerPlay = (yardsPerPass + yardsPerRushAttempt) / 2) %>%\n  select(-totalYards, -posMM, -posSS, -fourthDownEff, -thirdDownEff)\n\nI found that there are a number of NAs in the data. For this demonstration, I’ll just convert them all to 0. If I was cleaning this data for an actual competition or to try to beat Vegas, I’d take a much harder look at those NAs and treat them carefully.\n\ngs[is.na(gs)] <- 0\n\nI also found that there’s a game ID that’s repeated, so I’ll filter out the repeat.\n\n# there's an id that's repeated - there should be two games for each ID\ngs %>% group_by(id) %>% count() %>% filter(n > 2)\n\n\n\n  \n\n\n# take a look at that ID\ngs %>% filter(id == 401309547)\n\n\n\n  \n\n\n# filter out the ID with zeros in the rushingTDs column\ngs <- gs %>% filter(!(id == 401309547 & rushingTDs == 0))\n\nIn the article, they did some feature engineering that I’ll replicate here.\n\ngs <- \n  gs %>% \n  mutate(offensePoints = (rushingTDs + passingTDs) * 7 + kickingPoints,\n         offenseYards = netPassingYards + rushingYards) %>% \n  arrange(id)\n\nThey also created some features for one team based on how the other team performed. Doing this in the original gs dataframe made my head hurt, so I split it into separate dataframes for the home and away teams. While I’m at it, I’ll also calculate MOV, the margin of victory.\n\nhome <- gs %>% filter(homeAway == \"home\")\naway <- gs %>% filter(homeAway == \"away\")\n\nhome <- \n  home %>% \n  mutate(\n    defensePointsAllowed = away$offensePoints,\n    defenseYPPAAllowed = away$yardsPerPass,\n    defenseYPRAAllowed = away$yardsPerRushAttempt,\n    defensePassYardsAllowed = away$netPassingYards,\n    defenseRushYardsAllowed = away$rushingYards,\n    defenseYardsPerPlayAllowed = away$yardsPerPlay,\n    forcedPenaltyYards = away$penaltiesYards,\n    forcedTO = away$turnovers,\n    MOV = points - away$points\n  )\n\naway <- \n  away %>%\n  mutate(\n    defensePointsAllowed = home$offensePoints,\n    defenseYPPAAllowed = home$yardsPerPass,\n    defenseYPRAAllowed = home$yardsPerRushAttempt,\n    defensePassYardsAllowed = home$netPassingYards,\n    defenseRushYardsAllowed = home$rushingYards,\n    defenseYardsPerPlayAllowed = home$yardsPerPlay,\n    forcedPenaltyYards = home$penaltiesYards,\n    forcedTO = home$turnovers,\n    MOV = points - home$points\n  )\n\nI also wanted to get each team’s Elo score,which I found in the /games data via the API. For some reason, I grabbed the pregame Elo score instead of the post-game Elo, but that’s fine. I’ll account for that later. I’ll also filter out those games using the game IDs in the home dataframe. The /games data also contains a boolean neutral_site, which will be useful. There’s always a home and away team in every game (as indicated in the homeAway column) even if the game is played on a neutral site. If I want to correctly factor in whether a team has a home field advantage, I’ll need to account for those games played at a neutral site.\n\neloSite <- readRDS(\"eloSite.RData\") %>% as_tibble()\n\neloSite <- eloSite %>% filter(id %in% (home %>% .$id))\n\nhead(eloSite)\n\n\n\n  \n\n\n\nNow I’ll combine that data with the home and away dataframes.\n\nhome <-\n  home %>% \n  left_join(eloSite %>% \n              select(id, home_team, home_pregame_elo, neutral_site),\n            by = c(\"id\" = \"id\", \"school\" = \"home_team\")) %>%\n  rename(\"elo\" = \"home_pregame_elo\")\n\naway <-\n  away %>% \n  left_join(eloSite %>% \n              select(id, away_team, away_pregame_elo, neutral_site),\n            by = c(\"id\" = \"id\", \"school\" = \"away_team\")) %>%\n  rename(\"elo\" = \"away_pregame_elo\")\n\nThe authors explained that their methodology didn’t use raw game results. Instead, they used seasonal cumulative means. Let’s take rushingTDs for an example. Say a given team in week 1 had 1 rushing TD. Week 1 results are unchanged. If they scored 2 rushing TDs in week 2, then the cumulative mean becomes 1.5 and so on through the weeks. At the beginning of the next season, reset and start over. Hopefully, that makes sense.\n\n# re-combine the home and away dataframes into one long dataframe\nha <- \n  bind_rows(home, away) %>% \n  arrange(school, year, week)\n\n# I want to calculate the cumulative MOV mean, but I also need to preserve the\n# actual MOV as the response variable.\nmov_actual <- ha %>% .$MOV \n\n# now calculate the cumulative means\ncmeans<-\n  ha %>%\n  group_by(school, year) %>%\n  summarize_at(vars(c(3:52)), cummean) %>%\n  ungroup()\n\nhead(cmeans)\n\n\n\n  \n\n\n\nThere are a few columns from the ha dataframe, like the game ID, schools names, the week, the year, etc. that I don’t want to have a cumulative mean - I want the original columns. So I’ll select those columns and combine them with the cumulative means. Then I’ll add the actual MOV results for use as the predictor variable.\n\nha <- bind_cols(ha[, c(1:3, 5, 6, 55, 56)], cmeans[, c(3, 5:52)])\nha$MOV_actual <- mov_actual\n\nhead(ha)\n\n\n\n  \n\n\n\nI also want to include the spread as a predictor variable, which I can get via the API from /lines.\n\nspread <- readRDS(\"spread.RData\")\nhead(spread)\n\n\n\n  \n\n\n\nNow I’ll deal with the neutral site column since the dataframe is merged back together. I’m going to make a HFA column that’s a 1 for the home team at their home field, a -1 for the away team, and a 0 for both teams at a neutral site. Then I’ll add the pre-game spread for the home and away teams.\n\nha <-\n  ha %>%\n  mutate(HFA = case_when(\n    homeAway == \"home\" & !neutral_site ~ 1,\n    homeAway == \"away\" & !neutral_site ~ -1,\n    neutral_site ~ 0)) %>% \n  left_join(spread, by = \"id\") %>% \n  mutate(spread = ifelse(homeAway == \"home\", home_spread, -home_spread))\n\nSomething else I want to add is each team’s seasonal win/loss record in the for of the percent of games they’ve won. I keep adding more and more predictors. We’ll find out later if any of it will even be useful.\n\nha <- \n  ha %>%\n  mutate(wins = ifelse(MOV_actual > 0, 1, 0)) %>% \n  group_by(school, year) %>%\n  mutate(games = row_number(),\n         cumwins = cumsum(wins),\n         winPct = cumwins / games) %>%\n  select(-wins, -games, -cumwins)\n\nOk, this is where things get a little complicated. But first, a sidebar. I wrote this code over the course of a few weekends. Until this particular project, I’d never dealt with data where there are “sides” to think about. It took me a while to wrap my mind around it, and this is code from my first efforts where I was just trying to find “a way” to get things done. Pure brute force. Later you’ll see that I came up with a much better (and faster) way to do what I about to demonstrate. All part of the sausage-making the way I look at it.\nThe idea here is that I wanted to account for home field advantage. It’s a definite thing with a measurable effect. I’ll demonstrate using the home dataframe from earlier. I’ll remove games played at a neutral site, and get the mean margin of victory.\n\nhome %>% \n  filter(!neutral_site) %>%\n  summarize(meanMOV = mean(MOV))\n\n\n\n  \n\n\n\nHome teams win by almost 4 points on average. We can also check to see if the mean MOV is statistically significant. I’ll use the Wilcoxon signed rank test because I’m certain the data are not normally distributed - they’re football scores, after all. I’ll do a one-sided test where the null hypothesis is that the mean MOV is 0.\n\nwilcox.test(home %>% \n              filter(!neutral_site) %>% .$MOV, \n            alternative = \"greater\")\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  home %>% filter(!neutral_site) %>% .$MOV\nV = 3355196, p-value < 2.2e-16\nalternative hypothesis: true location is greater than 0\n\n\nBased on that p-value, we reject the null hypothesis at the 95% confidence level. Right, so home field advantage is a real thing, and I need to account for it.\nTo do that, I wanted to get away from the idea of home team and away team and instead think of them more generically by randomly picking one team as “the team” and the other as “the opponent”. My ha dataframe is still grouped by school and year, so I’ll ungroup it and call it my end-of-week results data - it’s a mental trick so I think about things differently. Then I’ll get all of the unique game IDs.\n\neowResults <- ha %>% ungroup()\n\nids <- sort(unique(eowResults %>% .$id))\n\nNext, I’ll loop over the ID. For each ID, I’ll start by picking out the common columns (id, week, and year), and then identify one team as team1 and the other as team2. For the first game, and for column-naming consistency as I build this new dataframe df, team 1 will be “the team”, and team2 will be “the opponent”. For all other games, I pick a random number between 0 and 1. If it’s greater than or equal to 0.5, team1 is “the team” and team2 is “the opponent”. Otherwise, it’s the reverse.\n\nfor (i in ids){\n  common <- eowResults %>% \n    filter(id == i) %>% \n    slice(1) %>% \n    select(id, week, year)\n  \n  team1 <- eowResults %>% \n    filter(id == i) %>% \n    slice(1) %>% \n    select(-id, -week, -year)\n  \n  colnames(team1) <- paste(colnames(team1), \"team\", sep = \"_\")\n  \n  team2 <- eowResults %>% \n    filter(id == i) %>% \n    slice(2) %>% \n    select(-id, -week, -year)\n  \n  colnames(team2) <- paste(colnames(team2), \"opponent\", sep = \"_\")\n  \n  if (i == min(ids)){df <- bind_cols(common, team1, team2)}\n  else{\n    ifelse(runif(1) >= 0.5, \n           newRow <- bind_cols(common, team1, team2), \n           newRow <- bind_cols(common, team2, team1))\n    df <- df %>% bind_rows(newRow)}\n  \n}\n\nLike I said, brute force. Now I have some redundant columns, so I’ll drop drop them before going further.\n\ndf <- \n  df %>% \n  select(-homeAway_team, -homeAway_opponent, -neutral_site_team,\n         -neutral_site_opponent, -MOV_actual_opponent, -HFA_opponent,\n         -home_spread_opponent, -spread_opponent, -home_spread_team)\n\nThe final big step is to add a bunch of new columns to represent the difference in one team’s average performance versus the average performance of the other team. Think of it this way. If a team scores on average 30 points a game, you might think that’s pretty good. However, if the teams they’ve played have on average given up 40 points, then that 30 points doesn’t look as impressive. I want to account for that for a number of statistics, so that’s what I do here.\n\ndf <- \n  df %>%\n  mutate(\n    diffPointsScored = offensePoints_team - defensePointsAllowed_opponent,\n    diffPointsAllowed = defensePointsAllowed_team - offensePoints_opponent,\n    diffYPPAOff = yardsPerPass_team - defenseYPPAAllowed_opponent,\n    diffYPPADef = defenseYPPAAllowed_team - yardsPerPass_opponent,\n    diffYPRAOff = yardsPerRushAttempt_team - defenseYPRAAllowed_opponent,\n    diffYPRADef = defenseYPRAAllowed_team - yardsPerRushAttempt_opponent,\n    diffPassYardsOff = netPassingYards_team - defensePassYardsAllowed_opponent,\n    diffPassYardsDef = defensePassYardsAllowed_team - netPassingYards_opponent,\n    diffRushYardsOff = rushingYards_team - defenseRushYardsAllowed_opponent,\n    diffRushYardsDef = defenseRushYardsAllowed_team - rushingYards_opponent,\n    diffYPPOff = yardsPerPlay_team - defenseYardsPerPlayAllowed_opponent,\n    diffYPPDef = defenseYardsPerPlayAllowed_team - yardsPerPlay_opponent,\n    diffELO = elo_team - elo_opponent,\n    diffWinPct = winPct_team - winPct_opponent\n  ) %>%\n  select(-elo_team, -elo_opponent, -winPct_team, -winPct_opponent)\n\nAt this point, I noticed that there are a number of NAs in the spread_team column, and they’re all from the 2020 season. I guess that might have to do with it being the first COVID season. I’ll replace those NAs with 0s - again, maybe not the best idea if I was doing this for real.\n\ndf <- df %>% mutate(spread_team = replace_na(spread_team, 0))\n\nOne last thing. This is going to seem out of place here, but since I cranked away at this data set a bunch of times as I was working through this, I developed some insights and some practices. First, I’ll explicitly set the teams as factors instead of strings. I’ll also generate a new column, diffMOV as the difference in the mean margin of victory - that turned out to be useful. I’ll also get rid of any column with “TD” in it’s name because I found them to be unhelpful. Same thing with pass completions, pass attempts (I essentially have those already in passEfficiency), and rushing attempts. Finally, I reorder columns to make looking at the dataset a little easier.\n\nlvls <- sort(unique(c(df %>% .$school_team, df %>% .$school_opponent)))\n\ndf <- df %>%\n  mutate(school_team = factor(school_team, levels = lvls),\n         school_opponent = factor(school_opponent, levels = lvls),\n         diffMOV  = MOV_team - MOV_opponent) %>%\n  select(\n    -MOV_team, -MOV_opponent,\n    -rushingTDs_team, -rushingTDs_opponent,\n    -puntReturnTDs_team, -puntReturnTDs_opponent,\n    -passingTDs_team, -passingTDs_opponent,\n    -kickReturnTDs_team, -kickReturnTDs_opponent,\n    -interceptionTDs_team, -interceptionTDs_opponent,\n    -passCompletions_team, -passCompletions_opponent,\n    -passAttempts_team, -passAttempts_opponent,\n    -rushingAttempts_team, -rushingAttempts_opponent\n  ) %>%\n  select(id, week, year, school_team, MOV_actual_team, HFA_team, spread_team, \n         school_opponent, diffELO, everything())\n\nData wrangling is finally complete, and the data are now ready to be split into training and test data sets. The training data will be the 2017-2020 seasons (df20), and I’ll train some models on that to predict the 2021 season (df21).\n\ndf20 <- df %>% filter(year < 2021) %>% select(-year)\ndf21 <- df %>% filter(year == 2021)\n\nPause one more time and think about the data at this point. Recall that the df20 and df21 data are game results. I can’t use game results in the test data set - I won’t have results prior to each game being played! I will however have week 1 results available at the start of week 2, and both of those will be available at the start of week 3, and so on. If I group the test data by team and week, I can just shift all of the results data down one row. That will produce NAs for the first game each team plays, so I’ll drop those. This means I won’t have a prediction for the first game each team plays, either. I’ll need to find another way of doing things if I want to do this for real. Forging ahead anyway for now…\n\ndf21 <-\n  df21 %>%  \n  arrange(school_team, week) %>% \n  group_by(school_team) %>% \n  mutate(across(9:102, lag)) %>%\n  ungroup() %>%\n  drop_na() %>% \n  select(-year)\n\nhead(df21)"
  },
  {
    "objectID": "posts/predict_football/index.html#feature-selection-with-lasso-regression",
    "href": "posts/predict_football/index.html#feature-selection-with-lasso-regression",
    "title": "Predictive Modeling",
    "section": "Feature Selection With Lasso Regression",
    "text": "Feature Selection With Lasso Regression\nThat was a fair amount of work! We now have data sets with 102 columns, and I know that a decent amount of those columns are just noise. My go-to method for feature selection is lasso regression, and it happens to be the same techniques used by the authors. But wait! I’ve been a fan of the caret package to do modeling, but now there’s tidymodels - written with tidy concepts in mind. And written by Max Kuhn - the author of caret. I’ve been wanting to try this out, so here goes.\nAfter importing the library, I need to get my training and test sets back into one dataset so tidymodels can do its thing in its tidy way. I join them and the re-split them with initial_time_split() because my observations are by week and year. The proportion is 2682/3298 because the first 2682 observations are the games in the 2017-2020 seasons. I’ll also create a validation data set.\n\nlibrary(tidymodels)\n\ndf_joined <- df20 %>% bind_rows(df21)\n\ndata_split <- initial_time_split(df_joined, prop = 2682/3298)\n\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)\n\nI’m not going to go into detail here about what each of these steps are doing because I can’t to it any better than the docs.\n\n# create the recipe\n# note I omit the categorical school variables\nlm_rec <-\n  recipe(MOV_actual_team ~ ., data = train_data %>% select(-id, -week, -school_team, -school_opponent)) %>% \n  step_dummy(all_nominal_predictors()) %>%        # one-hot encoding\n  step_zv(all_predictors()) %>%                   # eliminate zero variance columns\n  step_normalize(all_numeric(), -all_outcomes())  # normalize numeric variables\n\n# define the model and hyperparameters to tune\nlm_mod <-\n  linear_reg(penalty = tune(), mixture = 1) %>%\n  set_engine(\"glmnet\")\n\n# values to select when tuning\nlambda_grid <- tibble(penalty = 10^seq(-2, 1, length.out = 50))\n\n# cross validation folds\nset.seed(345)\nfolds <- vfold_cv(train_data, v = 10)\n\n# create the workflow\nlm_wflow <- \n  workflow() %>% \n  add_model(lm_mod) %>% \n  add_recipe(lm_rec)\n\n# tune the model\nlm_res <- \n  lm_wflow %>% \n  tune_grid(\n    grid = lambda_grid,\n    resamples = folds,\n    control = control_grid(save_pred = TRUE)\n  )\n\nLet’s take a look at the mean error and R-squared for the different penalty values (thanks to this post for the nice plot idea).\n\nlm_res %>%\n  collect_metrics() %>%\n  ggplot(aes(penalty, mean, color = .metric)) +\n  geom_errorbar(aes(\n    ymin = mean - std_err,\n    ymax = mean + std_err\n  ),\n  alpha = 0.5\n  ) +\n  geom_line(size = 1.5) +\n  facet_wrap(~.metric, scales = \"free\", nrow = 2) +\n  scale_x_log10() +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\nI have some choices of what do to next. Normally, what I’d do is select the simplest model within one standard error of the model with the lowest RMSE. I can do that with the select_by_one_std_err() function as shown below. Note the penalty value for that model and hold that thought.\n\nlm_res %>% select_by_one_std_err(metric = \"rmse\", desc(penalty))\n\n\n\n  \n\n\n\nI could also forget the “within one standard error” aspect and just select the model with the lowest RMSE (or I could select the least complex model that has no more than a certain percentage loss of RMSE). Select the model with the lowest error gives the folloing penalty value. Which one shoud I choose?\n\nlowest_rmse <- lm_res %>% select_best(\"rmse\")\nlowest_rmse\n\n\n\n  \n\n\n\nTake a look at the model object below. If I go with the 1SE lambda of 1.6, based on the degrees of freedom (Df) I’ll have only 2 or 3 predictors in the model (1.64 just happens to fall between the two penalty values below, so I’m not sure how many predictor will actually be selected). So out of about 100 predictors, only 2 or three will be selected. To me, that means a few variables dominate and the rest are just noise. With the lowest RMSE lambda value of 0.256, there will be 21-22 predictors selected. For the purpose of this post, I’m going to choose the lowest RMSE.\nFor the final fit, I finalize the work flow based on the lowest RMSE. Notice I also use data_split, which contains both the training and test data. From that fit, I get the RMSE and R-squared for the test data set. The RMSE is about 3 touchdowns, and the R-squared is alarmingly low.\n\nlm_final_wf <- \n  lm_wflow %>% \n  finalize_workflow(lowest_rmse)\n\nlm_final_fit <- \n  lm_final_wf %>%\n  last_fit(data_split) \n\nlm_final_fit %>% collect_metrics()\n\n\n\n  \n\n\n\n\nlm_final_fit %>% \n  extract_fit_parsnip()\n\nparsnip model object\n\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev  Lambda\n1   0  0.00 17.7300\n2   1 11.16 16.1500\n3   1 20.42 14.7200\n4   1 28.11 13.4100\n5   1 34.50 12.2200\n6   1 39.80 11.1300\n7   1 44.20 10.1500\n8   1 47.85  9.2440\n9   1 50.88  8.4230\n10  1 53.40  7.6750\n11  1 55.49  6.9930\n12  1 57.23  6.3720\n13  1 58.67  5.8060\n14  2 59.87  5.2900\n15  2 60.98  4.8200\n16  2 61.90  4.3920\n17  2 62.66  4.0020\n18  2 63.29  3.6460\n19  2 63.82  3.3220\n20  2 64.25  3.0270\n21  2 64.62  2.7580\n22  2 64.92  2.5130\n23  2 65.17  2.2900\n24  2 65.37  2.0860\n25  2 65.54  1.9010\n26  2 65.69  1.7320\n27  3 65.84  1.5780\n28  3 66.00  1.4380\n29  3 66.13  1.3100\n30  4 66.24  1.1940\n31  5 66.35  1.0880\n32  6 66.45  0.9913\n33  6 66.55  0.9032\n34  6 66.63  0.8230\n35  7 66.72  0.7498\n36  7 66.80  0.6832\n37 10 66.87  0.6225\n38 13 66.95  0.5672\n39 14 67.03  0.5168\n40 15 67.10  0.4709\n41 15 67.16  0.4291\n42 17 67.21  0.3910\n43 18 67.26  0.3562\n44 20 67.30  0.3246\n45 21 67.35  0.2958\n46 21 67.38  0.2695\n47 22 67.42  0.2455\n48 22 67.45  0.2237\n49 22 67.47  0.2039\n50 24 67.50  0.1857\n51 28 67.52  0.1692\n52 32 67.55  0.1542\n53 35 67.58  0.1405\n54 39 67.64  0.1280\n55 42 67.69  0.1167\n56 43 67.74  0.1063\n57 45 67.79  0.0969\n58 46 67.83  0.0882\n59 47 67.86  0.0804\n60 51 67.90  0.0733\n61 54 67.93  0.0668\n62 54 67.96  0.0608\n63 55 67.98  0.0554\n64 56 68.00  0.0505\n65 57 68.02  0.0460\n66 59 68.05  0.0419\n67 59 68.06  0.0382\n68 59 68.07  0.0348\n69 59 68.08  0.0317\n70 61 68.09  0.0289\n71 62 68.10  0.0263\n72 64 68.10  0.0240\n73 65 68.11  0.0219\n74 68 68.11  0.0199\n75 69 68.13  0.0181\n76 70 68.13  0.0165\n77 72 68.14  0.0151\n78 72 68.15  0.0137\n79 73 68.15  0.0125\n80 74 68.16  0.0114\n81 74 68.16  0.0104\n82 74 68.17  0.0095\n83 74 68.17  0.0086\n84 75 68.17  0.0079\n85 75 68.19  0.0072\n86 76 68.19  0.0065\n\n\nHere I plot the selected predictors color coded by whether the coefficient is positive or negative. Clearly, diffMOV dominates all other variables. Other important variables include the spread and home field advantage. Everything after that is probably just noise.\n\nlm_final_fit %>% \n  extract_fit_parsnip() %>%\n  vip::vi(lambda = lowest_rmse$penalty) %>%\n  mutate(\n    Importance = abs(Importance),\n    Variable = forcats::fct_reorder(Variable, Importance)\n  ) %>%\n  filter(Importance != 0) %>%\n  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +\n  geom_col() +\n  scale_x_continuous(expand = c(0, 0)) +\n  labs(y = NULL) +\n  theme_bw()\n\n\n\n\nWell, what if we were to make predictions based off of this model? First I’ll make a scatter plot of predictions versus actual MOV.\n\nlm_final_fit %>% \n  collect_predictions() %>%\n  ggplot() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  geom_point(aes(x=`.pred`, y=MOV_actual_team)) +\n  theme_bw() +\n  labs(title = \"Linear Model Prediction Results\",\n       x = \"Predicted MOV\",\n       y = \"Actual MOV\")\n\n\n\n\nNThere’s a little bit of a trend there, but it looks more like a shotgun blast. Not surprising, though, given the RMSE and R-squared we saw earlier. Let’s see what percent of predictions were correct in a head-to-head sense - in other words, what percent predicted just the correct winner.\n\nlm_final_fit %>% \n  collect_predictions() %>%\n  mutate(both_positive = `.pred` > 0 & MOV_actual_team > 0,\n         both_negative = `.pred` < 0 & MOV_actual_team < 0,\n         correct = both_positive + both_negative) %>% \n  summarize(sumCorrect = sum(correct)) / nrow(df21)\n\n\n\n  \n\n\n\nHmm… 64% Better than half, anyway, but not very impressive. Before I move on to another model type, I’ll grab the 22 predictors so I can remove everything else from the training and test data sets.\n\nkeep_vars <- \n  lm_final_fit %>% \n  extract_fit_parsnip() %>%\n  vip::vi(lambda = lowest_rmse$penalty) %>%\n  filter(Importance != 0) %>%\n  .$ Variable"
  },
  {
    "objectID": "posts/predict_football/index.html#random-forest-model",
    "href": "posts/predict_football/index.html#random-forest-model",
    "title": "Predictive Modeling",
    "section": "Random Forest Model",
    "text": "Random Forest Model\nI’ll try a few different models with the same data and see how they compare. I’ll start with a random forest model using the ranger package. I’ll use the same tidymodels procedure as above: create a recipe, model, and workflow, tune hyperparameters, and show the errors for the best model. This takes a while to execute on my laptop even when using all available cores.\n\ncores <- parallel::detectCores()\n\n# the recipe\nrf_rec <- \n  recipe(MOV_actual_team ~ ., data = train_data %>% select(keep_vars, MOV_actual_team)) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors())\n\n# the model to tune\nrf_mod <-\n  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%\n  set_engine(\"ranger\", importance = \"impurity\", num.threads = cores) %>%\n  set_mode(\"regression\")\n\n# the workflow\nrf_wflow <- \n  workflow() %>% \n  add_model(rf_mod) %>% \n  add_recipe(rf_rec)\n\n# now set the seed for reproducability and tune the hyperparameters\nset.seed(1234)\n\nrf_fit <-\n  rf_wflow %>%\n  tune_grid(grid = 25,\n            control = control_grid(),\n            resamples = folds)\n\n# take a look at the best models\nrf_fit %>% show_best(metric = \"rmse\")\n\n\n\n  \n\n\n\nThere’s a nice autoplot() function that comes with tune (one of the tidymodels dependencies) that can be used with various tidymodels objects. Let’s check it out with rf_fit.\n\nautoplot(rf_fit) + theme_bw()\n\n\n\n\nLooks like large numbers of mtry are good, but the error flattens out after 50 or so.\n\n# final fit\nlast_rf_mod <- \n  rand_forest(mtry = 17, min_n = 22, trees = 1000) %>% # original \n  set_engine(\"ranger\", importance = \"impurity\", num.threads = cores) %>% \n  set_mode(\"regression\")\n\n# the last workflow\nlast_rf_workflow <- \n  rf_wflow %>% \n  update_model(last_rf_mod)\n\n# the last fit\nset.seed(345)\nlast_rf_fit <- \n  last_rf_workflow %>% \n  last_fit(data_split)\n\nlast_rf_fit %>% collect_metrics()\n\n\n\n  \n\n\n\nStill pretty bad. Let’s take a look at variable importance for this model.\n\nlast_rf_fit %>% \n  pluck(\".workflow\", 1) %>%   \n  extract_fit_parsnip() %>% \n  vip::vip(num_features = 22) +\n  theme_bw()\n\n\n\n\nReally only three variables that are contributing much to the model. I’ll do predicted versus actual again and hope for less of a shotgun blast.\n\nlast_rf_fit %>% \n  collect_predictions() %>%\n  ggplot() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  geom_point(aes(x=`.pred`, y=MOV_actual_team)) +\n  theme_bw() +\n  labs(title = \"Ranger Prediction Results\",\n       x = \"Predicted MOV\",\n       y = \"Actual MOV\")\n\n\n\n\nMaybe looks a little more elongated but that could be wishful thinking. How about the percent correct head to head?\n\nlast_rf_fit %>% \n  collect_predictions() %>%\n  mutate(both_positive = `.pred` > 0 & MOV_actual_team > 0,\n         both_negative = `.pred` < 0 & MOV_actual_team < 0,\n         correct = both_positive + both_negative) %>%\n  summarize(sumCorrect = sum(correct)) / nrow(df21)\n\n\n\n  \n\n\n\nSlightly worse, but basically the same. Well, while I’m at it, I’ll look at predictions, actual MOV, and the spread.\n\nlast_rf_fit %>% \n  collect_predictions() %>%\n  mutate(spread_team = test_data$spread_team) %>%\n  select(`.pred`, MOV_actual_team, spread_team) %>%\n  head()\n\n\n\n  \n\n\n\nThe predictions don’t compare well, that’s for sure. Well, this may partially be the result of replacing NAs with 0s earlier, and I suspect I just have too many garbage predictors in the model. The authors were getting a head to head accuracy in the low 70’s, so there’s definite room for improvement. I’ll drive on anyway and look at teams that did a lot better or worse than predicted. Here I show only the extreme two ends. I plot (predicted - actual), so positive values are for teams that I predicted would win by a much larger margin than they actually did.\n\nlast_rf_fit %>% \n  collect_predictions() %>%\n  mutate(team = df21$school_team,\n         opp = df21$school_opponent,\n         spread_team = df21$spread_team) %>%\n  group_by(team) %>%\n  summarize(meanDiff = mean(`.pred` - MOV_actual_team)) %>%\n  arrange(meanDiff) %>%\n  mutate(team = forcats::fct_reorder(team, meanDiff)) %>%\n  filter(abs(meanDiff) > 7) %>%\n  ggplot() +\n  geom_col(aes(x=team, y=meanDiff)) +\n  coord_flip() +\n  theme_bw()\n\n\n\n\nFinally, I’ll look at the proportion of games I correctly predicted over time. I’m curious if I do poorly early in the season and then improve, or if it’s relatively consistent. Quick note: the first time I made this plot, I was getting 100% accuracy in the early weeks, and then it got steadily worse over time. That was a giant red flag, and after thoroughly scrubbing my code, I found an error where some actual results had snuck into the test data. I’m glad I caught that!\n\nlast_rf_fit %>%\n  collect_predictions() %>%\n  bind_cols(test_data %>% \n              select(id, week, school_team, school_opponent) %>% \n              rename(\"ID\" = \"id\")) %>%\n  mutate(incorrect = case_when(\n    `.pred` > 0 & MOV_actual_team < 0 ~ 1,\n    `.pred` < 0 & MOV_actual_team > 0 ~ 1,\n    TRUE ~ 0), \n    correct= case_when(\n      `.pred` > 0 & MOV_actual_team > 0 ~ 1,\n      `.pred` < 0 & MOV_actual_team < 0 ~ 1,\n      TRUE ~ 0)) %>%\n  group_by(week) %>%\n  summarize(sumIncorrect = sum(incorrect),\n            sumCorrect = sum(correct),\n            n = n(),\n            proportionCorrect = sumCorrect / n) %>%\n  ggplot() +\n  geom_col(aes(x=week, y=proportionCorrect)) +\n  theme_bw()\n\n\n\n\nLooks reasonably consistent, so at least I know I fixed that error."
  },
  {
    "objectID": "posts/predict_football/index.html#gradient-boost-machine-model",
    "href": "posts/predict_football/index.html#gradient-boost-machine-model",
    "title": "Predictive Modeling",
    "section": "Gradient Boost Machine Model",
    "text": "Gradient Boost Machine Model\nI don’t have much hope that this will be any better than the last two models, but it’ll give me another rep with the tidymodels workflow. Note that I’m using the random forest recipe (rf_rec). In hindsight, the recipe is model agnostic, so I should have just called it something generic to avoid confusion.\n\nset.seed(345)\ngbm_mod <-\n  boost_tree(trees = 1000, \n             tree_depth = tune(), \n             min_n = tune(), \n             loss_reduction = tune(),\n             sample_size = tune(), \n             mtry = tune(),\n             learn_rate = tune()) %>%\n  set_engine(\"xgboost\", num.threads = cores) %>%\n  set_mode(\"regression\")\n\ngbm_wflow <- \n  workflow() %>% \n  add_model(gbm_mod) %>% \n  add_recipe(rf_rec)\n\ngbm_fit <- \n  gbm_wflow %>% \n  tune_grid(grid = 25,\n            control = control_grid(save_pred = TRUE),\n            resamples = folds)\n\ngbm_fit %>% show_best(metric = \"rmse\")\n\n\n\n  \n\n\n\nThen the final fit.\n\ngbm_best <- \n  gbm_fit %>% \n  select_best(metric = \"rmse\")\n\n# final fit\nlast_gbm_mod <- \n  boost_tree(trees = 1000, \n              tree_depth = gbm_best %>% .$tree_depth, \n              min_n = gbm_best %>% .$min_n, \n              loss_reduction = gbm_best %>% .$loss_reduction,\n              sample_size = gbm_best %>% .$sample_size, \n              mtry = gbm_best %>% .$mtry,\n              learn_rate = gbm_best %>% .$learn_rate) %>% \n  set_engine(\"xgboost\", num.threads = cores) %>% \n  set_mode(\"regression\")\n\n# the last workflow\nlast_gbm_workflow <- \n  gbm_wflow %>% \n  update_model(last_gbm_mod)\n\n# the last fit\nset.seed(345)\nlast_gbm_fit <- \n  last_gbm_workflow %>% \n  last_fit(data_split)\n\n# since I used data_split above, this includes the test data set\nlast_gbm_fit %>% collect_metrics()\n\n\n\n  \n\n\n\nMore of the same, of course, and now variable importance.\n\nlast_gbm_fit %>% \n  pluck(\".workflow\", 1) %>%   \n  extract_fit_parsnip() %>% \n  vip::vip(num_features = 20) +\n  theme_bw()\n\n\n\n\nYep, and now accuracy.\n\nlast_gbm_fit %>% \n  collect_predictions() %>%\n  mutate(both_positive = `.pred` > 0 & MOV_actual_team > 0,\n         both_negative = `.pred` < 0 & MOV_actual_team < 0,\n         correct = both_positive + both_negative) %>%\n  summarize(sumCorrect = sum(correct)) / nrow(df21)\n\n\n\n  \n\n\n\nAbout the same as the others, so no surprise. Well, aside from demonstrating the garbage-in garbage-out principle, it was interesting to dip my toe in the waters of predictive modeling of sports events. As a bonus, I got some practice with tidymodels, which was great.\nAt this point in my weekend modeling, I paused and thought about what might have gone wrong and what might have gone right in the methodology I used. I also read this post about using a neural net to predict college football games. I tend to shy away from neural nets for regression problems. In my experience, they’re a pain to set up, take a long time to train, and I really haven’t seen them outperform the simpler, faster, and interpretable tree-based models.\nThen I stumbled across this blog post from RStudio that demonstrated the use of tabnet, a neural net model developed by Google specifically for tabular data that incorporates some of the processes in tree-based models to improve interpretability. In the original paper, the authors also demonstrated that tabnet was on par, and often better, than the current go-to models like xgboost. So, time to give it a whack."
  },
  {
    "objectID": "posts/predict_football/index.html#neural-net-model",
    "href": "posts/predict_football/index.html#neural-net-model",
    "title": "Predictive Modeling",
    "section": "Neural Net Model",
    "text": "Neural Net Model\nAs I mentioned, I’m not too happy with the mess of predictors I’ve been using so far. Time for a fresh start using just a few predictors similar to the CollegeFootballData.com blog I just mentioned. I have lots of variables in my environment right now, so first I’m going to purge.\n\nrm(list = ls())\n\nI’ll load some of the same data sets from disk that I used earlier and filter non-FBS games as before. However, this time, I’m only going to keep a few columns: id, school, conference, homeAway, points, week, and year.\n\ngs <- readRDS(\"gameStats_full.RData\")\ngs <- type.convert(gs, as.is = TRUE)\n\n# get the ID of non-FBS games\nfbs <- readRDS(\"fbs_teams.RData\")\nfcsIDs <- gs %>% filter(!school %in% (fbs %>% .$school)) %>% .$id\n\n# filter\ngs <- \n  gs %>%\n  filter(!id %in% fcsIDs) %>%\n  select(id, school, conference, homeAway, points, week, year)\n\nI’ll create home and away dataframes again as earlier and add the Elo and neutral site data, and then recombine it all into a dataframe called pregame. I’ll calculate the margin of victory (mov) here, too. I noticed one of the away conference names was missing but was able to track down that it was a FBS independent team.\n\nhome <- gs %>% filter(homeAway == \"home\")\naway <- gs %>% filter(homeAway == \"away\")\n\n# get pre-game Elo and neutral vield data\neloSite <- readRDS(\"eloSite.RData\") %>% \n  filter(!id %in% (fcsIDs)) %>% \n  tidyjson::as_tibble(eloSite)\n\n# get pre-game spread\nspread <- readRDS(\"spread.RData\")\nspread <- spread %>% filter(!id %in% (fcsIDs))\nspread <- spread %>% drop_na()\n\npregame <- spread %>% left_join(eloSite, by = \"id\")\n\npregame <- \n  pregame %>%\n  left_join(home %>% select(-school, -homeAway), by = \"id\") %>%\n  rename(\"home_conference\" = \"conference\", \"home_points\" = \"points\") %>%\n  left_join(away %>% select(-school, -year, -homeAway, -week), by = \"id\") %>%\n  rename(\"away_conference\" = \"conference\", \"away_points\" = \"points\") %>% \n  mutate(mov = home_points - away_points) %>%\n  select(-home_points, -away_points) %>% \n  arrange(year) %>%\n  mutate(away_conference = replace_na(away_conference, \"FBS Independents\"))\n\nhead(pregame)\n\n\n\n  \n\n\n\nI’m only working with 12 columns this time - well, 9 if you don’t count id, week, and year. I won’t use those for training or testing. They’re just to keep track of things. So it’s down to team, conference, spread, Elo, and the home field advantage thing, which I’ll address next.\nRecall my first attempt was complicated and involved looping over game IDs. I came up with a much better way this time. I randomly select game IDs, then apply the HFA indicator in place. Way faster and a lot clearer what I’m doing.\n\nset.seed(42)\nidx <- sample(1:nrow(pregame), trunc(nrow(pregame) / 2))\npregame$HFA <- 0\npregame[idx, \"HFA\"] <- 1\npregame[-idx, \"HFA\"] <- -1\npregame[-idx, \"mov\"] <- pregame[-idx, \"mov\"]\npregame <- pregame %>% mutate(HFA = ifelse(neutral_site, 0, HFA))\n\nThe last thing to do is turn the team names into factors.\n\npregame <-\n  pregame %>%\n  mutate(home_team = factor(home_team),\n         away_team = factor(away_team))\n\nThat’s it! I’m ready to split the data in the training, test, and validation sets and get to model building.\n\ndata_split <- \n  initial_time_split(\n    pregame %>% select(-id, -year, -neutral_site, -week),\n    prop = 2656/3391)\n\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)\nset.seed(234)\nval_set <- validation_split(train_data, prop = 0.80)\n\nThere are many hyperparameters to tune, and I’ll exclude epochs, batch_size, and virtual_batch_size just to speed this part up.\n\nlibrary(tabnet)\n\nnn_rec <- recipe(mov ~ ., train_data) %>%\n  step_normalize(all_numeric(), -all_outcomes())\n\nnn_mod <-\n  tabnet(epochs = 5, \n         batch_size = 256, \n         decision_width = tune(), \n         attention_width = tune(),\n         num_steps = tune(), \n         penalty = tune(), \n         virtual_batch_size = 64, \n         momentum = tune(),\n         feature_reusage = tune(), \n         learn_rate = tune()\n  ) %>%\n  set_engine(\"torch\") %>%\n  set_mode(\"regression\")\n\nnn_wf <- \n  workflow() %>%\n  add_model(nn_mod) %>%\n  add_recipe(nn_rec)\n\nset.seed(42)\nnn_fit <-\n  nn_wf %>%\n  tune_grid(val_set,\n            grid = 50,\n            control = control_grid())\n\nnn_fit$.notes[[1]]$note\n\ncharacter(0)\n\nnn_fit %>% show_best(metric = \"rmse\")\n\n\n\n  \n\n\n\nThe final fit.\n\n# with tuned parameters\nlast_nn_mod <-\n  tabnet(epochs = 15, \n         batch_size = 256, \n         decision_width = 15, \n         attention_width = 62,\n         num_steps = 8, \n         penalty = 0.1430669, \n         virtual_batch_size = 64, \n         momentum = 0.194,\n         feature_reusage = 1.148, \n         learn_rate = 0.02395\n  ) %>%\n  set_engine(\"torch\", verbose = TRUE) %>%\n  set_mode(\"regression\")\n\n# the last workflow\nlast_nn_workflow <- \n  nn_wf %>% \n  update_model(last_nn_mod)\n\n# the last fit\nset.seed(42)\nlast_nn_fit <- \n  last_nn_workflow %>% \n  last_fit(data_split)\n\n# since I used data_split above, this includes the test data set\nlast_nn_fit %>% collect_metrics()\n\n\n\n  \n\n\n\nBoth metrics are quite a bit higher than what we’ve seen up to this point, so I’m optimistic this model will perform better.\n\nlast_nn_fit %>% \n  pluck(\".workflow\", 1) %>%   \n  extract_fit_parsnip() %>% \n  vip::vip() +\n  theme_bw()\n\n\n\n\nThe model relies heavily on the spread, which makes sense since given that it’s hard to beat the spread, and I’m considering only a few other predictors.\n\n# find percent I predicted the correct winner\nlast_nn_fit %>% \n  collect_predictions() %>%\n  mutate(both_positive = `.pred` > 0 & mov > 0,\n         both_negative = `.pred` < 0 & mov < 0,\n         correct = both_positive + both_negative) %>%\n  summarize(Correct = sum(correct)) / nrow(test_data)\n\n\n\n  \n\n\n\nThis model correctly predicts 72.3% of the game winners, compared to the ~ 60% for the earlier models. How does that compare to the spread?\n\n# how does the spread do?\nlast_nn_fit %>% \n  collect_predictions() %>%\n  mutate(spread = -test_data$home_spread,\n         both_positive = spread > 0 & mov > 0,\n         both_negative = spread < 0 & mov < 0,\n         correct = both_positive + both_negative) %>%\n  summarize(Correct = sum(correct)) / nrow(test_data)\n\n\n\n  \n\n\n\nNot surprising that it’s almost the same. Really at this point, this model basically is the spread. Obviously, if we want to beat the spread, we need to improve on this. Let’s check the predicted vs. actual raw values.\n\nlast_nn_fit %>% \n  collect_predictions() %>%\n  head(10)\n\n\n\n  \n\n\n\nAnd now let’s plot them.\n\nlast_nn_fit %>% \n  collect_predictions() %>%\n  ggplot() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  geom_point(aes(x=`.pred`, y=mov)) +\n  theme_bw() +\n  labs(title = \"NN Prediction Results\",\n       x = \"Predicted MOV\",\n       y = \"Actual MOV\")\n\n\n\n\n\nInterpretability Plots\nEarlier I mentioned that tabnet models were developed to be interpretable. The following plot shows the first 50 games in the test set with games on the x axis. For these games, it seems the Elo scores were the biggest contributors. Interesting. I wonder why we don’t see more of an impact from the spread.\n\nexp_fit <- \n  last_nn_fit %>% \n  pluck(\".workflow\", 1) %>%   \n  extract_fit_parsnip()\n\nex_fit <- tabnet_explain(exp_fit$fit, test_data[1:50, ])\n\nautoplot(ex_fit) +\n  labs(x = \"Game\", y = \"Predictor\")\n\n\n\n\nNext, for the same 50 games, we see that different predictors are important in different steps. Here we see that spread comes into play mostly in the 7th and 8th step.\n\n# PER-STEP, OBSERVATION-LEVEL FEATURE IMPORTANCES\nautoplot(ex_fit, type=\"steps\") +\n  labs(x = \"Game\", y = \"Predictor\")"
  },
  {
    "objectID": "posts/predict_football/index.html#next-steps",
    "href": "posts/predict_football/index.html#next-steps",
    "title": "Predictive Modeling",
    "section": "Next Steps",
    "text": "Next Steps\nWhile tabnet looks promising, I can’t directly compare its performance with the other models in this post because I used a different data set for it. I saw in the tidymodels docs that there’s a way to do model comparison using work flow sets, so I’ll check that out in another post."
  },
  {
    "objectID": "posts/qlearn/index.html",
    "href": "posts/qlearn/index.html",
    "title": "Q Learning",
    "section": "",
    "text": "I’ve been reading some books on machine learning, and recently started going through Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aurélien Géron. His chapter on reinforcement learning is great, and it inspired me to apply some of the methods on my own. I decided to start with the game of tic-tac-toe since it’s a little more complicated than a trivial simple example but not as complicated as chess or go."
  },
  {
    "objectID": "posts/qlearn/index.html#states-and-actions",
    "href": "posts/qlearn/index.html#states-and-actions",
    "title": "Q Learning",
    "section": "States and Actions",
    "text": "States and Actions\nQ-Learning is a type of reinforcement learning that can be applied to situations where there are a discrete number of states and actions, but the transition probabilities between states are unknown. In the game of tic-tac-toe, the state is the game board, which consists of nine possible locations to play that are either empty, contain an X, or contain a O. I will represent the state as an array of length nine where 0 represents an empty space, 1 represents an X, and -1 represents an O. I’m defining things this way because eventually I want to apply deep Q learning to the same problem, and these will become inputs to a neural network. Anyway, at the beginning of the game, the board is empty, so it will be represented as:\n\nstate = [0 for i in range(9)]\nstate\n\n[0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n\nAt the beginning of the game, there are nine possible actions for the first player (in this case, player X). An action of 0 represents X playing in the upper left board position, an action of 1 represents the upper middle position, etc. If X plays action 0, and O plays action 3, the state becomes:\n\nstate[0] = 1   # X plays action 0\nstate[3] = -1  # O plays action 3\nstate\n\n[1, 0, 0, -1, 0, 0, 0, 0, 0]"
  },
  {
    "objectID": "posts/qlearn/index.html#rewards",
    "href": "posts/qlearn/index.html#rewards",
    "title": "Q Learning",
    "section": "Rewards",
    "text": "Rewards\nIn all reinforcement learning techniques, a reward system is used. To develop a game playing policy for X, if an action results in X winning the game, the reward will be 1. If O wins, the reward will be -1. All other actions will have a reward of 0."
  },
  {
    "objectID": "posts/qlearn/index.html#q-values",
    "href": "posts/qlearn/index.html#q-values",
    "title": "Q Learning",
    "section": "Q-Values",
    "text": "Q-Values\nIn Q-Learning, state-action pairs (s, a) have an associated quality value \\(Q\\) that are used to determine what the best move is for that state (the higher the Q-value, the better the move). When the algorithm begins and the first game is played, all Q-values are zero, so the AI (player X) is essentially playing randomly. Once the first game is played that is either a win or a loss, the first non-zero reward is given, and so the first non-zero Q-value is produced for the state-action pair that resulted in that reward. As the algorithm explores the state space, non-zero Q-values propagate back from the state-action pairs that won or lost the game to the state-action pairs that led up to the winning or losing move. In other words, the Q-values are slowly updated based on a running average of the reward \\(r\\) received when leaving a state \\(s\\) with an action \\(a\\) plus the sum of discounted future rewards it expects to get assuming optimal play from that point on and adjusted by a learning rate \\(\\alpha\\). To estimate the sum of the discounted future rewards, I take the maximum of the Q-value estimates for the next state \\(s′\\). That was a mouthful. Maybe in this case, math might be clearer than English:\n\\[\nQ\\left(s, a\\right) \\xleftarrow[\\alpha ]{} r \\gamma\\cdot \\underset{a'}{max}Q\\left(s', a'\\right)\n\\]\nThat’s it. It ends up just being a big book-keeping exercise to keep track of a whole bunch of state-action pairs and their associated Q-values. To do that, I use a dictionary data structure with the state-action pair as the key and the Q-value as the value."
  },
  {
    "objectID": "posts/qlearn/index.html#epsilon-greedy-policy",
    "href": "posts/qlearn/index.html#epsilon-greedy-policy",
    "title": "Q Learning",
    "section": "Epsilon-Greedy Policy",
    "text": "Epsilon-Greedy Policy\nI added an epsilon-greedy policy to encourage the algorithm to continue to explore the state-space throughout the training process. I implemented it in the following way. Before each action by X, I draw a random number and compare it to a variable represented by epsilon. If the random number is less than epsilon, then player X’s next action is random. Otherwise, player X’s next action is determined by the Q-values. When the algorithm starts, epsilon is 1.0. As training progresses, epsilon gradually decreases to 0.01."
  },
  {
    "objectID": "posts/qlearn/index.html#the-tic-tac-toe-environment",
    "href": "posts/qlearn/index.html#the-tic-tac-toe-environment",
    "title": "Q Learning",
    "section": "The Tic-Tac-Toe Environment",
    "text": "The Tic-Tac-Toe Environment\nI set this all up in a{python} class called env with the following functions:\n\nreset sets the state to represent an empty board (the start of a new game)\ngame_over checks to see if the game is over, and if so, gives a reward for a win or loss\nstep represents a single action either by player X or player O\ntrain_q implements the Q-learning algorithm\n\nAfter importing the usual suspects, I define the env class.\n\nimport copy\nimport random\nimport plotly.express as px\n\n\nclass env:\n\n    def __init__(self):\n        self.state = self.reset()\n        self.alpha0 = 0.05           # learning rate\n        self.decay = 0.005           # learning rate decay\n        self.gamma = 0.95            # discount factor\n        self.Qvalues = {}\n\n    def reset(self):\n        return [0 for i in range(9)]\n\n    def game_over(self, s):\n        done = False\n        reward = 0\n        if (s[0] + s[1] + s[3]  == 3 or s[3] + s[4] + s[5]  == 3 or s[6] + s[7] + s[8]  == 3 or\n            s[0] + s[3] + s[6]  == 3 or s[1] + s[4] + s[7]  == 3 or s[2] + s[5] + s[8]  == 3 or\n            s[0] + s[4] + s[8]  == 3 or s[2] + s[4] + s[6]  == 3):\n            done = True\n            reward = 1\n        if (s[0] + s[1] + s[3]  == -3 or s[3] + s[4] + s[5]  == -3 or s[6] + s[7] + s[8]  == -3 or\n            s[0] + s[3] + s[6]  == -3 or s[1] + s[4] + s[7]  == -3 or s[2] + s[5] + s[8]  == -3 or\n            s[0] + s[4] + s[8]  == -3 or s[2] + s[4] + s[6]  == -3):\n            done = True\n            reward = -1\n        if sum(1 for i in s if i != 0)==9 and not done:\n            done = True\n        return done, reward\n\n    def step(self, state, action, player):\n        next_state = state.copy()\n        if player == 0: next_state[action] = 1\n        else: next_state[action] = -1\n        done, reward = self.game_over(next_state)\n        return next_state, done, reward\n\n    def train_q(self, iterations):\n        for iteration in range(iterations): # loop to play a bunch of games\n            state = self.reset()\n            next_state = state.copy()\n            done = False\n            epsilon = max(1 - iteration/(iterations*0.8), 0.01)\n            while not done: # loop to play one game\n                if random.random() < epsilon:  # epsilon greedy policy for player X\n                    action = random.choice([i for i in range(len(state)) if state[i] == 0])\n                else:\n                    xq = [self.Qvalues.get((tuple(state), i)) for i in range(9) if self.Qvalues.get((tuple(state), i)) is not None]\n                    if len(xq) == 0: action = random.choice([i for i in range(len(state)) if state[i] == 0])\n                    else:\n                        idx = [i for i in range(9) if self.Qvalues.get((tuple(state), i)) is not None]\n                        action = idx[xq.index(max(xq))]\n                next_state, done, reward = self.step(state, action, 0)\n                if not done: # random policy for player O\n                    omove = random.choice([i for i in range(len(next_state)) if next_state[i] == 0])\n                    next_state, done, reward = self.step(next_state, omove, 1)\n                if not done:\n                    key = (tuple(state), action)\n                    if key not in self.Qvalues:\n                        self.Qvalues[key] = reward\n                    next_idx = [i for i in range(9) if self.Qvalues.get((tuple(next_state), i)) is not None]\n                    if len(next_idx) > 0:\n                        next_value = max([self.Qvalues.get((tuple(next_state), i)) for i in next_idx])\n                    else: next_value = 0\n                else: next_value = reward\n                # now update the Q-value for the state-action pair\n                alpha = self.alpha0 / (1 + iteration * self.decay)\n                self.Qvalues[key] *= 1 - alpha\n                self.Qvalues[key] += alpha * (reward + self.gamma * next_value)\n                state = next_state.copy()\n        return self.Qvalues"
  },
  {
    "objectID": "posts/qlearn/index.html#results",
    "href": "posts/qlearn/index.html#results",
    "title": "Q Learning",
    "section": "Results",
    "text": "Results\nThe play_v_random function pits the AI-enabled player against an opponent that plays randomly. It takes the number of games to be played as an argument and returns the number of X wins, O wins, and ties.\n\ndef play_v_random (games):\n    results = [0 for i in range(games)]\n    for i in range(games):\n        state = ttt.reset()\n        next_state = state.copy()\n        done = False\n        while not done:\n            xq = [Q_values.get((tuple(state), i)) for i in range(9) if Q_values.get((tuple(state), i)) is not None]\n            if len(xq) == 0:\n                action = random.choice([i for i in range(len(state)) if state[i] == 0])\n            else:\n                idx = [i for i in range(9) if Q_values.get((tuple(state), i)) is not None]\n                action = idx[xq.index(max(xq))]\n            next_state, done, reward = ttt.step(state, action, 0)\n            if not done:\n                omove = random.choice([i for i in range(len(next_state)) if next_state[i] == 0])\n                next_state, done, reward = ttt.step(next_state, omove, 1)\n            state = next_state.copy()\n        results[i] = reward\n    return results\n\nSo here we go. I’ll have X play O before any Q-learning, update the Q-values based on 20,000 games, have X play O again, update Q-values some more, and so on for 10 iterations. By the end, the algorithm will have seen 200,000 games. Recall that the output is X wins, O wins, and ties and that X plays O 1,000 times, so move the decimal to the left once for the percent of wins.\n\nttt = env()\nfor i in range(101):\n    Q_values = ttt.train_q(2000)\n    if i % 10 == 0:\n        results = play_v_random(1000)\n        print(sum(1 for i in results if i == 1), sum(1 for i in results if i == -1), sum(1 for i in results if i == 0))\n\n566 199 235\n\n\n637 134 229\n\n\n657 148 195\n\n\n657 146 197\n\n\n657 169 174\n\n\n653 152 195\n\n\n686 158 156\n\n\n731 65 204\n\n\n751 67 182\n\n\n711 70 219\n\n\n761 32 207\n\n\nSeems to be working nicely! X, the AI-enabled player, definitely improves over time. This shows all of the non-zero Q-values at the end of training. There are a little over 6,500 of them and most are positive, which suggests to me that the algorithm capitalizing on what it’s learned and back propagating Q-values. Since there are more positive Q-values than negative, it’s seeking out those positive rewards like it’s supposed to.\n\n# fig.height: 3\nx = [i for i in range(len(Q_values))]\nq = list(Q_values.values())\nq.sort()\n\nfig = px.scatter(x=x, y=q)\nfig.update_layout(xaxis_title='X', yaxis_title='Q Value')\nfig.show()\n\n\n                                                \n\n\nAnyone who has played tic-tac-toe more than a few times learns that the best opening move is to play in the center of the board. I was curious of the Q-leaning algorithm picked this up, so I looked up the Q-values for an empty board. The fifth value listed represents the center of the board, and it is in fact the greatest Q-value. Pretty cool!\n\n[Q_values.get(((0,0,0,0,0,0,0,0,0), i)) for i in range(9)]\n\n[0.6190608868991323,\n 0.4058863119343145,\n 0.4373177028290766,\n 0.617483986187406,\n 0.7045528805228131,\n 0.3919743371735407,\n 0.5356424094685056,\n 0.4459128192284273,\n 0.45070040964118824]"
  },
  {
    "objectID": "posts/recaman/index.html",
    "href": "posts/recaman/index.html",
    "title": "Recaman Sequence",
    "section": "",
    "text": "The next stop on my generative art journey took me to the Recaman Sequence. As opposed to many other algorithms, there’s no randomness in this sequence - the sequence is what it is. At some point, someone much more clever than me figured out an interesting way to visualize it. First, I’ll describe the sequence, then the visualization technique."
  },
  {
    "objectID": "posts/recaman/index.html#the-recaman-sequence",
    "href": "posts/recaman/index.html#the-recaman-sequence",
    "title": "Recaman Sequence",
    "section": "The Recaman Sequence",
    "text": "The Recaman Sequence\nThe Wikipedia entry states that, to generate the Recaman sequence, apply the following algorithm.\n\n\n\n\n\nIn English, that means we make a counter n that starts at 1 and goes up. For each counter number, we’re going to try to go backwards that many positions in the sequence. If that position in the sequence doesn’t exist or if that position already has a value in it, then we go forward that many positions.\nBesides the counter, we also need to keep track of the sequence itself, which starts at 0. Let’s consider that a moment. We have a counter that starts at 1, and a sequence that starts at 0. Indexing in R starts at 1, so we need to be really careful not to get an off-by-one error (OOBE)! If we’re careful, we should end up with the following first few numbers in the sequence [0, 1, 3, 6, 2, 7].\nSo, time to give it a shot and generate the first six numbers. So I didn’t commit the OOBE, I actually re-wrote the rules above by hand and walked through the rules. From that exercise, I found that instead of the +1 and -1 terms, I need +[n-1] and -[n-1]. This code does the trick.\n\na <- 0\n\nfor (n in 1:1000){\n  ifelse(a[n - 1] - (n - 1) > 0 & !(a[n - 1] - (n - 1)) %in% a, \n         a <- c(a, a[n - 1] - (n - 1)), \n         a <- c(a, a[n - 1] + (n - 1)))\n}\n\na[1:6]\n\n[1] 0 1 3 6 2 7\n\n\nThat’s really all there is to it. All we need to do is just expand that sequence out however long we want."
  },
  {
    "objectID": "posts/recaman/index.html#data-visualization",
    "href": "posts/recaman/index.html#data-visualization",
    "title": "Recaman Sequence",
    "section": "Data Visualization",
    "text": "Data Visualization\nAs I said earlier, I think this is the clever bit. The way this works is that we connect each number in the sequence with a half-circle that alternates between arching above and arching below a horizontal line. To do that, I wrote this function. It takes two adjacent numbers from the sequence and creates 33 (x, y) coordinates that, when connected together with a line, will make a half circle. There’s some logic built in to handle the need to alternate up and down, and also the difference between arcing to the right on the horizontal line between 1 and 3 versus arcing to the left between 6 and 3.\n\ngenerate_circle_points <- function(idx, p1, p2){\n  radius <- (p2 - p1) /2\n  x <- radius\n  y <- 0\n  for (alpha in 1:32 * 180/32 * pi/180){\n    x <- c(x, radius * cos(alpha))\n    y <- c(y, radius * sin(alpha))\n  }\n  if (idx %% 2 != 0){y <- -y}   # logic for up/down\n  if (p1 > p2){y <- -y}         # logic for left/right\n  x <- rev(x)\n  data.frame(x = x + radius + p1, y = y)\n}\n\nNow we iterate though the sequence and create all of the half circles.\n\nfor (i in 1:(length(a) - 1)){\n  if(i == 1){\n    df <- generate_circle_points(i, a[i], a[i+1])\n    df$arc <- i\n  }else{\n    df_new <- generate_circle_points(i, a[i], a[i+1])\n    df_new$arc <- i\n    df <- rbind(df, df_new)\n  }\n}\n\nhead(df)\n\n\n\n  \n\n\n\n\nlibrary(plotly)\nlibrary(RColorBrewer)\n\n\ndf %>% mutate(arc = factor(arc)) %>%\n  plot_ly() %>%\n  add_paths(x=~x, y=~y, hoverinfo = \"none\", showlegend = FALSE, \n            line = list(width = 0.5),\n            color=~arc, \n            colors = colorRampPalette(brewer.pal(10, \"Spectral\"))(nrow(df))) %>%\n  layout(\n    xaxis = list(\n      title = \"\",\n      zeroline = FALSE,\n      showline = FALSE,\n      showticklabels = FALSE,\n      showgrid = FALSE),\n    yaxis = list(\n      title = \"\",\n      zeroline = FALSE,\n      showline = FALSE,\n      showticklabels = FALSE,\n      showgrid = FALSE,\n      scaleanchor = \"x\",\n      scaleratio = 1\n    ),\n    paper_bgcolor = \"#000000\", plot_bgcolor = \"#000000\")\n\n\n\n\n\nYou can click here for my Shiny app to generate your own art using this and other algorithms. Enjoy!"
  },
  {
    "objectID": "posts/res5/index.html",
    "href": "posts/res5/index.html",
    "title": "Resolution 5 Fractional Factorial Designs",
    "section": "",
    "text": "At work, I needed to create a resolution V (R5) fractional factorial design for use in a study of the relative effectiveness of various modernization programs. The computer network on which I was working was an isolated network completely disconnected from the internet with specific versions of software installed to support the type of work done. When I’ve needed to create an R5 design in the past, I either used the FrF2 R package or a Ruby script written by Dr. Paul Sanchez at the Naval Postgraduate School. Neither of those were viable options on the isolated computer network, so I needed to create an R5 design for 14 factors from scratch. Since base R was available for this task, that seemed the path of least resistance."
  },
  {
    "objectID": "posts/res5/index.html#resolution-v-designs",
    "href": "posts/res5/index.html#resolution-v-designs",
    "title": "Resolution 5 Fractional Factorial Designs",
    "section": "Resolution V Designs",
    "text": "Resolution V Designs\nSo why did I need to create an R5 design, and what is one? The study involved 14 modernization programs, and I wanted to evaluate the contribution of each modernization program compared to its current equivalent. That means I had 14 factors with two levels each: current and future. The approach was to represent each of the factors at both levels in a computer simulation, query the simulation results for one or more measure of effectiveness, and fit one or more machine learning models to the results to understand the relationship between the factors and the measures of effectiveness.\nOne approach to generate these results is to run the simulation for each unique combination of factors and their levels. 14 factors at 2 levels each would require \\(2^{14} = 16,384\\) simulation runs. A benefit of this design (called a full factorial design) is that it allows for the evaluation of main effects, 2-way interactions, 3-way interactions, and all the way up to 13-way interactions. Full factorial designs are easy to create in R using the expand.grid() function. An example for thee factors is shown below, and this is often referred to as a design matrix. Note the use of -1 and 1 to represent the two factor levels instead of 0 and 1 will be explained later. When evaluating the properties of the design itself, -1 and 1 is necessary (and will be demonstrated later), but we re-code to 0 and 1 when applying machine learning methods to the results.\n\nexpand.grid(\n  f1 = c(-1, 1),\n  f2 = c(-1, 1),\n  f3 = c(-1, 1)\n  )\n\n\n\n  \n\n\n\nFor this study, each simulation took approximately one hour of computing time, and the network was fairly small, so there were a limited number of machines on which to farm out all of these runs. Each simulation also produced approximately 10Gb of output. Unfortunately, the computer network simply didn’t have the computing power or storage space to handle all 16,384 runs.\nClearly, we needed to reduce the number of simulation runs to something feasible. That’s what an R5 design does for us, but it comes at a cost. If we were willing to give up the ability to evaluate 3-way and higher order interactions (we were), then we could drastically reduce the number of simulation runs. For example, to evaluate main effects and all 2-way interactions for 14 factors, we know we need at least \\(1 + 14 + \\left(\\begin{array}{c}14\\\\ 2\\end{array}\\right) = 106\\) simulation runs to provide the necessary degrees of freedom - a huge decrease from 16,384. We might need more than 106 runs, but not orders of magnitude more.\nSo we know we needed a design matrix with at least 106 rows, but how do we create it? Montgomery (2013) discusses a method that uses the smallest full factorial design with more rows than needed for the R5 design as a basis. The smallest full factorial design with at least 106 rows is \\(2^7 = 128\\), so we have a design matrix of size 128 x 128. From this matrix, we would then select 14 columns to represent our 14 factors. Montgomery (2013) specifies which columns to select for R5 designs but only up to 10 factors. One option for creating our design would be to continue with this approach of using full factorial designs as a basis. Dr. Sanchez’s Ruby script uses a different option, which piqued my interest. Instead of a full factorial design as the basis, his script uses a naturally ordered Walsh matrix (also referred to as a Hadamard matrix) as the basis."
  },
  {
    "objectID": "posts/res5/index.html#walsh-matrices",
    "href": "posts/res5/index.html#walsh-matrices",
    "title": "Resolution 5 Fractional Factorial Designs",
    "section": "Walsh Matrices",
    "text": "Walsh Matrices\nAn internet search will produce plenty of options to learn all about Walsh matrices, but I’ll boil that down into their properties that matter for this purpose.\n\nAs with full factorial designs, they are square with dimensions of \\(2^x\\).\nThey consist entirely of binary values.\nAll columns (and less importantly, rows) are orthogonal.\nThey are easy to generate.\n\nThe smallest Walsh matrix is 2x2 with the following values.\n\nw <- matrix(c(1, 1, 1, -1), nrow = 2)\nw\n\n     [,1] [,2]\n[1,]    1    1\n[2,]    1   -1\n\n\nThe Kronecker product of two 2x2 Walsh matrices creates a 4x4 Walsh matrix.\n\nbig_w <- w %x% w\nbig_w\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    1    1    1\n[2,]    1   -1    1   -1\n[3,]    1    1   -1   -1\n[4,]    1   -1   -1    1\n\n\nIf you’re like me and have never heard of the Kronecker product before, I’ve highlighted and divided the matrix into sections to illustrate what it does. Take the upper left value in the right hand matrix and multiply it by the entire left hand matrix. Place that matrix in the upper left quadrant of the new matrix big_w. Then cycle through each value in the right hand matrix in the same manner.\n\n\n\n\n \n  \n    V1 \n    V2 \n    V3 \n    V4 \n  \n \n\n  \n    1 \n    1 \n    1 \n    1 \n  \n  \n    1 \n    -1 \n    1 \n    -1 \n  \n  \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    1 \n    -1 \n    -1 \n    1 \n  \n\n\n\n\n\nTo create an 8x8 matrix, take the Kronecker product of the 4x4 and 2x2 Walsh matrices. Repeating this process will generate increasingly large matrices bounded only by available RAM.\n\nbig_w <- big_w %x% w\nbig_w\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    1    1    1    1    1    1    1    1\n[2,]    1   -1    1   -1    1   -1    1   -1\n[3,]    1    1   -1   -1    1    1   -1   -1\n[4,]    1   -1   -1    1    1   -1   -1    1\n[5,]    1    1    1    1   -1   -1   -1   -1\n[6,]    1   -1    1   -1   -1    1   -1    1\n[7,]    1    1   -1   -1   -1   -1    1    1\n[8,]    1   -1   -1    1   -1    1    1   -1"
  },
  {
    "objectID": "posts/res5/index.html#determine-valid-matrix-size-and-column-indices",
    "href": "posts/res5/index.html#determine-valid-matrix-size-and-column-indices",
    "title": "Resolution 5 Fractional Factorial Designs",
    "section": "Determine Valid Matrix Size and Column Indices",
    "text": "Determine Valid Matrix Size and Column Indices\nLet’s start with the smallest case possible: create an R5 design for two factors. The first step is to identify the theoretical minimum number of runs using the method described earlier: intercept + # main effects + # interactions. Then start with the smallest \\(2^x\\) Walsh matrix with at least that many rows. For two factors, that’s 4 rows, so we need a 4x4 Walsh matrix. We also know we’re going to drop the first column, so we have 3 columns to work with for our 2 factors.\n\n# minimum number of rows\n1 + 2 + choose(2, 2) \n\n[1] 4\n\n# get Walsh matrix and display only last three columns\nbig_w <- w %x% w\nbig_w\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    1    1    1\n[2,]    1   -1    1   -1\n[3,]    1    1   -1   -1\n[4,]    1   -1   -1    1\n\n\nWe don’t need to check if main effects are aliased because we know they aren’t based on the properties of Walsh matrices. We just need to check whether main effects are aliased with 2-way interactions. Later we’ll also need to check whether 2-way interactions are aliased with any other 2-way interactions. We can skip that for 2 factors since there’s only one 2-way interaction. So, we have three possible candidates for our 2 factors: columns 2, 3, and 4. Let’s just start at the left and work to the right as needed. We’ll select columns 2 and 3 and check the correlation between the two main effects with the interaction term. If no correlation exists, then we’ll keep track of the size of the matrix we tried and the indices of the columns that produced no correlation.\n\n# get the interaction between the candidate columns\nint_term <- big_w[, 2] * big_w[, 3]\n\n# check for correlation of both main effects and the interaction term\ncor(big_w[, 2], int_term)\n\n[1] 0\n\ncor(big_w[, 3], int_term)\n\n[1] 0\n\n\nSince there is no correlation, we know that for 2 factors, we can use columns 2 and 3 from a 4x4 matrix. If we weren’t able to find a combination of columns that worked, we’d increase the size of the underlying Walsh matrix and repeat the process. The pwr vector will keep track of the number of times we needs to use the Kronecker product to generate the correct sized matrix. For 2 factors and a 4x4 matrix, we use the kronecker product once, so we add a 1 to the vector at an index of 2.\n\nidx <- c(2, 3)\npwr <- c(0, 1)"
  },
  {
    "objectID": "posts/res5/index.html#adding-factors",
    "href": "posts/res5/index.html#adding-factors",
    "title": "Resolution 5 Fractional Factorial Designs",
    "section": "Adding Factors",
    "text": "Adding Factors\nNow that we know the indices and matrix size for two factors, we can use this same algorithm to successively add one more factor at a time.\n\n# minimum number of rows\n1 + 3 + choose(3, 2)\n\n[1] 7\n\n\nSo we need at least an 8x8 matrix.\n\nbig_w <- big_w %x% w\ndim(big_w)\n\n[1] 8 8\n\n\nWe’re already using columns 2 and 3, so for our third factor, we’ll start with column 4 as a candidate and check for non-zero correlation. If it fails the correlation checks, we’ll move on to column 5, and continue until we find a valid column. If we find a valid column, we’ll record the column index number.\n\nfn <- function(x){m[, x[1]] * m[, x[2]]}\n\n# loop through the candidate columns\nfor (i in (1+max(idx)):ncol(big_w)){\n  # create a new dataframe with just the columns of interest\n  m <- big_w[, c(idx, i)]\n  # check for correlation. If none found, save the column number and stop the loop.\n  if (all(colSums(cor(cbind(m[, ncol(m)], combn(1:(length(idx)+1), 2, fn)))) == 1)){\n    idx <- c(idx, i)\n    break\n  }\n}\nidx\n\n[1] 2 3 5\n\n\nLooks like column 4 failed the test but column 5 passed. We found a valid column, so we’ll also record the number of times we invoked the Kronecker product (2) in the pwr vector power at index 3 (because it corresponds with 3 factors).\n\npwr <- c(pwr, 2)"
  },
  {
    "objectID": "posts/res5/index.html#continuing-on",
    "href": "posts/res5/index.html#continuing-on",
    "title": "Resolution 5 Fractional Factorial Designs",
    "section": "Continuing On",
    "text": "Continuing On\nFrom here forward, it’s the same process applied to each new column. However, once the number of indices in the idx vector gets into the 30s, the algorithm as written starts slowing down drastically. By avoiding checking correlations that have been checked during earlier iterations and improving the efficiency of my code, I was able to generate R5 matrices for up to 70 factors. At that point, the base matrix dimensions were approximately 32,000 by 32,000 which required 8GB of RAM. My personal computer has 16GB of RAM, and so due to R’s copy on modify behavior, I was unable to create a matrix of that size. I’ve yet to be involved in a DOE study where more than 70 factors were being considered, so I’ll just accept that limitation for now. The full algorithm for up to 20 factors is shown below.\n\n# function to create a Walsh matrix of a certain size.\n# Since I iterate from 2:p, I get the size of the Walsh matrix as 2^p\nres5 <- function(fct, p){\n  a <- matrix(c(1,1,1,-1), nrow=2)\n  w <- matrix(c(1,1,1,-1), nrow=2)\n  for (i in 2:p){w <- w %x% a}\n  w\n}\n\nidx <- c(2, 3)\npwr <- c(0, 1)\n\n# function to check aliasing between main effects and two-way interactions\none_two <- function(i) {sum(m[, ncol(m)] == m[, int1[1, i]] * m[, int1[2, i]]) == nrow(m) | \n    sum(m[, ncol(m)] == -(m[, int1[1, i]] * m[, int1[2, i]])) == nrow(m)}\n\n# iterate through new factors\nfor (f in 3:20){\n  # get the minimum size of the Walsh matrix\n  new_p <- max(ceiling(log2(1 + f + choose(f, 2))), pwr[length(pwr)])\n  # try the smallest matrix size first, then try one bigger \n  for (ps in c(new_p, new_p+1)){\n    p_worked <- FALSE\n    dm <- res5(f, ps)\n    # get the various combinations of interactions\n    int1 <- combn(1:length(idx), 2)\n    int2 <- combn(1:(length(idx)+1), 2)\n    # filter out the interactions we've already checked\n    int_n <- int2[, int2[2, ]==f]\n    # iterate through the new potential matrix columns\n    for (k in (max(idx)+1):ncol(dm)){\n      m <- dm[, c(idx, k)]\n      # check new factor against 2-way interaction terms\n      keepchecking <- TRUE\n      if(any(1:ncol(int1) %>% purrr::map_lgl(function(x) one_two(x)))) {next}\n      # check 2-way interactions vs. 2-way interactions\n      if (keepchecking){\n        for (j in 1:ncol(int1)){\n          for (i in 1:ncol(int_n)){\n            if(cor(m[, int1[1, j]] * m[, int1[2, j]], m[, int_n[1, i]] * m[, int_n[2, i]]) > 0){\n              keepchecking <- FALSE\n              break\n            }\n          }\n          if (!keepchecking){break}\n        }\n      }\n      if (keepchecking){\n        idx <- c(idx, k)\n        pwr <- c(pwr, ps)\n        p_worked <- TRUE\n        break}\n    }\n    if (p_worked){break}\n  }\n}\n\nThe column indices are:\n\nidx\n\n [1]   2   3   5   9  16  17  33  52  65  86 107 129 151 172 220 238 248 257 280\n[20] 298\n\n\nAnd the associated sizes of the Walsh matrices are:\n\npwr\n\n [1] 0 1 3 4 4 5 6 6 7 7 7 8 8 8 8 8 8 9 9 9\n\n\nNow we have everything needed to create R5 designs for up to 20 factors. We’ll wrap everything up in a new function and replace all -1’s with 0’s for convenience. Then we’ll display the first few rows of the design matrix for 14 factors.\n\nr5 <- function(fct){\n  idx <- c(2, 3, 5, 9, 16, 17, 33, 52, 65, 86, 107, 129, 151, 172, 220, \n           238, 248, 257, 280, 298)\n  pwr <- c(0, 1, 3, 4, 4, 5, 6, 6, 7, 7, 7, 8, 8, 8, 8, 8, 8, 9, 9, 9)\n  \n  a <- matrix(c(1,1,1,-1), nrow=2)\n  w <- matrix(c(1,1,1,-1), nrow=2)\n  for (i in 2:pwr[fct]){w <- w %x% a}\n  # select only the columns we need\n  w <- w[, idx[1:fct]]\n  # replace -1 with 0\n  w[w == -1] <- 0\n  w\n}\n\nhead(r5(14))\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n[1,]    1    1    1    1    1    1    1    1    1     1     1     1     1     1\n[2,]    0    1    1    1    0    1    1    0    1     0     1     1     1     0\n[3,]    1    0    1    1    0    1    1    0    1     1     0     1     0     0\n[4,]    0    0    1    1    1    1    1    1    1     0     0     1     0     1\n[5,]    1    1    0    1    0    1    1    1    1     0     1     1     0     1\n[6,]    0    1    0    1    1    1    1    0    1     1     1     1     0     0\n\n\nThe number of runs required for this design is simply nrow(r5(14)) = 256.\nI am currently working with a colleague to develop an R package that uses the methodology described above to generate Resolution III, V, and VII fractional factorial designs. The package will also provide the ability to generate nearly orthogonal Latin hypercube designs."
  },
  {
    "objectID": "posts/simple_rating_system/index.html",
    "href": "posts/simple_rating_system/index.html",
    "title": "Simple Rating System",
    "section": "",
    "text": "In my last post, I was playing around with data from an API offered by http://collegefootballdata.com.\nThen I found some blog posts on the site, and I thought a couple of them on how Simple Rating Systems work were interesting.\nIn particular, this one.\nThey did all their coding in Python, but I’m becoming more and more of an R fan, so I thought I’d re-code the wheel, so to speak.\nIn their post, they used data from the 2019 season, so for consistency, I’ll do the same.\nAlso, in the last post, I was using tidyr to un-nest JSON data.\nIt seemed add to me that you have to do these repeated unnest_auto() steps to get things parsed out.\nIf it’s automatic, why do I have to keep doing it manually?\nAfter some more Googling, I found the tidyjson package, which has a nice spread_all function that I’ll use instead."
  },
  {
    "objectID": "posts/simple_rating_system/index.html#simple-rating-system-the-math",
    "href": "posts/simple_rating_system/index.html#simple-rating-system-the-math",
    "title": "Simple Rating System",
    "section": "Simple Rating System: The Math",
    "text": "Simple Rating System: The Math\nThis, to me, is the coolest part.\nI had no idea this is how some rating systems work, and it’s pretty slick.\nIt’s just one big system of equations that you solve with regular ’ol linear algebra.\nIn other words, solve \\(Ax=b\\).\nThat’s it.\nI’ll start with the \\(b\\) vector - that’s easiest to explain.\n\nThe b Vector\nThe \\(b\\) vector is each team’s average margin of victory for the season.\nCouldn’t be any simpler.\n\n\nThe A Matrix\nThis is a little more complicated.\nThe \\(A\\) matrix will have dimensions of 130x130 - one row and column for each FBS team.\nThe diagonal will be 1’s (i.e., the identity matrix).\nThink of the rest of the matrix in terms of rows.\nWe’ll set it up alphabetically, so the first row will be for Air Force.\nFirst, we’ll count how many games Air Force played that season.\nThen we’ll identify all of Air Force’s opponents - those are the columns.\nAs I said, the Air Force-Air Force entry will have a 1.\nMoving across the columns, if Air Force didn’t play that team, put a 0 there.\nIf they did, divide the number of times Air Force played that team by the total number of games played and put that value in the column.\nKeep doing that until you get to the last column (i.e., that last potential match-up).\nThen repeat that process for the next team, Akron, and then the next, etc.\nThat’s it.\nThis matrix represents each team’s strength of schedule.\nPretty clever, right?\nA teams rating is it’s mean margin of victory adjusted by it’s strength of schedule."
  },
  {
    "objectID": "posts/simple_rating_system/index.html#the-code",
    "href": "posts/simple_rating_system/index.html#the-code",
    "title": "Simple Rating System",
    "section": "The Code",
    "text": "The Code\nFirst we need to get all the FBS team names so we can exclude non-FBS games.\n\nlibrary(tidyjson)\nlibrary(dplyr)\nlibrary(httr)\nfbs <-\n  httr::GET(\n    url = \"https://api.collegefootballdata.com/teams/fbs?year=2019\",\n    httr::add_headers(\n      Authorization = paste(\"Bearer\", Sys.getenv(\"CFDB_API_TOKEN\"))\n    )\n  )\n\nfbs_teams <- \n  httr::content(fbs, \"parsed\") %>% # convert response to a nested list\n  spread_all %>% # rectangularize nested list into a dataframe\n  arrange(school) # make sure teams are in alphabetical order\n\nNow we’ll get team win-loss records.\n\nrecords <- \n  httr::GET(\n  url = \"https://api.collegefootballdata.com/games?year=2019\",\n  httr::add_headers(\n    accept = \"application/json\",\n    Authorization = paste(\"Bearer\", Sys.getenv(\"CFDB_API_TOKEN\"))\n  )\n)\n\nteam_records <- \n  httr::content(records, \"parsed\")\n\nteam_records <- tibble(data=team_records)\n\nteam_records <- \n  team_records %>% \n  tidyr::unnest_auto(data)\n\nNow get scores and margin of victory for each game and eliminate non-FBS games.\nEventually we’ll use this for the \\(b\\) vector, but first we’ll need it in this format for the \\(A\\) matrix.\n\nscores <- team_records %>% \n  filter(home_team %in% (fbs_teams %>% .$school) & \n           away_team %in% (fbs_teams %>% .$school)) %>%\n  select(home_team, away_team, home_points, away_points) %>%\n  mutate(home_mov = home_points - away_points)\nhead(scores)\n\n\n\n  \n\n\n\nOk, now we can start to generate the \\(A\\) matrix.\nFirst, I’ll populate it with the number of times each team faced each other.\nThere’s probably a more elegant way, but this is what came to me first.\n\nA <- data.frame(diag(0, nrow=130, ncol=130), \n                row.names = fbs_teams %>% .$school)\ncolnames(A) <- fbs_teams %>% .$school\n\n# populate dataframe with \nfor (r in 1:nrow(scores)){\n  home <- scores[r, 1] %>% .$home_team\n  away <- scores[r, 2] %>% .$away_team\n  A[home, away] <- A[home, away] + 1\n  A[away, home] <- A[away, home] + 1\n}\n\n# clean up\nrm(away, home, r)\n\nA[1:6, 1:6]\n\n\n\n  \n\n\n\nHold that thought on the \\(A\\) matrix - we need a little more work to proceed.\nNext, rearrange the scores data to get one margin of victory score for each team and each game.\n\nmov <- scores %>% \n  select(home_team, home_mov) %>% \n  rename(team = home_team, mov = home_mov) %>% \n  bind_rows(scores %>% \n              select(away_team, home_mov) %>% \n              rename(team = away_team, mov = home_mov) %>% \n              mutate(mov = -mov))\n\nNow count the total number of games each team played.\n\nn_games <- mov %>% count(team) %>% .$n\n\nMultiply \\(A\\)’s columns by \\(1 / n_games\\).\nMARGIN=1 specifies to sweep across columns.\n\nA <- sweep(A, 1/n_games, MARGIN=1, FUN =`*`)\n\nFinally, add the identity matrix and \\(A\\) is built.\n\nA <- A + diag(1, nrow=130, ncol=130)\nA[1:6, 1:6]\n\n\n\n  \n\n\n\nNow calculate the mean margin of victory for each team.\nThis is the \\(b\\) vector for the system of equations.\n\nb <- \n  mov %>%\n  group_by(team) %>% \n  summarize(mean_mov = mean(mov)) %>%\n  .$mean_mov\n\nIt took a while to build the system of equations, but solving it is a one-liner.\n\nsolve(A, b)\n\n            Air Force                 Akron               Alabama \n          12.17488262          -22.66408637           32.60720600 \n    Appalachian State               Arizona         Arizona State \n          21.80515687          -13.03736947            1.10205537 \n             Arkansas        Arkansas State                  Army \n         -24.46835876           -0.52637216            1.05006201 \n               Auburn            Ball State                Baylor \n           5.88613703            0.40799928           14.36295601 \n          Boise State        Boston College         Bowling Green \n          17.32564683           -2.78480549          -33.10414546 \n              Buffalo                   BYU            California \n          10.20151104            0.46092289           -6.46928285 \n     Central Michigan             Charlotte            Cincinnati \n          10.31277888           -4.34284716            7.36008829 \n              Clemson      Coastal Carolina              Colorado \n          41.48627376            0.83014089          -11.45324149 \n       Colorado State           Connecticut                  Duke \n          -2.51297888          -22.92597291          -10.33694277 \n        East Carolina      Eastern Michigan               Florida \n         -12.21872118           -2.75282448           12.69160016 \n     Florida Atlantic Florida International         Florida State \n           8.62463543            1.37276246           -6.07744689 \n         Fresno State               Georgia      Georgia Southern \n           0.50823966           15.23789346           -1.56619791 \n        Georgia State          Georgia Tech               Hawai'i \n          -2.53872623          -20.00376049            0.03002992 \n              Houston              Illinois               Indiana \n         -11.22633847            5.83248366            6.84673009 \n                 Iowa            Iowa State                Kansas \n          10.19598383            8.32429892          -17.55664637 \n         Kansas State            Kent State              Kentucky \n           9.86298135           -0.90629641           11.16790944 \n              Liberty             Louisiana      Louisiana Monroe \n          11.38026279           14.12089636          -10.96010135 \n       Louisiana Tech            Louisville                   LSU \n          19.83837303          -11.11555104           24.61782878 \n             Marshall              Maryland               Memphis \n          -4.53506310          -21.13126417           16.67762020 \n                Miami            Miami (OH)              Michigan \n           3.40729287           -9.17435086            9.40439559 \n       Michigan State      Middle Tennessee             Minnesota \n          -2.84667123           -5.41633827           14.54830499 \n    Mississippi State              Missouri                  Navy \n         -10.04679640            5.12551526           14.51847575 \n             NC State              Nebraska                Nevada \n         -10.13346295           -1.06634224          -10.70442169 \n           New Mexico      New Mexico State        North Carolina \n         -18.96441796          -26.88151749            1.61200847 \n          North Texas     Northern Illinois          Northwestern \n          -1.33040741           -5.75767308           -9.39415805 \n           Notre Dame                  Ohio            Ohio State \n          21.22221810           10.59616933           36.35622851 \n             Oklahoma        Oklahoma State          Old Dominion \n          15.82512098            3.23809017          -14.57638255 \n             Ole Miss                Oregon          Oregon State \n          -0.47404615           20.18886699           -6.91414098 \n           Penn State            Pittsburgh                Purdue \n          13.07833337           -4.24030653           -4.44792009 \n                 Rice               Rutgers       San Diego State \n          -7.94447288          -27.74865910           11.75886571 \n       San José State                   SMU         South Alabama \n          -0.51222213           14.52255973          -17.50751350 \n       South Carolina         South Florida  Southern Mississippi \n         -20.16977658          -15.55091539           -3.01895055 \n             Stanford              Syracuse                   TCU \n         -10.41277836           -5.93538233           -1.45957680 \n               Temple             Tennessee                 Texas \n           2.25488829           -4.03217697            1.54246752 \n            Texas A&M           Texas State            Texas Tech \n           0.24533392          -19.98074165           -2.56724537 \n               Toledo                  Troy                Tulane \n          -9.23735051           -1.27992009           -1.44073888 \n                Tulsa                   UAB                   UCF \n          -9.98788905            7.89737667           22.70582261 \n                 UCLA                 UMass                  UNLV \n          -9.73407669          -28.89607277          -12.08819590 \n                  USC        UT San Antonio                  Utah \n           3.58662283          -18.39429259           20.89793643 \n           Utah State                  UTEP            Vanderbilt \n          -9.99000619          -13.81130518          -22.07917803 \n             Virginia         Virginia Tech           Wake Forest \n           1.72160891            8.58302730            1.14879646 \n           Washington      Washington State         West Virginia \n           8.68703548            7.56504382          -12.05131807 \n     Western Kentucky      Western Michigan             Wisconsin \n          11.04805953           10.09365243           11.70814611 \n              Wyoming \n          10.07670372 \n\n\nIf you’re familiar with linear models in R, this bit of code does the same thing.\nDon’t forget to include a -1 to drop the intercept term.\n\nlm_A <- cbind(A, b)\ncoefficients(lm(b ~ . -1 , data=lm_A))\n\n            `Air Force`                   Akron                 Alabama \n            12.17488262            -22.66408637             32.60720600 \n    `Appalachian State`                 Arizona         `Arizona State` \n            21.80515687            -13.03736947              1.10205537 \n               Arkansas        `Arkansas State`                    Army \n           -24.46835876             -0.52637216              1.05006201 \n                 Auburn            `Ball State`                  Baylor \n             5.88613703              0.40799928             14.36295601 \n          `Boise State`        `Boston College`         `Bowling Green` \n            17.32564683             -2.78480549            -33.10414546 \n                Buffalo                     BYU              California \n            10.20151104              0.46092289             -6.46928285 \n     `Central Michigan`               Charlotte              Cincinnati \n            10.31277888             -4.34284716              7.36008829 \n                Clemson      `Coastal Carolina`                Colorado \n            41.48627376              0.83014089            -11.45324149 \n       `Colorado State`             Connecticut                    Duke \n            -2.51297888            -22.92597291            -10.33694277 \n        `East Carolina`      `Eastern Michigan`                 Florida \n           -12.21872118             -2.75282448             12.69160016 \n     `Florida Atlantic` `Florida International`         `Florida State` \n             8.62463543              1.37276246             -6.07744689 \n         `Fresno State`                 Georgia      `Georgia Southern` \n             0.50823966             15.23789346             -1.56619791 \n        `Georgia State`          `Georgia Tech`               `Hawai'i` \n            -2.53872623            -20.00376049              0.03002992 \n                Houston                Illinois                 Indiana \n           -11.22633847              5.83248366              6.84673009 \n                   Iowa            `Iowa State`                  Kansas \n            10.19598383              8.32429892            -17.55664637 \n         `Kansas State`            `Kent State`                Kentucky \n             9.86298135             -0.90629641             11.16790944 \n                Liberty               Louisiana      `Louisiana Monroe` \n            11.38026279             14.12089636            -10.96010135 \n       `Louisiana Tech`              Louisville                     LSU \n            19.83837303            -11.11555104             24.61782878 \n               Marshall                Maryland                 Memphis \n            -4.53506310            -21.13126417             16.67762020 \n                  Miami            `Miami (OH)`                Michigan \n             3.40729287             -9.17435086              9.40439559 \n       `Michigan State`      `Middle Tennessee`               Minnesota \n            -2.84667123             -5.41633827             14.54830499 \n    `Mississippi State`                Missouri                    Navy \n           -10.04679640              5.12551526             14.51847575 \n             `NC State`                Nebraska                  Nevada \n           -10.13346295             -1.06634224            -10.70442169 \n           `New Mexico`      `New Mexico State`        `North Carolina` \n           -18.96441796            -26.88151749              1.61200847 \n          `North Texas`     `Northern Illinois`            Northwestern \n            -1.33040741             -5.75767308             -9.39415805 \n           `Notre Dame`                    Ohio            `Ohio State` \n            21.22221810             10.59616933             36.35622851 \n               Oklahoma        `Oklahoma State`          `Old Dominion` \n            15.82512098              3.23809017            -14.57638255 \n             `Ole Miss`                  Oregon          `Oregon State` \n            -0.47404615             20.18886699             -6.91414098 \n           `Penn State`              Pittsburgh                  Purdue \n            13.07833337             -4.24030653             -4.44792009 \n                   Rice                 Rutgers       `San Diego State` \n            -7.94447288            -27.74865910             11.75886571 \n       `San José State`                     SMU         `South Alabama` \n            -0.51222213             14.52255973            -17.50751350 \n       `South Carolina`         `South Florida`  `Southern Mississippi` \n           -20.16977658            -15.55091539             -3.01895055 \n               Stanford                Syracuse                     TCU \n           -10.41277836             -5.93538233             -1.45957680 \n                 Temple               Tennessee                   Texas \n             2.25488829             -4.03217697              1.54246752 \n            `Texas A&M`           `Texas State`            `Texas Tech` \n             0.24533392            -19.98074165             -2.56724537 \n                 Toledo                    Troy                  Tulane \n            -9.23735051             -1.27992009             -1.44073888 \n                  Tulsa                     UAB                     UCF \n            -9.98788905              7.89737667             22.70582261 \n                   UCLA                   UMass                    UNLV \n            -9.73407669            -28.89607277            -12.08819590 \n                    USC        `UT San Antonio`                    Utah \n             3.58662283            -18.39429259             20.89793643 \n           `Utah State`                    UTEP              Vanderbilt \n            -9.99000619            -13.81130518            -22.07917803 \n               Virginia         `Virginia Tech`           `Wake Forest` \n             1.72160891              8.58302730              1.14879646 \n             Washington      `Washington State`         `West Virginia` \n             8.68703548              7.56504382            -12.05131807 \n     `Western Kentucky`      `Western Michigan`               Wisconsin \n            11.04805953             10.09365243             11.70814611 \n                Wyoming \n            10.07670372 \n\n\nTo visualize the ratings, let’s make a plot of the top 25.\n\nlibrary(ggplot2)\nlibrary(forcats)\n\nsrs <- \n  tibble(team = fbs_teams$school,\n         rating = solve(A, b),\n         color = fbs_teams$color)\n\ntop_25 <- \n  srs %>% \n  arrange(desc(rating)) %>% \n  slice(1:25)\n\nggplot() +\n  geom_col(data = top_25, \n           aes(x = fct_reorder(team, rating), y = rating), \n           fill = top_25$color) +\n  coord_flip() +\n  theme_bw() +\n  ylab(\"Rating\") +\n  xlab(\"Team\")\n\n\n\n\nIn the College Football Data blog, they further refine the rating by factoring in home field advantage, conference strength, and things like that.\nThat’s fine, but I just wanted to get the basic mechanics down."
  },
  {
    "objectID": "posts/tripeaks/index.html",
    "href": "posts/tripeaks/index.html",
    "title": "Tripeaks Solver",
    "section": "",
    "text": "During COVID-19 brain fog, I had just enough mental capacity to play a lot of Microsoft Solitaire games. I found that TriPeaks on master and grandmaster difficulty gave me more trouble than the others. You have to find just the right combination of not clearing cards from the board, and at times, it would take me an annoyingly long time to find that magical combination. As the brain fog cleared, I decided to write some code to take care of those pesky games, and this is it.\nTo address the elephant in the room: yes, this is completely cheating! On the other hand,\nAnyway, the method I came up with requires me to input all of the cards manually, which means I have to at least expose all of the cards, write them all down, and type them into my console. The point is, this will solve a board, but it won’t win any speed contests."
  },
  {
    "objectID": "posts/tripeaks/index.html#the-algorithm",
    "href": "posts/tripeaks/index.html#the-algorithm",
    "title": "Tripeaks Solver",
    "section": "The Algorithm",
    "text": "The Algorithm\nMy first thought about how to go about this was to try and keep track of all possible branches of possible moves and grind through them all until I found a solution. That seemed like a lot of work, so I thought about a different approach. As I clear a board the first time, I have a general strategy, but with the more difficult levels, I almost never clear the board on the first attempt. It feels a bit random about which cards to clear or leave on the board for later, so why not embrace the randomness? Instead of keeping track of all possible moves, I just randomly select among the available moves (including not making a move at all). Depending on the level of randomness, it turns out, this method can clear a board in 20 attempts, on average. A lot less than I expected!\nLet’s get to it. The first thing we need are the cards, so I create two vectors: one for the stock cards at the bottom stock, and one for the main board board_orig. For the stock, I put the cards in order from top to bottom (or, from right to left based on the Microsoft Solitaire layout). For the board, I start at the lower left, and go left to right across the bottom row, then left to right across each successively higher row.\n\n# right to left (i.e., top to bottom)\nstock <- c(3,9,12,2,2,8,3,10,11,10,6,13,7,12,11,12,1,11,5,8,6,3,6,11)\n\n# by row, starting at the lower left\nboard_orig <- c(\n  5, 9, 1, 2, 7, 4, 13, 9, 7, 3,\n  5, 6, 8, 7, 4, 9, 4, 13, 13, \n  2, 10, 12, 5, 8, 1, \n  1, 10, 4\n)\n\nBefore I get to the main script, I wrote one function get_viz_idx() that takes the board vector as input and returns a vector of the board indices that are currently visible. As I mention below, when a card from the board is cleared, I replace it with a 0 in the board vector. I take advantage of that to determine what the remaining cards are. Then, starting with the bottom row, I see what cards remain in that row. Based on what remains in the bottom row, I can then determine what cards are visible in the next higher row. I apply that same logic to the 3rd and 4th rows and then return a vector containing the indices of visible cards.\n\nget_viz_idx <- function(bd){\n  remain <- bd != 0          # boolean for the remaining cards\n  r1 <- which(remain[1:10])  # indices of remaining bottom row (row 1) cards\n  r2 <- c()                  # second from bottom row, etc.\n  r3 <- c()\n  r4 <- c()\n  \n  r2_filter <- remain[1:10] # check row 1 to see what's visible in row 2\n  for (i in 1:(length(r2_filter)-1)){\n    if(!r2_filter[i] & !r2_filter[i+1] & remain[i+10] != 0){\n      r2 <- c(r2, i+10)}\n  }\n  r3_filter <- remain[11:19] # check row 2 to see what's visible in row 3\n  for (i in 1:(length(r3_filter)-1)){\n    if(!r3_filter[i] & !r3_filter[i+1] & remain[i+19] != 0 & i <= 2){\n      r3 <- c(r3, i+19)}\n    if(i == 3 | i == 6) next\n    if(!r3_filter[i] & !r3_filter[i+1] & remain[i+18] != 0 & (i == 4 | i == 5)){\n      r3 <- c(r3, i+18)}\n    if(!r3_filter[i] & !r3_filter[i+1] & remain[i+17] != 0 & i > 6){\n      r3 <- c(r3, i+17)}\n  }\n  r4_filter <- remain[20:25] # check row 3 to see what's visible in row 4\n  for (i in c(1,3,5)){\n    if(!r4_filter[i] & !r4_filter[i+1] & remain[i+25] != 0 & i == 1){\n      r4 <- c(r4, i+25)}\n    if(!r4_filter[i] & !r4_filter[i+1] & remain[i+24] != 0 & i == 3){\n      r4 <- c(r4, i+24)}\n    if(!r4_filter[i] & !r4_filter[i+1] & remain[i+23] != 0 & i == 5){\n      r4 <- c(r4, i+23)}\n  }\n  viz <- c(r1, r2, r3, r4) # combine indices of visible cards into one vector\n  viz\n}\n\nFor the main script, I put everything in a nested while() loop. Each pass through the outer loop is an attempt at solving the game. As I mentioned, when a card is cleared from the board, I change the card to a 0, so the loop keeps going as long as there are less than 28 zeros in the board vector. I also added a counter count to keep track of how many attempts have been made. If it reaches 1000, then I’m fairly certain the board is unsolvable, and the loop will stop.\nI keep track of a game’s history with history(). If the next move is to remove a card from the stock, I add a 0. Otherwise, I add the index of the board vector cleared.\nThe inner loop will stop when there are no stock cards left, and the comments should explain the rest of the logic.\n\nboard <- board_orig  # make a copy of the original board.\ncount <- 0           # attempt counter\n\nwhile (sum(board==0) != 28 & count <= 1000){\n  \n  board <- board_orig\n  history <- c()    # 0 if draw from stock, else index of board played\n  idx_played <- c() # helps keeps track of what cards are visible\n  stock_count <- 1  # the current stock card index  \n  card <- stock[stock_count] # the current card being considered for a move\n  \n  while(stock_count <= 24){\n    if(is.null(idx_played)){ # at the start of a game, \n      viz_idx <- 1:10        # the visible board indices are 1:10\n    }else{ # otherwise call a function to get the visible board indices\n        viz_idx <- get_viz_idx(board)\n        } \n    \n    # get indices for all valid moves\n    if (card == 1) {  # allows for wrap-aound if card is an Ace\n      poss_next <- \n        viz_idx[c(which(board[viz_idx] == card + 12), \n                  which(board[viz_idx] == card + 1))]}\n    if (card == 13) { # allows for wrap-aound if card is a King\n      poss_next <-\n        viz_idx[c(which(board[viz_idx] == card - 12), \n                  which(board[viz_idx] == card - 1))]}\n    if (card > 1 & card < 13) {\n      poss_next <- \n        viz_idx[c(which(board[viz_idx] == card - 1), \n                  which(board[viz_idx] == card + 1))]}\n    \n    # if no available card to remove or randomly choose not to clear it,\n    # then advance to next stock card\n    if (length(poss_next) == 0 | runif(1) > 0.99){ \n      stock_count <- stock_count + 1\n      history <- c(history, 0)\n      card <- stock[stock_count]\n    }else{\n      next_idx <- ifelse(length(poss_next) == 1, poss_next, sample(poss_next, 1))\n      idx_played <- c(idx_played, next_idx)\n      history <- c(history, next_idx)\n      card <- board[next_idx]\n      board[next_idx] <- 0\n    }\n  }\n  count <- count + 1\n}\n\nhistory\n\n [1]  6 10  4  3  7  0 13  9  0 19  0  0  0  5  0 15  1  0  2  0  0  8  0 14  0\n[26] 22 18 25  0 12 11 17  0  0 21 16 24  0  0 20 26  0  0 28 23  0  0  0  0  0\n[51] 27  0\n\n\nSo there’s a solution! Clear as mud, right? Recall that a 0 in history means that a stock card was played, so I can replace those with stock vector. If it’s not a zero, it represents the index of the board_orig vector that was cleared. I need to put that all together and clean things up so I can make a clean visualization of how to win the game."
  },
  {
    "objectID": "posts/tripeaks/index.html#results-cleanup",
    "href": "posts/tripeaks/index.html#results-cleanup",
    "title": "Tripeaks Solver",
    "section": "Results Cleanup",
    "text": "Results Cleanup\nI’m going to need some help from dplyr for data wrangling.\n\nlibrary(dplyr)\n\nThe first thing I’m going to do is make a dataframe df for the cleaned up results. The card column contains all of the cards from the stock and board vectors. The pile column just indicates which of the two vectors the card came from, and index is simple the indices of the original two vectors.\n\nboard <- board_orig\n\ndf <- tibble(\n  card = c(stock, board),\n  pile = c(rep(\"stock\", length(stock)), rep(\"board\", length(board))),\n  index = c(1:24, 1:28)\n)\n\nhead(df)\n\n\n\n  \n\n\n\nI then create a dataframe h to contain the history vector and which “pile”, or vector, the history entry refers to. I replace all of the 0s (the stock cards) with the order in which they appeared in stock, and then I join the two dataframes together.\n\nh <- tibble(\n  index = history,\n  order = 1:52,\n  pile = ifelse(index == 0, \"stock\", \"board\")\n)\n\nh[h$index==0, \"index\"] <- 1:sum(h$index==0)\n\ndf <- df |> left_join(h, by = c(\"pile\", \"index\"))\n\nhead(df)"
  },
  {
    "objectID": "posts/tripeaks/index.html#visualization",
    "href": "posts/tripeaks/index.html#visualization",
    "title": "Tripeaks Solver",
    "section": "Visualization",
    "text": "Visualization\nWhat I want to visualize is something like the game itself, so I found some .png files for each of the cards. Since suits don’t matter in TriPeaks, I ignored them from the beginning. It might look a little funny, but I’m just going to display spades for each card. Here I match the card rank with the card image.\n\ndf <-\n  df |> mutate(\n  img = case_when(\n    card == 1 ~ \"playing_cards/ace_of_spades.png\",\n    card == 2 ~ \"playing_cards/2_of_spades.png\",\n    card == 3 ~ \"playing_cards/3_of_spades.png\",\n    card == 4 ~ \"playing_cards/4_of_spades.png\",\n    card == 5 ~ \"playing_cards/5_of_spades.png\",\n    card == 6 ~ \"playing_cards/6_of_spades.png\",\n    card == 7 ~ \"playing_cards/7_of_spades.png\",\n    card == 8 ~ \"playing_cards/8_of_spades.png\",\n    card == 9 ~ \"playing_cards/9_of_spades.png\",\n    card == 10 ~ \"playing_cards/10_of_spades.png\",\n    card == 11 ~ \"playing_cards/jack_of_spades.png\",\n    card == 12 ~ \"playing_cards/queen_of_spades.png\",\n    card == 13 ~ \"playing_cards/king_of_spades.png\"\n  )\n)\nhead(df)\n\n\n\n  \n\n\n\nThis next bit took a fair amount of trial and error. What I’m doing is creating x and y columns that will be used as coordinates for where each card goes on the board. Then I make sure the dataframe is arranged by the order in which the cards were played.\n\ndf <-\n  df |> mutate(\n  x = c(24:1, 1:10*1.1, (1:9+0.5)*1.1, c(2,3,5,6,8,9)*1.1, c(2.5, 5.5, 8.5)*1.1),\n  y = c(rep(0.98, 24), \n        rep(1.02, 10),\n        rep(1.04, 9),\n        rep(1.06, 6), \n        rep(1.08, 3))\n)\n\ndf <- df |> arrange(order)\n\nhead(df)\n\n\n\n  \n\n\n\nNow I put it all together to make the visual with ggplot for the base graphics, and I use ggimage to render the cards. The number on top of each card is the order in which the card should be cleared from the board.\n\nlibrary(ggplot2)\nlibrary(ggimage)\n\nggplot(df) +\n  geom_image(aes(x=x, y=y, image=img), size = 0.038, by = \"width\", asp = 2) +\n  geom_label(aes(x=x, y=y, label=order), color=\"red\", \n             fontface=\"bold\", label.size=1) +\n  theme_void() +\n  ylim(0.96, 1.12) +\n  theme(aspect.ratio = 1/2,\n        panel.background = element_rect(fill = 'forestgreen', color = 'forestgreen'))\n\n\n\n\nI made a Shiny app that incorporates everything up to this point. For this post, I take it one more level by animating the solution with gganimate. First, though, I need to create a dataframe, df2, that is structured to support the animation. The first “frame” of the animation will display all of the cards. The second frame will show the same thing minus the first cleared card, and so on. So, I need a dataframe with \\(52 + 51 + 50 + ... + 1 = 1378\\) rows. The map_dfr() function from the purrr package is a nice tool for this. It will bind a 52-row dataframe with a 51-row dataframe with a 50-row dataframe, and so on to produce exactly what I need.\n\nlibrary(purrr)\ndf2 <- 1:52 |> map_dfr(~ df |> slice(.x:52) |> mutate(frame=.x))\nhead(df2)\n\n\n\n  \n\n\n\nFinally, I use gganimate to create an animated gif as follows.\n\nlibrary(gganimate)\n\nanim <- \n  ggplot(df2) +\n  geom_image(aes(x=x, y=y, image=img), size = 0.038, by = \"width\", asp = 2) +\n  theme_void() +\n  ylim(0.96, 1.12) +\n  theme(aspect.ratio = 1/2,\n        panel.background = element_rect(fill = 'forestgreen', \n                                        color = 'forestgreen')) +\n  transition_time(frame)\n\nanimate(anim, nframes=52, duration=26, width=6, height=3, units=\"in\", res=300)"
  },
  {
    "objectID": "posts/oregon_football/index.html",
    "href": "posts/oregon_football/index.html",
    "title": "Oregon Football 2021",
    "section": "",
    "text": "I’m not a huge football fan. I can’t even remember the last time I watched an NFL game. However, I do sort of follow college football. And being from Oregon, I root for whatever team from Oregon is playing. Sorry Beaver fans, but for quite a few years the Ducks have given me more to root for. The departure of Oregon’s head coach Mario Cristobal at the end of the 2021 season… Well, almost the end. He bolted before Oregon’s bowl game. Anyway, the whole season got me thinking about changes in college football over the years. It seemed like there were a lot of coaches that moved around. It also seemed like there were a lot more players that entered the transfer portal. Well, time to get some data and find some answers.\nAfter some poking around, I found what looked like a great source for college football data at http://collegefootballdata.com. The offer a free API, too, which piqued my interest. I haven’t done anything with an API since grad school, so this was a good opportunity to re-learn forgotten skills. So. What to look into first?"
  },
  {
    "objectID": "posts/oregon_football/index.html#oregons-quarterback",
    "href": "posts/oregon_football/index.html#oregons-quarterback",
    "title": "Oregon Football 2021",
    "section": "Oregon’s Quarterback",
    "text": "Oregon’s Quarterback\nQuite a few fans were not happy with Oregon’s starting quarterback, Anthony Brown. See this article, for example. The sentiment seemed to be that he was an ok quarterback, but just ok. Fans wanted see if the freshman and 4-star recruit Ty Thompson could do any better. Was this criticism of Brown justified? Let’s take a look.\nI’m going to need some data on Anthony Brown’s performance, so let’s see what we can get via the API. If we go to their API docs page, the games/players/ data looks promising. To read the data into an R session, I’m going to use the GET() function from httr package. I want to grab all player data from the 2021 regular season for games in which a PAC-12 team played, so I need to build the URL accordingly. To do that, I add the base URL for the API, “https://api.collegefootballdata.com/”. Then the data set “games/players”. And then the filters “?year=2021&seasonType=regular&conference=PAC”. To use the API, I need to request an API token and include the word “Bearer” followed by my token. I do that with the add_headers argument. Notice I stored by API token in my .Renviron file and retrieved it with Sys.getenv() so that my token isn’t just printed in this post. So here we go.\n\nlibrary(httr)\n\npac12 <- httr::GET(\n  url = \"https://api.collegefootballdata.com/games/players?year=2021&seasonType=regular&conference=PAC\", \n  httr::add_headers(\n    Authorization = paste(\"Bearer\", Sys.getenv(\"CFDB_API_TOKEN\"))\n  )\n)\n\nGreat! So What do I do with this thing? Well, I can check to see if I got an error or not from my request.\n\nstatus_code(pac12)\n\n[1] 200\n\n\nA status code of 200 is good. 400 means there was an error of some kind. Maybe a typo in the URL, for example. Ok, but still. What is this pac12 thing?\n\nclass(pac12)\n\n[1] \"response\"\n\n\nIt’s not a data frame or JSON or some kind of data structure, it’s a response object. So now I need to get it into a data structure. I can do that with the content() function from httr.\n\npac12_parsed <- content(pac12, \"parsed\")\n\nGreat! So NOW what do I have to work with?\n\nclass(pac12_parsed)\n\n[1] \"list\"\n\n\nA list. Specifically, it’s JSON. Better, but still not what I want to work with. Turns out there are a number of packages and functions to work with JSON, and the first package I came across with the functions I’ll need is tidyr. According to the docs, the idea is to “rectangularize” the JSON into a tibble, so the first step is to create a tibble with a single column named data and then begin un-nesting the tibble to get at the underlying data.\n\nlibrary(tibble)\nlibrary(tidyr)\n\np12 <- tibble(data = pac12_parsed)\np12 <- p12 %>% unnest_auto(data)\nhead(p12)\n\n\n\n  \n\n\n\nunnest_auto is handy when you don’t know the JSON structure. In this case, it applied unnest_wider() to create a tibble with now two columns. We can see that the new column teams is a nested list, so now we need to un-nest that column.\n\np12 <- p12 %>% unnest_auto(teams)\nhead(p12)\n\n\n\n  \n\n\n\nOk, so it applied unnest_longer(), but teams is still a nested list - only now there are 5 things in the lists instead of 2, so I guess we need to keep at it.\n\np12 <- p12 %>% unnest_auto(teams)\nhead(p12)\n\n\n\n  \n\n\n\nNow we’re getting somewhere. The categories column is next.\n\np12 <- p12 %>% unnest_auto(categories)\nhead(p12)\n\n\n\n  \n\n\n\nGotta do it again.\n\np12 <- p12 %>% unnest_auto(categories)\nhead(p12)\n\n\n\n  \n\n\n\nAh, there’s a passing category. I’ll filter out all of the others and then un-nest types.\n\nlibrary(dplyr)\n\np12 <- p12 %>% filter(name == \"passing\") %>% unnest_auto(types)\nhead(p12)\n\n\n\n  \n\n\n\nOne more time.\n\np12 <- p12 %>% unnest_auto(types)\nhead(p12)\n\n\n\n  \n\n\n\nNow dig into athletes. We’ll need to un-nest it twice again.\n\np12 <- p12 %>% unnest_auto(athletes) %>% unnest_auto(athletes)\nhead(p12)\n\n\n\n  \n\n\n\nI don’t see any more nested columns, so our un-nesting is done. That seemed like a lot of work, but at least it looks like we have some good stats to work with. Since we had some duplicate column names, things got messy, so I’ll clean them up.\n\ncolnames(p12) <- c(\"game_id\", \"school\", \"conference\", \"homeAway\", \"points\", \n                  \"category\", \"stat\", \"player_id\", \"name\", \"value\")\n\nLet’s see. I think I’ll look at the YDS stat first. How about the mean passing years for each player? The value column is character, so I’ll convert that to numeric and then plot a bar chart with Anthony Brown in green. Remember I grabbed all games that PAC-12 teams played in, so the data will also include all of the PAC-12 non-conference game quarterbacks. I’ll also filter out the plays that were considered “team” QB plays.\n\nlibrary(ggplot2)\nlibrary(forcats)\n\nqb_yards <- p12 %>% filter(name != \" Team\" & stat == \"YDS\") %>%\n  mutate(value = as.numeric(value)) %>%\n  group_by(stat, name) %>%\n  summarize(mean_stat = mean(value)) %>%\n  mutate(name = forcats::fct_reorder(name, mean_stat))\n\nqb_yards %>%\n  ggplot() +\n  geom_col(aes(x=name, y=mean_stat), \n           fill = ifelse(qb_yards$name == \"Anthony Brown\", \"darkgreen\", \"darkgray\")) +\n  coord_flip() +\n  theme_bw() +\n  labs(x = \"Player\", y = \"Mean Passing Yards\")\n\n\n\n\nAnthony averaged just over 200 passing years per game, and it looks like he’s around the middle of the pack. The QBs in the tail end of the chart had to have been 2nd and 3rd string players. I’m not sure how to identify starters vs. non-starters, so I’ll just assume that any player with less than a mean of 100 passing yards probably didn’t get enough playing time for a fair comparison. I’ll take that list of quarterbacks and get all of the QB stats and compare where Anthony is with the mean for each stat. The stats for completions and attempts is in a different format than the rest, so I’ll treat that stat separately.\n\nqb_stats <- \n  p12 %>% \n  filter(name != \" Team\" &\n           !name %in% (qb_yards %>% filter(mean_stat < 100) %>% .$name) &\n           stat != \"C/ATT\") %>%\n  drop_na() %>%\n  mutate(value = as.numeric(value)) %>%\n  group_by(stat, name) %>%\n  summarize(mean_stat = mean(value)) \n\nqb_stats %>%\n  filter(name == \"Anthony Brown\") %>% \n  select(stat, mean_stat) %>%\n  rename(\"Anthony\" = \"mean_stat\") %>%\n  bind_cols(\n    qb_stats %>%\n  group_by(stat) %>%\n  summarize(group_mean = mean(mean_stat)) %>%\n  select(group_mean)\n  )\n\n\n\n  \n\n\n\nHe’s consistently near the mean for each stat except maybe interceptions, which he’s had fewer of. Oregon fans seem to have missed the mark. Now for completions and attempts. First Anthony’s stats.\n\nqb_comp <- \n  p12 %>% filter(name != \" Team\" &\n           !name %in% (qb_yards %>% filter(mean_stat < 100) %>% .$name) &\n             stat == \"C/ATT\") %>%\n  separate(value, c(\"comp\", \"att\")) %>%\n  mutate(comp = as.numeric(comp),\n         att = as.numeric(att),\n         comp_pct = comp/att) %>%\n  group_by(name) %>%\n  summarize(completions = sum(comp),\n    attempts = sum(att),\n    mean_pct = mean(comp_pct))\n\nqb_comp %>% filter(name == \"Anthony Brown\") \n\n\n\n  \n\n\n\nNow the group mean.\n\nqb_comp %>% summarize(group_mean_pct = mean(mean_pct)) %>% .$ group_mean_pct\n\n[1] 0.6082262\n\n\nHe completed 63.7% of his passes, which is little higher than the group mean of 60.8%. Sorry, Oregon fans. I’m still not seeing it."
  },
  {
    "objectID": "posts/oregon_football/index.html#player-transfer-portal",
    "href": "posts/oregon_football/index.html#player-transfer-portal",
    "title": "Oregon Football 2021",
    "section": "Player Transfer Portal",
    "text": "Player Transfer Portal\nFor this data, the API docs state that I need to request data one year at a time. There’s not a lot of historical data available via the API, so I’ll just this year and last year’s data. To get a feel for the data, first is a plot of last year’s portal activity broken out by how many stars each player was as a recruit.\n\nfor (yr in 2021:2022){\n  portal <- httr::GET(\n    url = paste0(\"https://api.collegefootballdata.com/player/portal?year=\", yr), \n    httr::add_headers(\n      Authorization = paste(\"Bearer\", Sys.getenv(\"CFDB_API_TOKEN\"))\n    )\n  )\n  pp <- tibble(data = content(portal, \"parsed\"))\n  if (yr == 2021) {port <- pp %>% unnest_wider(data)}\n  else {port <- port %>% bind_rows(pp %>% unnest_wider(data))}\n}\n\nport <- port %>% mutate(transferDate = as.Date(transferDate),\n                stars = as.factor(replace_na(stars, 0)))\n\nggplot(port %>% filter(season == 2021)) +\n  geom_freqpoly(aes(x=transferDate, color=stars), binwidth=7, size = 1) +\n  theme_bw() +\n  labs(title = \"2021 Transfer Portal Activity\",\n       x = \"Transfer Date\",\n       y = \"Number of Transfers\")\n\n\n\n\nClearly a peak in activity at the end of the regular season, and there’s another one around May. We’re in the middle of the 2022 season, so I’ll do a cumulative tally of the number of transfers this year compared to last.\n\nport %>% \n  mutate(season = as.factor(season)) %>%\n  group_by(season, week = lubridate::floor_date(transferDate, \"week\")) %>% \n  tally %>% \n  mutate(csum = cumsum(n)) %>% \n  ungroup() %>%\n  mutate(week_no = c(1:53, 1:56)) %>%\n  ggplot() +\n  geom_line(aes(x=week_no, y=csum, color=season), size = 1) +\n  labs(title = \"2021 and 2022 Player Portal\",\n       x = \"Week Number\",\n       y = \"Cumulative Transfers\") +\n  theme_bw()\n\n\n\n\nIt wasn’t just my imagination. There are a lot more players in the portal this year compared to last. So now I’ll look at coaching changes."
  },
  {
    "objectID": "posts/oregon_football/index.html#head-coaches",
    "href": "posts/oregon_football/index.html#head-coaches",
    "title": "Oregon Football 2021",
    "section": "Head Coaches",
    "text": "Head Coaches\nMy thought is to calculate coach tenure over time. In other words, for each school, how long has their head coach been there? For that, I think I’ll need to go back quite a few years to make sure I’m capturing coaches with a lot of tenure. To be safe, I want to go back 35 years to 1987. That data isn’t available via the API, but I found this Google Sheet that does. Here are the first few rows and columns to see how the data is structured.\n\nlibrary(googlesheets4) \nlibrary(janitor) # to clean column names\n\ngs4_deauth() # the google sheet is public, so this will skip the authentication by token \n\ncoach <- \n  read_sheet(\n    \"https://docs.google.com/spreadsheets/d/1UXbBC7T4NtN1JwJs6Gk5Qm_y_pI1JXnOFtVuE3Iu3OQ/edit#gid=0\",\n    range = \"A3:AK133\")\n\ncoach[1:6, 1:6]\n\n\n\n  \n\n\n\nHere’s a histogram of coach tenure to get a feel for the data.\n\n# get the data into a long form and clean names\ncl <- coach %>% \n  pivot_longer(cols = 2:37, names_to = \"year\", values_to = \"coach\") %>%\n  janitor::clean_names()\n\n# convert back to wide data for tenure calculation \nct <- cl %>% \n  pivot_wider(names_from = \"fbs_team\", values_from = \"coach\") %>% \n  janitor::clean_names()\n\n# number of distinct coach names for each school\nggplot() +\n  geom_histogram(aes(x=sapply(ct[, 2:131], n_distinct)), \n                 bins=11, binwidth=1, boundary=-0.5) +\n  scale_x_continuous(breaks=2:12) +\n  theme_bw() +\n  labs(x=\"Tenure\", y=\"Count\")\n\n\n\n\nWow, there’s a couple of teams that have only had two head coaches in the last 35 years! Looks like the average tenure is around 7 years, and at least one unlucky team has had 12 coaches - that’s one new coach every three years. Now I’ll look at the trend over time.\n\nlibrary(purrr)\n\nteams <- unique(cl$fbs_team)\n\ntenure <- \n  1:nrow(ct) %>% \n  map_dbl(function(y)\n    mean(1:130 %>% \n           map_dbl(function(x) \n             nrow(cl %>% \n                    filter(fbs_team == teams[x] & \n                             coach == as.character(ct[y, x+1])))))\n  )\n\ncoach_tenure <- \n  tibble(year = 2022:1987, ave_tenure = tenure)\n\nggplot(coach_tenure %>% filter(year>=1997 & year<2023),\n       aes(x=year, y=ave_tenure)) +\n  geom_point() +\n  geom_smooth(method = 'loess', formula = y~x) +\n  theme_bw() +\n  labs(title = \"Mean Head Coach Tenure\",\n       x = \"Year\",\n       y = \"Mean Tenure\")\n\n\n\n\nI just plotted data after 1997 because the first ten years of data will be influenced by having history that goes back ten years or less. This shows that average tenure was around 10 years up until about 2007. After that, coach tenure steadily dropped to be only 5 years as of 2022. That’s startling. The average coach tenure is only 5 years - half of what it used to be.\nAnother way of looking at this would be to think of it in terms of coach turnover per year. I’ll plot that and fit a linear regression line.\n\nturn_over <- tibble(\n  year = 2022:1988,\n  pct_new = 2:36 %>% map_dbl(function(x) sum(coach %>% select(all_of(x)) != coach %>% select(all_of(x+1))) / 130)\n)\n\n# percent of new head coaches\nggplot(turn_over, aes(x=year, y=pct_new)) +\n  geom_point() +\n  geom_smooth(method = lm, formula = y~x) +\n  theme_bw() +\n  labs(title = \"Head Coach Turnover\",\n       x = \"Year\", \n       y = \"Proportion of Teams With New Coach\")\n\n\n\n\nWell, not as striking as the previous plot, but there’s a definite upward trend in turnover. In 2022, more than 1 team in every 5 had a new head coach. Let’s check the model statistics.\n\nsummary(lm(pct_new ~ year, data = turn_over))\n\n\nCall:\nlm(formula = pct_new ~ year, data = turn_over)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.074286 -0.024824  0.003445  0.029549  0.068414 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)  \n(Intercept) -3.0260267  1.2905918  -2.345   0.0252 *\nyear         0.0015923  0.0006437   2.474   0.0187 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03846 on 33 degrees of freedom\nMultiple R-squared:  0.1564,    Adjusted R-squared:  0.1309 \nF-statistic:  6.12 on 1 and 33 DF,  p-value: 0.01869\n\n\nBased on the p-value, the year term is statistically significant. We can interpret the coefficient to mean that each year, the proportion of teams with a new head coach increases by 0.0018. Not that dramatic, and the R-squared value means there’s a lot more going on in explaining the trend in coach turnover than just the passage of time.\nBayes. Should we go there? Nah, leave that for another post."
  },
  {
    "objectID": "posts/oregon_football/index.html#oregon-recruiting",
    "href": "posts/oregon_football/index.html#oregon-recruiting",
    "title": "Oregon Football 2021",
    "section": "Oregon Recruiting",
    "text": "Oregon Recruiting\nMario Cristobal is regarded as a great recruiter, which got me thinking. Since I have coach data going back a ways, let’s see how Oregon recruiting has done over time. The API has information on recruiting, so I’ll grab everything they have on high school players who committed to Oregon. The data contains the star rating for each recruit, and it also has something called a player’s rating. I don’t know how that’s calculated, but it looks like it’s a number between 0 and 1 with larger number indicating a higher caliber recruit. For this purpose, I think a continuous number like rating is better than a categorical “star” rating. I’ll use that to plot the mean recruit rating over time and color the points according to the head coach.\n\nrec <- httr::GET(\n  url = \"https://api.collegefootballdata.com/recruiting/players?classification=HighSchool&team=Oregon\", \n  httr::add_headers(\n    Authorization = paste(\"Bearer\", Sys.getenv(\"CFDB_API_TOKEN\"))\n  )\n)\n\ntibble(data = content(rec, \"parsed\")) %>% \n  unnest_wider(data) %>% \n  unnest_wider(hometownInfo) %>%\n  filter(year != 2001) %>%\n  mutate(latitude = as.numeric(latitude),\n         longitude = as.numeric(longitude),\n         year = year - 1) %>% \n  group_by(year) %>% \n  summarize(mean_rating = mean(rating, na.rm=TRUE)) %>%\n  left_join(\n    cl %>% \n      filter(fbs_team == \"Oregon\") %>% \n      mutate(year = as.integer(year)\n             ), by = \"year\") %>%\n  ggplot() +\n  geom_point(aes(x=year, y=mean_rating, color=coach), size = 2) +\n  geom_smooth(aes(x=year, y=mean_rating), method = \"lm\", formula = y~x) +\n  labs(x = \"Year\", y = \"Mean Recruit Rating\") +\n  theme_bw()\n\n\n\n\nIf you’re a Ducks fan, that chart should make you happy. Mike Bellotti was a little shaky early on but got things off to a nice start. Chip Kelly was really consistent and above the overall trend. The fact that the Duck’s did so well nationally during his time given his recruit ratings were less than 0.9 suggests he was able to make the most of the available talent. Things dropped off a bit with Mark Helfrich, and then there’s Willie Taggart’s one hit wonder. Mario got things back on track, and Dan Lanning’s is certainly off to a great start.\nOk, so that’s Oregon. It would be interesting to see how Oregon does compared to the rest of the teams. But instead of just a comparison of all the teams’ ratings, the observation about Chip Kelly getting a lot of bang for the buck gave me an idea. How about a chart that, for a given year, has the recruit ratings on one axis and a team’s win record on the other? That ought to highlight those teams that over and under perform given the talent on their roster."
  },
  {
    "objectID": "posts/oregon_football/index.html#bang-for-the-buck-chart",
    "href": "posts/oregon_football/index.html#bang-for-the-buck-chart",
    "title": "Oregon Football 2021",
    "section": "Bang For The Buck Chart",
    "text": "Bang For The Buck Chart\nFor this chart, I’ll take a different approach than I did for the last one. I’ll start by getting each team’s current roster, and then go back and get each player’s recruit rating. This will account for players who transferred at some point. I’ll also get all of the recruit sources, not just high school players. I’ll also limit the chart to the 130 FBS teams. I’ll then get each team’s 2021 win-loss record, and combine it all together for the chart. I noticed that the FBS data set also contains team logos, so for some bling, I’ll use team logos as markers. This takes a decent chunk of code, so here goes. One step at a time.\n\nTeam Rosters For 2021\n\nroster <-\n  httr::GET(\n    url = paste0(\"https://api.collegefootballdata.com/roster?year=2021\"),\n    httr::add_headers(\n      accept = \"application/json\",\n      Authorization = paste(\"Bearer\", Sys.getenv(\"CFDB_API_TOKEN\"))\n    )\n  )\n\nrosters <- \n  tibble(data = content(roster, \"parsed\")) %>%\n  unnest_wider(data) %>%\n  drop_na(recruit_ids) %>%\n  unnest_longer(recruit_ids) %>%\n  select(first_name, last_name, team, recruit_ids)\n\n\n\nPlayer Recruit Ratings\n\nyear <- as.character(2017:2021)\nrecruit_source <- c(\"HighSchool\", \"JUCO\", \"PrepSchool\")\n\nfor (yr in year){\n  for (rs in recruit_source){\n    recruiting <-\n      httr::GET(\n        url = paste0(\"https://api.collegefootballdata.com/recruiting/players?classification=\", \n                     rs, \"&year=\", yr),\n        httr::add_headers(\n          Authorization = paste(\"Bearer\", Sys.getenv(\"CFDB_API_TOKEN\"))\n        )\n      )\n    recruits <- tibble(data = content(recruiting, \"parsed\"))\n    if (nrow(recruits) == 0) next\n    if (yr == \"2017\" & rs ==\"HighSchool\"){\n      recruits_all <-\n        recruits %>% \n        unnest_wider(data) %>% \n        select(id, committedTo, rating)}\n    else{\n      recruits_all <-\n        recruits_all %>% \n        bind_rows(recruits %>% \n                    unnest_wider(data) %>% \n                    select(id, committedTo, rating))\n    }\n  }\n}\n\nrecruits_all <- \n  recruits_all %>% mutate(id = as.integer(id))\n\n\n\nGet List Of FBS Teams and Logos\n\nfbs <-\n  httr::GET(\n    url = paste0(\"https://api.collegefootballdata.com/teams/fbs?year=2021\"),\n    httr::add_headers(\n      accept = \"application/json\",\n      Authorization = paste(\"Bearer\", Sys.getenv(\"CFDB_API_TOKEN\"))\n    )\n  )\nfbs_teams <- \n  tibble(data = content(fbs, \"parsed\")) %>%\n  unnest_wider(data) %>% unnest_longer(logos)\n\n# combine datasets\nteam_recruit_rating <-\n  fbs_teams %>% \n  select(school, conference, logos) %>%\n  left_join(rosters, by = c(\"school\" = \"team\")) %>%\n  left_join(recruits_all, by = c(\"recruit_ids\" = \"id\")) %>% \n  drop_na() %>%\n  group_by(school, conference, logos) %>%\n  summarize(mean_rating = mean(rating)) %>%\n  arrange(desc(mean_rating))\n\n\n\nTeam Win-Loss Records For 2021\n\nrecords <- httr::GET(\n  url = \"https://api.collegefootballdata.com/records?year=2021\",\n  httr::add_headers(\n    accept = \"application/json\",\n    Authorization = paste(\"Bearer\", Sys.getenv(\"CFDB_API_TOKEN\"))\n  )\n)\n\nteam_records <- \n  tibble(data = content(records, \"parsed\")) %>%\n  unnest_wider(data) %>%\n  unnest_wider(total) %>% \n  mutate(win_pct = wins / games)\n\n\n\nAt Last, The Chart\n\nlibrary(ggimage)\n\nplot_data <- team_recruit_rating %>%\n left_join(team_records, by = c(\"school\" = \"team\", \"conference\" = \"conference\"))\n\nggplot() +\n  geom_image(data = plot_data, aes(x=mean_rating, y=win_pct, image=logos)) +\n  geom_hline(yintercept = 0.5) +\n  geom_vline(xintercept = mean(plot_data$mean_rating)) +\n  ggtitle(\"2021 Season\") +\n  xlab(\"Mean Player Rating\") +\n  ylab(\"Wins / Total Games\") +\n  theme_bw()\n\n\n\n\nI like this chart.\nLet’s start in the upper left. These teams, Georgia, Alabama, and Ohio State, had the highest player ratings and some of the best records.\nStaying on the top of the chart and moving left, these teams had just as good of a record as the Georgia’s and Alabama’s, but they accomplished it with less talent. These teams, like Cincinatti and Louisiana, were like what I was guessing the Chip Kelly years at Oregon were like.\nThe teams in the lower right are those had a lot of talent but also lost a lot of games. USC, LSU, Florida, Texas, and Nebraska. I’m guessing those head coaches are on the hot seat.\nWe can see Oregon towards the upper right on that kind of middle diagonal where you could say these teams performed about as expected with the talent they had.\nThis chart gave me an idea for another chart, but I’ll save that for another post."
  },
  {
    "objectID": "posts/flow/index.html",
    "href": "posts/flow/index.html",
    "title": "Flow Fields",
    "section": "",
    "text": "In this post, I’ll generate art based on the idea of a material flowing through an area. Again this was inspired by the the aRtsy package, which has a canvas_flow() function to create such art. However, I like to learn how these generative functions work, so I attempt to write my own function. In the canfav_flow() description, it mentions a post by Tyler Hobbs as a good reference. It does a nice job of walking through the concepts using pseudo code, which I’ll implement in R. For motivation, here’s an image from Tyler’s post that will give you an idea of what I’m trying to create."
  },
  {
    "objectID": "posts/flow/index.html#manual-flow-fields",
    "href": "posts/flow/index.html#manual-flow-fields",
    "title": "Flow Fields",
    "section": "Manual Flow Fields",
    "text": "Manual Flow Fields\nTyler Hobbs’ post starts here, and I found it useful to understand the basics. First, I’ll start with a 20x20 matrix, and define the flow as being in the same direction uniformly through the canvas. It helped me to visualize that by plotting a dot for every matrix cell with a line segment extending from the dot that indicates the direction of flow. For a uniform field, I populate the matrix with angles equal to \\(1\\frac{5}{8}\\pi\\).\n\n\nCode\nm <- matrix((13/8)*pi, nrow=20, ncol=20)\n\n\nThen, since I’m going to create the plot a few times, I’ll write a function get_plot() that takes the matrix as an argument, and returns the plot I described. First, the necessary libraries.\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\ntheme_set(theme_bw())\n\nget_plot <- function(M) {\n  as_tibble(M) %>% \n    pivot_longer(everything(), values_to='a') %>% \n    bind_cols(expand.grid(x=1:ncol(M), y=rev(1:nrow(M)))) %>% \n    mutate(xend = x+cos(a)*0.5, yend = y+sin(a)*0.5) %>%\n    ggplot() + \n    geom_point(aes(x=x, y=y)) +\n    geom_segment(aes(x=x, xend=xend, y=y, yend=yend)) +\n    coord_fixed()\n}\n\nget_plot(m)\n\n\n\n\n\nTo generate the art, I want to draw individual lines of varying length through the field. The direction each line travels will be determined by the direction specified by the closest dot. Drawing lines through this field wouldn’t be very interesting, so I’ll add some curvature to it.\n\n\nCode\nfor (i in 1:ncol(m)){m[, i] <- m[, i] + i/20 * pi*(3/4)}\n\np <- get_plot(m)\np\n\n\n\n\n\nSince I’ll generate lines through flow fields multiple times in this post, The function get_lines() takes the flow matrix m as an argument, and also a multiplier segment_length for how long I want the line segments to be and num_lines for how many lines I want to generate. I create a data frame df to store the x and y coordinates of the lines. The initial x and y values are random numbers somewhere within the bounds of the flow matrix. The variable theta contains the value from the flow matrix (an angle) that is closest to the x and y coordinate. A new x and y are then calculated based on theta and these values are added to the data frame. Comments in the code provide the rest of the details.\n\n\nCode\nget_lines <- function(M = NULL, segment_length=5, num_lines=400){\n  for (j in 1:num_lines){\n    dims <- nrow(M)\n    # the actual line segment length \n    seg_len <- sample(dims:dims*segment_length, 1)\n    \n    df <- data.frame(x=rep(0, seg_len), y=rep(0, seg_len), seg = rep(0, seg_len))\n    \n    for (i in 1:seg_len){\n      if (i == 1){\n        x <- runif(1, 1, dims-1) # random starting value for x\n        y <- runif(1, 1, dims-1) # random starting value for y\n        df[i, ] <- c(x, y, j)\n      } else {\n        theta <- M[round(y, 0), round(x, 0)+1] # find the closest angle to x & y\n        x <- x + cos(theta) * 0.1 # 0.1 controls how far apart the new x & y \n        y <- y + sin(theta) * 0.1 # are from the previous values\n        if (x > dims - 1 | x < 1) break  # prevents lines going out of bounds\n        if (y > dims | y < 1) break\n        df[i, ] <- c(x, y, j)\n      }\n    }\n    df <- df %>% filter(seg != 0) # get rid of empty rows if lise was out of bounds\n    ifelse(j==1, df_new <- df, df_new <- df_new %>% bind_rows(df))\n  }\n  df_new\n}\n\n\nFor generating one line, I’ll choose an arbitrarily long line segment to make sure the function doesn’t return a really short line.\n\n\nCode\nset.seed(4)\n\ndf <- get_lines(m, 50, 1)\n\nhead(df)\n\n\n\n\n  \n\n\n\nPlotting is straight forward. I’ll just add the line to the previous plot p.\n\n\nCode\np +\n  geom_path(data=df, aes(x=x, y=y, group=seg), \n            color=\"red\", size=1, lineend=\"round\", linejoin=\"bevel\") +\n  theme(legend.position=\"none\") \n\n\n\n\n\nPlotting several lines of varying length, I get:\n\n\nCode\nset.seed(4)\n\ndf <- get_lines(m, 5, 10)\n\np +\n  geom_path(data=df, aes(x=x, y=y, group=seg), \n            color=\"red\", size=1, lineend=\"round\", linejoin=\"bevel\") +\n  theme(legend.position=\"none\")"
  },
  {
    "objectID": "posts/flow/index.html#algorithm-based-flow-fields",
    "href": "posts/flow/index.html#algorithm-based-flow-fields",
    "title": "Flow Fields",
    "section": "Algorithm-Based Flow Fields",
    "text": "Algorithm-Based Flow Fields\nTo make more visually interesting flow fields like Tyler’s at the beginning of this post, I first tried to continue to manipulate the flow field above by adding different amounts of curvature to different part of the field. I had a difficult time controlling it so that I didn’t have divergent areas and eventually gave up on that approach. Tyler stated that initializing the matrix with “Perlin noise” was common and provided pseudo code for how to do it. Unfortunately, the pseudo code didn’t actually demonstrate how generate Perlin noise from scratch, rather it referred to a hypothetical function to generate the noise and then the pseudo code further manipulated those results. Buggers.\nThe Wikipedia article on Perlin noise stated that the algorithm\n\n…typically involves three steps: defining a grid of random gradient vectors, computing the dot product between the gradient vectors and their offsets, and interpolation between these values.\n\nAnd then provided about a page and a half of pseudo code to describe it. I wasn’t motivated enough to redo that in R, but at least the site showed some visualizations for what it looked like. Here’s one example.\n\n\n\nWikipedia image of Perlin noise.\n\n\nIt didn’t take much Googling to find an R package that generates Perlin noise. The package ambient does the trick with an aptly named noise_perlin() function that couldn’t be simpler to use. I’ll get a 20x20 matrix and plot it with the get_plot() function.\n\n\nCode\nlibrary(ambient)\n\nm <- noise_perlin(c(20, 20))\n\nget_plot(m) + ggtitle(\"Perlin Noise\")\n\n\n\n\n\nWell that’s not very interesting. Recall that we’re plotting these values as radians, and they just don’t vary enough as is. Note the range in values.\n\n\nCode\nrange(m)\n\n\n[1] 0.0000000 0.2337671\n\n\nI’ll rescale them to range between -pi/2 and pi/2 and re-plot the values.\n\n\nCode\nm2 <- scales::rescale(m, to=c(-pi/2, pi/2))\n\nget_plot(m2) + ggtitle(\"Perlin Noise Rescaled\")\n\n\n\n\n\nThat’s better. I’ll increase the scale even more.\n\n\nCode\nm3 <- scales::rescale(m, to=c(-pi, pi))\n\nget_plot(m3)  + ggtitle(\"Perlin Noise Rescaled Even More\")\n\n\n\n\n\nI’ll create a figure like the one in Wikipedia to visualize the matrix values.\n\n\nCode\nreshape2::melt(m3) %>%\n  ggplot() +\n  geom_tile(aes(x=Var1, y=Var2, fill=value)) +\n  coord_fixed()\n\n\n\n\n\nThat’s underwhelming… We need a bigger matrix!\n\n\nCode\nm <- noise_perlin(c(400,400))\n\nreshape2::melt(m) %>%\n  ggplot() +\n  geom_tile(aes(x=Var1, y=Var2, fill=value)) +\n  coord_fixed() +\n  theme_void() +\n  theme(legend.position=\"none\") \n\n\n\n\n\nWe can pass different parameters to the noise_perlin() function to get different types of noise. For example:\n\n\nCode\nm1 <- noise_perlin(c(100,100), fractal='fbm') # the default\nm2 <- noise_perlin(c(100,100), fractal='billow')\nm3 <- noise_perlin(c(100,100), fractal='rigid-multi') \n\np1 <- reshape2::melt(m1) %>%\n  ggplot() +\n  geom_tile(aes(x=Var1, y=Var2, fill=value)) +\n  coord_fixed() +\n  theme_void() +\n  ggtitle(\"FBM\") +\n  theme(legend.position=\"none\", plot.title = element_text(hjust = 0.5))\n\np2 <- reshape2::melt(m2) %>%\n  ggplot() +\n  geom_tile(aes(x=Var1, y=Var2, fill=value)) +\n  coord_fixed() +\n  theme_void() +\n  ggtitle(\"Billow\") +\n  theme(legend.position=\"none\", plot.title = element_text(hjust = 0.5)) \n\np3 <- reshape2::melt(m3) %>%\n  ggplot() +\n  geom_tile(aes(x=Var1, y=Var2, fill=value)) +\n  coord_fixed() +\n  theme_void() +\n  ggtitle(\"Rigid-Multi\") +\n  theme(legend.position=\"none\", plot.title = element_text(hjust = 0.5)) \n\ngridExtra::grid.arrange(p1, p2, p3, nrow=1)\n\n\n\n\n\nFrom these images, we can see that the choice in fractal will have a large impact on the resulting art. Speaking of which, time to make some!\n\n\nCode\nm <- noise_perlin(c(100,100))\nm <- scales::rescale(m, to=c(-pi/2, pi/2))\n\nget_lines(M=m, segment_length=5, num_lines=400)  %>%\n  ggplot() + \n  geom_path(aes(x=x, y=y, group=seg), \n            color=\"black\", alpha=0.5, size=0.5, \n            lineend=\"round\", linejoin=\"bevel\") +\n  theme_void() +\n  theme(legend.position=\"none\") +\n  ggtitle(\"Options: FBM Fractal, pi/2 scale, 400 lines\")\n\n\n\n\n\n\n\nCode\nm <- noise_perlin(c(100,100))\nm <- scales::rescale(m, to=c(-pi/2, pi/2))\n\nget_lines(M=m, segment_length=5, num_lines=400)  %>%\n  ggplot() + \n  geom_path(aes(x=x, y=y, group=seg), \n            color=\"black\", alpha=0.5, size=0.5, \n            lineend=\"round\", linejoin=\"bevel\") +\n  theme_void() +\n  theme(legend.position=\"none\") +\n  ggtitle(\"Options: Same as previous image.\")\n\n\n\n\n\n\n\nCode\nm <- noise_perlin(c(100,100), fractal='billow')\nm <- scales::rescale(m, to=c(-pi, pi))\n\nget_lines(M=m, segment_length=5, num_lines=800)  %>%\n  ggplot() + \n  geom_path(aes(x=x, y=y, group=seg, color=seg), \n            alpha=0.5, size=0.5, \n            lineend=\"round\", linejoin=\"bevel\") +\n  theme_void() +\n  theme(legend.position=\"none\") +\n  ggtitle(\"Options: Billow Fractal, pi scale, 800 lines\")\n\n\n\n\n\n\n\nCode\nm <- noise_perlin(c(100,100), fractal='rigid-multi')\nm <- scales::rescale(m, to=c(-pi/4, pi/4))\n\nget_lines(M=m, segment_length=5, num_lines=800)  %>%\n  ggplot() + \n  geom_path(aes(x=x, y=y, group=seg, color=seg), \n            alpha=0.5, size=1, \n            lineend=\"round\", linejoin=\"bevel\") +\n  theme_void() +\n  theme(legend.position=\"none\") +\n  ggtitle(\"Options: Rigid-Multi Fractal, pi/4 scale, 800 (thicker) lines\")\n\n\n\n\n\n\n\nCode\nm <- noise_perlin(c(100,100))\nm <- scales::rescale(m, to=c(-pi, pi))\ndf <- get_lines(M=m, segment_length=5, num_lines=400)\n\nm <- noise_perlin(c(100,100))\nm <- scales::rescale(m, to=c(-pi, pi))\ndf2 <- get_lines(M=m, segment_length=5, num_lines=400)\n\nggplot() + \n  geom_path(data=df, aes(x=x, y=y, group=seg), \n            color=\"black\", alpha=0.5, size=0.5, \n            lineend=\"round\", linejoin=\"bevel\") +\n  geom_path(data=df2, aes(x=x, y=y, group=seg), \n            color=\"black\", alpha=0.5, size=0.5, \n            lineend=\"round\", linejoin=\"bevel\") +\n  theme_void() +\n  theme(legend.position=\"none\") +\n  ggtitle(\"Options: 2xFBM Fractals, pi scale, 400 lines\")\n\n\n\n\n\n\n\nCode\nggplot() + \n  geom_path(data=df, aes(x=x, y=y, group=seg), \n            color=\"black\", alpha=0.5, size=1, \n            lineend=\"round\", linejoin=\"bevel\") +\n  geom_path(data=df2, aes(x=x, y=y, group=seg), \n            color=\"white\", size=0.5, \n            lineend=\"round\", linejoin=\"bevel\") +\n  theme_void() +\n  theme(legend.position=\"none\") +\n  ggtitle(\"Options: 2xFBM Fractals, White Lines Over Black\")\n\n\n\n\n\nFrom what I’ve seen in other posts, this is just the tip of the iceberg of the type of generative art from flow fields. Here are a few ideas.\n\nPlace starting points for lines on a grid instead of randomly.\nPlace several starting points for lines together in a tight circle.\nPlace starting points on just one side of the canvas.\nChange the color of lines as they increase in length.\nChange the thickness of lines as they increase in length.\n\nTo try any of these, I’ll either need to modify the get_lines() function or just forget about the function and just write some code. I think I’ll just write some code.\n\nGridded Starting Lines\nFor the gridded example, I also made the length of the lines a constant. The grid itself is apparent in the image below, which might not be desirable.\n\n\nCode\nm <- noise_perlin(c(100,100))\nm <- scales::rescale(m, to=c(-pi/2, pi/2))\n\nsp <- expand.grid(y=seq(1, 99, length.out=20), x=seq(1, 99, length.out=20))\n\nfor (j in 1:nrow(sp)){\n  dims <- nrow(m)\n  # the actual line segment length \n  seg_len <- 300\n  \n  df <- data.frame(x=rep(0, seg_len), y=rep(0, seg_len), seg = rep(0, seg_len))\n  \n  for (i in 1:seg_len){\n    if (i == 1){\n      x <- sp[j, 'x']\n      y <- sp[j, 'y']\n      df[i, ] <- c(x, y, j)\n    } else {\n      theta <- m[round(y, 0), round(x, 0)+1] # find the closest angle to x & y\n      x <- x + cos(theta) * 0.1 # 0.1 controls how far apart the new x & y \n      y <- y + sin(theta) * 0.1 # are from the previous values\n      if (x > dims - 1 | x < 1) break  # prevents lines going out of bounds\n      if (y > dims | y < 1) break\n      df[i, ] <- c(x, y, j)\n    }\n  }\n  df <- df %>% filter(seg != 0) # get rid of empty rows if lise was out of bounds\n  ifelse(j==1, df_new <- df, df_new <- df_new %>% bind_rows(df))\n}\n\np1<- \n  ggplot(df_new) + \n  geom_path(aes(x=x, y=y, group=seg), \n            color=\"black\", alpha=0.5, size=0.5, \n            lineend=\"round\", linejoin=\"bevel\") +\n  theme_void() +\n  theme(legend.position=\"none\") +\n  ggtitle(\"Gridded Starting Points\")\n\n# random starting lines for comparison\np2 <-\n  get_lines(M=m, segment_length=5, num_lines=400)  %>%\n  ggplot() + \n  geom_path(aes(x=x, y=y, group=seg), \n            color=\"black\", alpha=0.5, size=0.5, \n            lineend=\"round\", linejoin=\"bevel\") +\n  theme_void() +\n  theme(legend.position=\"none\") +\n  ggtitle(\"Random Starting Points\")\n\ngridExtra::grid.arrange(p1, p2, nrow=1)\n\n\n\n\n\n\n\nStarting Points In A Circle\nI also added jitter to the starting points in an attempt to get rid of the grid effect in the previous image.\n\n\nCode\n# create points in a circle with radius 2\na <- runif(10) * 2 * pi\nr <- 2 * sqrt(runif(10))\natt <- data.frame(x = r*cos(a), y = r*sin(a))\n\nseg_len <- 300\n\nsp <- expand.grid(y=seq(2, 99, length.out=10), x=seq(2, 99, length.out=10))\nsp$x <- jitter(sp$x, amount=4)\nsp$y <- jitter(sp$y, amount=4)\nsp2 <- data.frame(x=rep(0, seg_len*nrow(att)), y=rep(0, seg_len*nrow(att)))\n\nfor (i in 1:nrow(sp)){\n  sp2[(i-1)*10 + 1:10, 'x'] <- sp[i, 'x'] + att[, 'x']\n  sp2[(i-1)*10 + 1:10, 'y'] <- sp[i, 'y'] + att[, 'y']\n}\n\nfor (j in 1:nrow(sp2)){\n  dims <- nrow(m)\n\n  df <- data.frame(x=rep(0, seg_len), \n                   y=rep(0, seg_len), \n                   seg = rep(0, seg_len))\n  \n  for (i in 1:seg_len){\n    if (i == 1){\n      x <- sp2[j, 'x']\n      y <- sp2[j, 'y']\n      df[i, ] <- c(x, y, j)\n    } else {\n      if (x > dims - 1 | x < 1) break  # prevents lines going out of bounds\n      if (y > dims | y < 1) break\n      theta <- m[round(y, 0), round(x, 0)+1] # find the closest angle to x & y\n      x <- x + cos(theta) * 0.1 # 0.1 controls how far apart the new x & y \n      y <- y + sin(theta) * 0.1 # are from the previous values\n\n      df[i, ] <- c(x, y, j)\n    }\n  }\n  df <- df %>% filter(seg != 0) # get rid of empty rows if line was out of bounds\n  ifelse(j==1, df_new <- df, df_new <- df_new %>% bind_rows(df))\n}\n\nggplot(df_new) + \n  geom_path(aes(x=x, y=y, group=seg), \n            color=\"black\", alpha=0.5, size=0.5, \n            lineend=\"round\", linejoin=\"bevel\") +\n  theme_void() +\n  theme(legend.position=\"none\") +\n  ggtitle(\"Starting Points In A Circle\")\n\n\n\n\n\n\n\nStarting Points On Just One Side And Color Gradient Along Lines\nI like the spectral color palette on a black background, so changing up several things in this plot.\n\n\nCode\nsp <- data.frame(x=rep(1, 200), y=seq(1, 99, length.out=100))\n\nfor (j in 1:nrow(sp)){\n  dims <- nrow(m)\n  # the actual line segment length \n  seg_len <- 600\n  \n  df <- data.frame(x=rep(0, seg_len), \n                   y=rep(0, seg_len), \n                   seg = rep(0, seg_len), \n                   step = rep(0, seg_len))\n  \n  for (i in 1:seg_len){\n    if (i == 1){\n      x <- sp[j, 'x']\n      y <- sp[j, 'y']\n      df[i, ] <- c(x, y, j, i)\n    } else {\n      theta <- m[round(y, 0), round(x, 0)+1] # find the closest angle to x & y\n      x <- x + cos(theta) * 0.1 # 0.1 controls how far apart the new x & y \n      y <- y + sin(theta) * 0.1 # are from the previous values\n      if (x > dims - 1 | x < 1) break  # prevents lines going out of bounds\n      if (y > dims | y < 1) break\n      df[i, ] <- c(x, y, j, i)\n    }\n  }\n  df <- df %>% filter(seg != 0) # get rid of empty rows if lise was out of bounds\n  ifelse(j==1, df_new <- df, df_new <- df_new %>% bind_rows(df))\n}\n\nggplot(df_new) + \n  geom_path(aes(x=x, y=y, group=seg, color=step), \n            alpha=0.5, size=0.5, \n            lineend=\"round\", linejoin=\"bevel\") +\n  scale_color_distiller(palette = \"Spectral\") +\n  theme_void() +\n  theme(legend.position=\"none\",\n        panel.background = element_rect(fill = 'black', color = 'black')) +\n  ggtitle(\"One-Sided Starting Points\")\n\n\n\n\n\n\n\nChange Line Thickness\nTo come full circle, I’m using Perlin noise to change the width of the lines.\n\n\nCode\nfor (j in 1:400){\n  dims <- nrow(m)\n  # the actual line segment length \n  seg_len <- sample(dims:dims*5, 1)\n  \n  df <- data.frame(x=rep(0, seg_len), y=rep(0, seg_len), seg = rep(0, seg_len))\n  \n  for (i in 1:seg_len){\n    if (i == 1){\n      x <- runif(1, 1, dims-1) # random starting value for x\n      y <- runif(1, 1, dims-1) # random starting value for y\n      df[i, ] <- c(x, y, j)\n    } else {\n      theta <- m[round(y, 0), round(x, 0)+1] # find the closest angle to x & y\n      x <- x + cos(theta) * 0.1 # 0.1 controls how far apart the new x & y \n      y <- y + sin(theta) * 0.1 # are from the previous values\n      if (x > dims - 1 | x < 1) break  # prevents lines going out of bounds\n      if (y > dims | y < 1) break\n      df[i, ] <- c(x, y, j)\n    }\n  }\n  df <- df %>% filter(seg != 0) # get rid of empty rows if lise was out of bounds\n  df$col <- sample(c(\"#3CAEA3\", \"#F6D55C\", \"#ED553B\"), 1)\n  df$wid <- noise_perlin(c(nrow(df), nrow(df)))[1, ]\n  ifelse(j==1, df_new <- df, df_new <- df_new %>% bind_rows(df))\n}\n\n\nggplot(df_new) + \n  geom_path(aes(x=x, y=y, group=seg), \n            color=df_new$col, size=scales::rescale(df_new$wid, to=c(0.1, 2)),\n            lineend=\"round\", linejoin=\"bevel\") +\n  theme_void() +\n  theme(legend.position=\"none\")"
  },
  {
    "objectID": "presentations/doe.html#outline",
    "href": "presentations/doe.html#outline",
    "title": "DoE Methodology Overview",
    "section": "Outline",
    "text": "Outline\nDesign of Experiments\n\nDesign Options\nEUCOM Scenario Design\n\nRegression\n\nSimple Linear Regression\nAssumptions and Diagnostics\nMultiple Linear Regression\n\nTrades Tool Integration\nTalking Points"
  },
  {
    "objectID": "presentations/doe.html#design-of-experiments",
    "href": "presentations/doe.html#design-of-experiments",
    "title": "DoE Methodology Overview",
    "section": "Design of Experiments",
    "text": "Design of Experiments\nThe NIST definition of Design of Experiments:\n\nA systematic, rigorous approach to problem-solving that applies principles and techniques at the data collection stage so as to ensure the generation of valid, defensible, and supportable conclusions. In addition, all of this is carried out under the constraint of a minimal expenditure of [simulation] runs, time, and money."
  },
  {
    "objectID": "presentations/doe.html#full-factorial-designs",
    "href": "presentations/doe.html#full-factorial-designs",
    "title": "DoE Methodology Overview",
    "section": "Full Factorial Designs",
    "text": "Full Factorial Designs\n\n\nExample Study\n\n3 OMFV systems (main gun, ATGM, and APS)\n2 levels each\n\\(2^{3} = 8\\) AWARS runs\n\nExample Full Factorial Design\n\n\n\nRun\nMain Gun\nATGM\nAPS\n\n\n\n\n1\n25mm\nTOW\nSmoke\n\n\n2\n50mm\nTOW\nSmoke\n\n\n3\n25mm\nJAGM\nSmoke\n\n\n4\n50mm\nJAGM\nSmoke\n\n\n5\n25mm\nTOW\nIron Fist\n\n\n6\n50mm\nTOW\nIron Fist\n\n\n7\n25mm\nJAGM\nIron Fist\n\n\n8\n50mm\nJAGM\nIron Fist\n\n\n\n\nEUCOM Scenario - 14 modernization programs\n\n2 levels each (current vs. future system)\n\\(2^{14} = 16,384\\) AWARS runs\n\nWe need a smaller design!\n\n\n\nLeft: Consider a study in which we are asked to evaluate the relative contribution of three future systems under consideration for including on a optionally manned fighting vehicle. For each system, there are two options: the current system and the future system. Each unique combination of the three systems at both levels can be evaluated in eight simulation runs. This is referred to as a full factorial design and is shown on the table.\nRight: Now consider a study in which there are 14 programs that we wish to evaluate at two levels each. Evaluating every unique combination would require 16,384 simulation runs. Although technically feasible, we can significantly reduce the number of simulation runs by choosing a different design."
  },
  {
    "objectID": "presentations/doe.html#fractional-factorial-designs",
    "href": "presentations/doe.html#fractional-factorial-designs",
    "title": "DoE Methodology Overview",
    "section": "Fractional Factorial Designs",
    "text": "Fractional Factorial Designs\nStart with the full factorial design, re-code with -1 and 1, and show all interactions. Then delete lower half of the design matrix and the resulting co-linear columns.\n\n\n\n\n \n  \n    DP \n    Y_Intercept \n    X1 \n    X2 \n    X3 \n    X1X2 \n    X1X3 \n    X2X3 \n    X1X2X3 \n  \n \n\n  \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    -1 \n  \n  \n    2 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n  \n  \n    3 \n    1 \n    -1 \n    1 \n    -1 \n    -1 \n    1 \n    -1 \n    1 \n  \n  \n    4 \n    1 \n    1 \n    1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    5 \n    1 \n    -1 \n    -1 \n    1 \n    1 \n    -1 \n    -1 \n    1 \n  \n  \n    6 \n    1 \n    1 \n    -1 \n    1 \n    -1 \n    1 \n    -1 \n    -1 \n  \n  \n    7 \n    1 \n    -1 \n    1 \n    1 \n    -1 \n    -1 \n    1 \n    -1 \n  \n  \n    8 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n\n\n\n\n\n\nThe full factorial design from the previous slide is represented here in numeric form. The first column is the design point (DP), or run number. The second column consists of all 1s to capture the y-intercept term that will later be used with regression. X1, X2, and X3 represent the three OMFV systems where a -1 represents the current system (e.g., 25mm main gun), and a 1 represents the future system. The next three columns represent all of the possible 2-way interaction terms. The values in these interaction columns are the product of the two respective individual columns (e.g., column X1X2 is the product of column X1 and X2). The farthest right column represents the 3-way interaction term.\nThe number of design points in a full factorial design can be reduced by deleting the bottom half of the design matrix (shown in blue). However, doing so produces colinear columns in the upper half of the table. For example, the remaining column X1 is the inverse of column X1X3, and so the X1X3 column must be eliminated. The other columns shown in blue must also be eliminated due to co-linearity."
  },
  {
    "objectID": "presentations/doe.html#resulting-design",
    "href": "presentations/doe.html#resulting-design",
    "title": "DoE Methodology Overview",
    "section": "Resulting Design",
    "text": "Resulting Design\nThe resulting design loses higher order interactions1.\n\n\n\n\n \n  \n    DP \n    Y_Intercept \n    X1 \n    X2 \n    X3 \n  \n \n\n  \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    2 \n    1 \n    1 \n    -1 \n    1 \n  \n  \n    3 \n    1 \n    -1 \n    1 \n    1 \n  \n  \n    4 \n    1 \n    1 \n    1 \n    -1 \n  \n\n\n\n\n\nThe EUCOM design matrix is a Resolution V fractional factorial design which includes: - 256 design points.\n\nThe Intercept.\nAll Main effects.\nAll 2-way interactions.\n\n\nThis table is what remains after eliminating the blue rows and columns shown on the previous slide and is known as a fractional factorial design. Although we were able to decrease the number of runs by half, we would only be able to evaluate main effects with this design. This means we would be able to determine the impact of upgrading just the main gun, and the impact of upgrading the ATGM (the main effects), but not the combined impact of upgrading both systems (the 2-way interaction). A similar methodology will be utilized for the AMA study that will reduce the number of runs from 32,768 to only 256. This design is referred to as a Resolution V fractional factorial design and, with it, we will be able to evaluate all main effects and all 2-way interactions (but not 3 or higher way interactions).\nEach row in the table represents a single AWARS run and defines the combination of systems that will be represented in that particular run. A unique set of AWARS output files will then be associated with each run. Using this output, we will establish various metrics to measure the effects associated with that particular combination of systems. Example metrics may include the total number of friendly kills or the end strength of a certain BCT. The relationship between the system levels and a metric may be understood using a variety of mathematical models. A commonly used, and easy to interpret model, is a linear regression model, which will be described in the next several slides.\n\nThis design is technically a Resolution III fractional factorial design."
  },
  {
    "objectID": "presentations/doe.html#simple-linear-regression",
    "href": "presentations/doe.html#simple-linear-regression",
    "title": "DoE Methodology Overview",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nPurpose: describe a relationship between a predictor variable (x) and a response variable (y).\n\\[y = \\beta_{0} + \\beta_{1}x + \\epsilon\\]\nwhere,\n\n\\(\\beta_{0}\\) is the y-intercept\n\\(\\beta_{1}\\) is the slope\n\\(\\epsilon\\) is the error in \\(y\\) not explained by \\(\\beta_{0}\\) and \\(\\beta_{1}\\).\n\n\nThis slide shows the mathematical form of a simple linear regression model. Ultimately, the x’s will represent the Army modernization programs, and the y will represent an AWARS output metric."
  },
  {
    "objectID": "presentations/doe.html#least-squares-method-graphically",
    "href": "presentations/doe.html#least-squares-method-graphically",
    "title": "DoE Methodology Overview",
    "section": "Least Squares Method Graphically",
    "text": "Least Squares Method Graphically\nObjective: Minimize the residual sum of squares (RSS).\n\\(RSS = \\sum\\limits_{i=1}^{n}{(y_{i} - \\hat{y}_{i})^2} = \\sum\\limits_{i=1}^{n}{\\hat\\epsilon_{i}^{2}}\\)\n\n\n\n\n\n\n\nIf we plot a data set consisting of 6 observations of height (the predictor, or x variable) and weight (the response, or y variable) shown with black circles and fit a linear model to these observations, we get the best fit regression line shown in red. The slope of the line, \\(\\beta_1\\), describes the nature of the relationship between height and weight as predicted by the linear model. Since the circles do not fall on the regression line, there is some error associated with this model that we can capture with the residuals (the difference between a predicted and actual weight value). Linear models identify the best fit regression line by minimizing the sum of the squared residuals (squared so that all errors are positive). In other words, it’s finding the line that minimizes the combined lengths of the dashed blue lines, thus finding the line with the least amount of error. This is referred to as the “least squares” method."
  },
  {
    "objectID": "presentations/doe.html#least-squares-mathematically",
    "href": "presentations/doe.html#least-squares-mathematically",
    "title": "DoE Methodology Overview",
    "section": "Least Squares Mathematically",
    "text": "Least Squares Mathematically\nRewrite expression in terms of \\(\\epsilon\\):\n\\(\\epsilon = y - \\beta_{0} - \\beta_{1}x\\)\nIf \\(y= \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{pmatrix}\\) and \\(X= \\begin{pmatrix} 1 & x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{pmatrix}\\) and \\(\\beta= \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}\\)\nThen \\(\\epsilon= \\begin{pmatrix} \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix} = \\begin{pmatrix} y_1-\\beta_0-\\beta_1x_1 \\\\ \\vdots \\\\ y_n-\\beta_0-\\beta_1x_n \\end{pmatrix} = y-X\\beta\\)\nOnce we solve for the coefficients, multiplying them by the predictors gives the estimated response, \\(\\hat{y}\\).\n\\(X\\beta \\equiv \\hat{y}\\)\nNext step: linear algebra.\n\nAfter re-writing the simple linear regression formula in terms of the error, we define three matrices: one each for the \\(x\\), \\(y\\), and \\(\\beta\\) terms. The \\(X\\) matrix is a combination of a column of 1s and a column of \\(x\\) values where each value is a height observation from the data set. The \\(y\\) matrix consists of each of the weight observations. The two coefficients we’re attempting to determine are \\(\\beta_0\\) and \\(\\beta_1\\), which represent the regression line y-intercept and slope, respectively. Combining each of the 6 observations into a system of equations, we can use linear algebra techniques to solve for the \\(\\beta\\) vector."
  },
  {
    "objectID": "presentations/doe.html#qr-decomposition",
    "href": "presentations/doe.html#qr-decomposition",
    "title": "DoE Methodology Overview",
    "section": "QR Decomposition",
    "text": "QR Decomposition\n\\[\nX\\beta \\equiv \\hat{y}\n\\]\n\\[\nX^TX\\beta = X^Ty\n\\]\n\\[\n\\beta = (X^TX)^{-1}X^Ty\n\\]\nSubstitute QR for X and solve.\n\\[\n\\beta = ((QR)^T(QR))^{-1}(QR)^Ty\n\\]\n\\[\n\\beta = (R^TQ^TQR)^{-1}R^TQ^Ty\n\\]\n\\[\n\\beta = (R^TR)^{-1}R^TQ^Ty\n\\]\n\\[\n\\beta = R^{-1}R^{-T}R^TQ^Ty\n\\]\n\\[\n\\beta = R^{-1}Q^Ty\n\\]\nFor a detailed discussion, refer to these slides.\n\nWe can solve for the \\(\\beta\\) vector using a technique called QR decomposition, where the \\(X\\) matrix is “decomposed” into two separate matrices \\(Q\\) and \\(R\\), that multiplied together, equals the \\(X\\) matrix. Solving for \\(\\beta\\) in terms of \\(Q\\) and \\(R\\) is more stable than solving for \\(\\beta\\) in terms of \\(X^TX\\) because solving in terms of \\(X^TX\\) can sometimes produce a singular matrix, which is non-invertible (i.e., \\((X^TX)^{-1}\\) fails)."
  },
  {
    "objectID": "presentations/doe.html#qr-decomposition-in-r",
    "href": "presentations/doe.html#qr-decomposition-in-r",
    "title": "DoE Methodology Overview",
    "section": "QR Decomposition in R",
    "text": "QR Decomposition in R\nIn R, use qr(X) to decompose \\(X\\), and then use solve.qr() to calculate the \\(\\beta\\) vector.\n\nintercept = rep(1, 6)\n\nX = cbind(intercept, df$height)\n\nQR = qr(X)\n\nqr.Q(QR) # the Q matrix\n\n           [,1]        [,2]\n[1,] -0.4082483  0.32712245\n[2,] -0.4082483 -0.26169796\n[3,] -0.4082483 -0.45797143\n[4,] -0.4082483  0.71966938\n[5,] -0.4082483 -0.01635612\n[6,] -0.4082483 -0.31076633\n\nqr.R(QR) # the R matrix\n\n     intercept           \n[1,]  -2.44949 -145.33639\n[2,]   0.00000   20.37973\n\nsolve.qr(QR, df$weight)\n\nintercept           \n62.071429  1.276886 \n\n\n\nWe can demonstrate QR decomposition explicitly in R using the built-in functions qr() and solve.qr() as shown. First, we define the y-intercept column as a matrix of six 1s. We combine that column with the height column to create our \\(X\\) matrix. We then decompose \\(X\\) into the \\(Q\\) and \\(R\\) matrices, and solve for \\(\\beta\\) by multiplying them by the weight matrix. As a result, we see that \\(\\beta_0\\) (the y-intercept) is 62.07 and \\(\\beta_1\\) (the slope) is 1.28."
  },
  {
    "objectID": "presentations/doe.html#the-lm-function-in-r",
    "href": "presentations/doe.html#the-lm-function-in-r",
    "title": "DoE Methodology Overview",
    "section": "The lm() Function in R",
    "text": "The lm() Function in R\n\ncoefficients(summary(lm(weight ~ height, data = df)))\n\n             Estimate Std. Error  t value   Pr(>|t|)\n(Intercept) 62.071429 17.7405059 3.498853 0.02492203\nheight       1.276886  0.2961004 4.312342 0.01252286\n\n\nBecause the system of equations is over-determined (more equations than unknowns), the lm() function also uses QR decomposition.\n\nIn R, we fit a linear model with lm() using a formula notation of the form y ~ x. We can also get the model summary, from which we can extract the coefficients, which exactly match those calculated manually on the previous slide. Other terms include the standard error (or standard deviation) for each estimate, which can be used to calculate confidence intervals. The t-value is (Estimate / Std. Error). Pr(>|t|) is the probability of observing a value at least as extreme as the estimate using a t-distribution, with the null hypothesis that the estimate = 0. For both estimates, we reject the null hypothesis and conclude the estimates are statistically significant at the 95% confidence level."
  },
  {
    "objectID": "presentations/doe.html#linear-regression-assumptions",
    "href": "presentations/doe.html#linear-regression-assumptions",
    "title": "DoE Methodology Overview",
    "section": "Linear Regression Assumptions",
    "text": "Linear Regression Assumptions\nFour assumptions fundamental to linear regression:\n\n\nLinearity: \\(y\\) must be a linear function of the \\(\\beta\\) coefficients.\nHomoscedasticity: The prediction error has a constant variance.\nIndependence: The prediction error is independent of the predictor variables.\nNormality: The prediction error is normally distributed.\n\n\n\nWhen fitting a linear model to a data set, an analyst must always check for violations of the linear model assumptions. The diagnostic checks presented in the next few slides demonstrate the commonly accepted methods for checking for violations."
  },
  {
    "objectID": "presentations/doe.html#example-data",
    "href": "presentations/doe.html#example-data",
    "title": "DoE Methodology Overview",
    "section": "Example Data",
    "text": "Example Data\n\n\n\n\n\n\nLinear Model Summary\n\n\n               Estimate  Std. Error   t value     Pr(>|t|)\n(Intercept)  0.40410902 0.015265025  26.47287 1.920026e-46\nx           -0.04439601 0.003721164 -11.93068 8.424310e-21\n\n\n\nThe height and weight data used up until this point did not violate any of the linear model assumptions, so for demonstration purposes, I created a new set of data that violates three of the four assumptions. The linear regression line is shown on the chart and visually appears to be a reasonable fit. In the model summary, both coefficients have very low p-values indicating statistical significance."
  },
  {
    "objectID": "presentations/doe.html#linearity",
    "href": "presentations/doe.html#linearity",
    "title": "DoE Methodology Overview",
    "section": "Linearity",
    "text": "Linearity\nA regression model is linear if \\(y\\) is a linear function of the \\(\\beta\\) coefficients, not of \\(x\\).\n\n\nExample linear models:\n\n\\(\\beta_{0} + \\beta_{1}x\\)\n\\(\\beta_{0} + \\beta_{1}x + \\beta_{2}x^{2} + \\beta_{3}x^{3}\\)\n\\(\\beta_{0} + \\beta_{1}log(x) + \\beta_{2}sin(x)\\)\n\nExample non-linear models:\n\n\\(\\beta_{0} + x^{\\beta_{1}}\\)\n\\(\\frac{1}{\\beta_{0} + \\beta_{1}x}\\)\n\n\nExample non-linear relationship.\n\n\n\n\n\n\n\n\nIt is important to note the distinction between y being a linear function of x as opposed to being a linear function of the \\(\\beta\\)s. This allows linear models to accommodate non-linear relationships between x and y as along as the coefficients remain linear. We can visually identify non-linearity by plotting the fitted values (the estimated y values) versus the residuals. If we see a non-linear trend in the data as shown in the upper right, we may be able to change the structure of the model by transforming either the predictor or response and improve the fit."
  },
  {
    "objectID": "presentations/doe.html#non-constant-variance",
    "href": "presentations/doe.html#non-constant-variance",
    "title": "DoE Methodology Overview",
    "section": "Non-Constant Variance",
    "text": "Non-Constant Variance\n\nBreusch-Pagan Test\n\ncar::ncvTest(diag.lm)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 61.8781, Df = 1, p = 3.6539e-15\n\n\n\nPlotting the fitted values against the square root of the standardized residuals provides a visual means of identifying non-constant variance. If the variance was constant, the red line would be horizontal and the vertical spread of the points from left to right across chart would also be constant. Instead, the red line curves, and the vertical spread increases from left to right.\nThe Breusch-Pagan test is a statistical test for non-constant variance. The null hypothesis is that the residuals have constant variance, and the alternative is that the error variance changes with the level of the response or with a linear combination of predictors. For this fit, we clearly reject the null hypothesis."
  },
  {
    "objectID": "presentations/doe.html#normality-and-independence",
    "href": "presentations/doe.html#normality-and-independence",
    "title": "DoE Methodology Overview",
    "section": "Normality And Independence",
    "text": "Normality And Independence\n\n\n\nShapiro-Wilk Test\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(diag.lm)\nW = 0.87315, p-value = 9.414e-08\n\n\n\nNo Formal Test For Independence\n\n\n\nA quantile-quantile plot is a visual tool used to compare two distributions: here the theoretical quantiles of normally distributed data on the x axis versus the actual quantiles of the standardized residuals. If the residuals were normally distributed, they would fall on or very near the dashed red line. This plot indicates that the larger residuals depart significantly from a normal distribution.\nThe Shapiro-Wilk test is commonly used to test whether data are normally distributed. After applying the test to the linear model residuals, we reject the null hypothesis that the residuals are normally distributed.\nEvaluating independence can be accomplished by inspecting how the data were generated. An example of violating the independence assumption in combat model results is as follows. Say we wanted to compare the number of IDF missions prosecuted by two BCTs. If the model behavior is such that when one BCT is under fire, the other BCT will support it with its IDF, then the number of IDF missions for each BCT is not independent. If the first BCT is more active, then the second BCT will also tend to be more active. The remedy is to exclude these kinds of metrics from the analysis."
  },
  {
    "objectID": "presentations/doe.html#what-to-do-about-violations",
    "href": "presentations/doe.html#what-to-do-about-violations",
    "title": "DoE Methodology Overview",
    "section": "What To Do About Violations",
    "text": "What To Do About Violations\nIf any of the linear model assumptions are violated, then there are three options:\n\nChange the structure of the model.\n\nTransform the predictor(s) and/or response.\nUse a non-Gaussian distribution.\n\nAccept the violations and use the model anyway.\nUse a non-parametric model.\n\nRandom forest.\nSupport vector machine.\nMany others."
  },
  {
    "objectID": "presentations/doe.html#multiple-linear-regression",
    "href": "presentations/doe.html#multiple-linear-regression",
    "title": "DoE Methodology Overview",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nExpand equation to allow for more predictors:\n\\[\ny = \\beta_{0} + \\beta_{1}x_{1}+ \\beta_{2}x_{2} + ... + \\beta_{(p-1)}x_{(p-1)} + \\epsilon\n\\]\nIn matrix form and in terms of \\(\\epsilon\\):\n\\[\n\\epsilon= \\begin{pmatrix} \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix} = \\begin{pmatrix} y_1-\\beta_0-\\beta_1x_1 - \\beta_2x_2 - ... \\\\ \\vdots \\\\ y_n-\\beta_0-\\beta_1x_n  - \\beta_2x_n - ...  \\end{pmatrix} = y-X\\beta\n\\]\nNote the final equation remains unchanged, so we still solve for the \\(\\beta\\) matrix using QR decomposition.\n\\[\nX\\beta \\equiv \\hat{y}\n\\]\n\nFor a data set that has multiple predictors, we simply increase the size of the \\(X\\) and \\(\\beta\\) matrices to account for the additional terms. In matrix notation, however, the final equation is identical to the equation for simple linear regression. We therefore proceed with the QR decomposition technique as before."
  },
  {
    "objectID": "presentations/doe.html#qr-decomposition-example",
    "href": "presentations/doe.html#qr-decomposition-example",
    "title": "DoE Methodology Overview",
    "section": "QR Decomposition Example",
    "text": "QR Decomposition Example\nOne-hot encode the categorical variables to generate X matrix. Then generate the response variable with the relationship:\n\\[\ny = 10 + 16x_{1}+ 22x_{2} + \\epsilon\n\\]\n\n\n\n\n \n\n\nX Matrix\nY Matrix\n\n  \n    DP \n    Program_1 \n    Program_2 \n    intercept \n    program_1_rep \n    program_2_rep \n    kills \n  \n \n\n  \n    1 \n    AH-64E \n    M2A4 \n    1 \n    0 \n    0 \n    16.314771 \n  \n  \n    2 \n    FARA \n    M2A4 \n    1 \n    1 \n    0 \n    24.368833 \n  \n  \n    3 \n    AH-64E \n    OMFV \n    1 \n    0 \n    1 \n    38.648996 \n  \n  \n    4 \n    FARA \n    OMFV \n    1 \n    1 \n    1 \n    54.362147 \n  \n  \n    5 \n    AH-64E \n    M2A4 \n    1 \n    0 \n    0 \n    12.073207 \n  \n  \n    6 \n    FARA \n    M2A4 \n    1 \n    1 \n    0 \n    18.300250 \n  \n  \n    7 \n    AH-64E \n    OMFV \n    1 \n    0 \n    1 \n    27.357165 \n  \n  \n    8 \n    FARA \n    OMFV \n    1 \n    1 \n    1 \n    46.526398 \n  \n  \n    9 \n    AH-64E \n    M2A4 \n    1 \n    0 \n    0 \n    9.971164 \n  \n  \n    10 \n    FARA \n    M2A4 \n    1 \n    1 \n    0 \n    38.023267 \n  \n  \n    11 \n    AH-64E \n    OMFV \n    1 \n    0 \n    1 \n    35.817967 \n  \n  \n    12 \n    FARA \n    OMFV \n    1 \n    1 \n    1 \n    44.004954 \n  \n\n\n\n\n\n\nIn this example, we’ll pull everything together using a simplified representation of the AMA analytic approach. In white, we have a full factorial design (rows 1-4) replicated three more times (rows 5-12) for a study that includes two modernization programs (FARA and OMFV) at two levels each (the current and future system). We then translate those system levels into 0s and 1s so they can be mathematically represented in a linear regression model, and we add a column of 1s for the y-intercept term to construct the \\(X\\) matrix shown in blue. We then execute an AWARS run for each of the design points and extract our metric of interest - in this case the total number of friendly kills achieved in the run and shown in green.\nNote that this is notional data generated based on the equation shown above the table. We should expect to calculate three \\(\\beta\\) coefficients similar to values of 10, 16, and 22 as shown in the equation (similar, and not exact, due to the error term)."
  },
  {
    "objectID": "presentations/doe.html#the-resulting-coefficients",
    "href": "presentations/doe.html#the-resulting-coefficients",
    "title": "DoE Methodology Overview",
    "section": "The Resulting Coefficients",
    "text": "The Resulting Coefficients\n\n{df.lm = lm(kills ~ program_1_rep + program_2_rep, data=df)}\ncoefficients(df.lm)\n\n  (Intercept) program_1_rep program_2_rep \n     12.72503      14.23376      21.27769 \n\n\nMathematical Interpretation\n\\[\n\\hat{y} = 12.7 + 14.2x_{1}+ 21.3x_{2}\n\\]\nCompare To True Relationship\n\\[\ny = 10 + 16x_{1}+ 22x_{2} + \\epsilon\n\\]\n\nThe \\(X\\) matrix consists of columns 4-6 from the previous slide, and the y matrix is the 7th column. We extract the three \\(\\beta\\) coefficients generated by the lm() function as shown earlier. As expected, our estimated coefficients are similar to the actual coefficients. Next, we’ll see how to interpret these coefficients.\n\n\n\n\n\n\n\n\n\nIf\nResulting Equation\nInterpretation\n\n\n\n\nAH-64E present: \\(x_1=0\\)\nM2A4 present: \\(x_2=0\\)\n\\(\\hat{y}=12.7+14.2(0)+21.3(0)\\)\n\\(\\hat{y}=12.7\\)\nThe AH-64E and FARA kill 12.7 threat units.\n\n\nFARA present: \\(x_1=1\\)\nM2A4 present: \\(x_2=0\\)\n\\(\\hat{y}=12.7+14.2(1)+21.3(0)\\)\n\\(\\hat{y}=28.9\\)\nUpgrading from the AH-64E to the FARA results in 14.2 additional kills.\n\n\nAH-64E present: \\(x_1=0\\)\nOMFV present: \\(x_2=1\\)\n\\(\\hat{y}=12.7+14.2(0)+21.3(1)\\)\n\\(\\hat{y}=34.0\\)\nUpgrading from the M2A4 to the OMFV results in 21.3 additional kills.\n\n\nFARA present: \\(x_1=1\\)\nOMFV present: \\(x_2=1\\)\n\\(\\hat{y}=12.7+14.2(1)+21.3(1)\\)\n\\(\\hat{y}=48.2\\)\nUpgrading both systems results in 35.5 additional kills."
  },
  {
    "objectID": "presentations/doe.html#d-graphical-interpretation",
    "href": "presentations/doe.html#d-graphical-interpretation",
    "title": "DoE Methodology Overview",
    "section": "2D Graphical Interpretation",
    "text": "2D Graphical Interpretation\nThe slopes of the lines are equal to the linear model coefficients.\n\n\n\n\n\n\n\nSince there are only two possible values for \\(x_1\\) and \\(x_2\\), the data points all fall at one of the two extremes."
  },
  {
    "objectID": "presentations/doe.html#d-graphical-interpretation-1",
    "href": "presentations/doe.html#d-graphical-interpretation-1",
    "title": "DoE Methodology Overview",
    "section": "3D Graphical Interpretation",
    "text": "3D Graphical Interpretation\n\n\n\n\n\n\n\nSince we have two predictors, we get a best fit regression plane instead of a line, which is shown on the plot. \\(\\beta_0\\) is the number of kills at the vertical (0,0) axis. The \\(\\beta_1\\) coefficient is the slope of the line that results from the intersection of the plane with the Program 1 vertical plane. Similarly, the \\(\\beta_2\\) coefficient is the slope of the line that results from the intersection of the plane with the Program 2 vertical plane. With no interaction present between the two programs, the plane passes through the values on the (1,1) vertical axis.\nThis chart is interactive, so the mouse can be used to rotate the graphic, and tools are available in the upper left of the plot to interact with the graphic in other ways."
  },
  {
    "objectID": "presentations/doe.html#if-interaction-is-present",
    "href": "presentations/doe.html#if-interaction-is-present",
    "title": "DoE Methodology Overview",
    "section": "If Interaction Is Present",
    "text": "If Interaction Is Present\nAdd “constructive” (or synergistic) interaction to example data.\n\ndf2 = df %>% mutate(kills = ifelse(\n  program_1_rep==1 & program_2_rep==1, \n  kills + 20,\n  kills))\n\ndf2.lm = summary(lm(\n  kills ~ program_1_rep + program_2_rep + program_1_rep:program_2_rep,\n  data=df2))\ncoefficients(df2.lm)\n\n                            Estimate Std. Error  t value    Pr(>|t|)\n(Intercept)                 12.78638   3.831617 3.337072 0.010275786\nprogram_1_rep               14.11107   5.418725 2.604131 0.031415492\nprogram_2_rep               21.15500   5.418725 3.904054 0.004518752\nprogram_1_rep:program_2_rep 20.24539   7.663234 2.641885 0.029626511\n\n\nWithout interaction, upgrading both systems results in 14.1 + 21.2 = 35.3 additional kills.\nWith interaction, upgrading both systems results in 14.1 + 21.2 + 20.2 = 55.7 additional kills.\n\nThis slide demonstrates the effect of “constructive” interaction between two programs. By constructive, I mean that the presence of both future programs in a simulation run results more kills than the sum of the two individual programs. The example on the slide shows that if no interaction is present between two programs, upgrading both systems results in 35.3 kills. With interaction, however, upgrading both systems results in 55.7 kills. A real world example of this phenomenon might be with an upgraded ISR platform and an upgraded IDF munition range. If the upgraded ISR platform is capable of providing more accurate targeting information at longer ranges, the upgraded IDF munition will benefit more from the presence of the upgraded ISR platform than without it.\nDestruction interaction may also be present between two programs. If there was an upgraded IDF munition that greatly improves our ability to kill tanks and an upgraded attack helicopter with a missile that also improves its ability to kill tanks, these two program will compete for the same targets. Upgrading just the IDF munition might result in 15 additional kills, and upgrading just the helicopter might result in 18 additional kills. If a simulation run has both systems upgraded, then there may only be 20 additional kills (which is less than 15 + 18)."
  },
  {
    "objectID": "presentations/doe.html#interaction-visualization",
    "href": "presentations/doe.html#interaction-visualization",
    "title": "DoE Methodology Overview",
    "section": "Interaction Visualization",
    "text": "Interaction Visualization\n\n\n\n\n\n\n\nThis is the same base graphic as shown earlier with the red plane representing the best fit regression plane if no interaction is present. The effect of constructive interaction can be seen by the black points on the (1, 1) vertical axis. Notice that the black points are all greater than the blue points, which causes the plane (also shown in black) to bend in the middle."
  },
  {
    "objectID": "presentations/doe.html#trades-tool-integration",
    "href": "presentations/doe.html#trades-tool-integration",
    "title": "DoE Methodology Overview",
    "section": "Trades Tool Integration",
    "text": "Trades Tool Integration\nTrades tool functional area measure: \\(FA_1 = f(w_1m_1 + w_2m_2 + ...)\\)\nwhere,\n\n\\(w_i\\) are user-supplied weights.\n\\(m_i\\) are AWARS-derived metrics.\n\n\nIn previous briefings, we have discussed a possible approach to combine multiple AWARS-derived metrics into a functional area-level measure. The next few slides demonstrate one way this could be accomplished. The idea is to associate a user-supplied weight value to each of the AWARS metrics as shown on the slide."
  },
  {
    "objectID": "presentations/doe.html#integration-example-1-of-2",
    "href": "presentations/doe.html#integration-example-1-of-2",
    "title": "DoE Methodology Overview",
    "section": "Integration Example (1 of 2)",
    "text": "Integration Example (1 of 2)\nThe following example represents ranked functional area “scores” based on two hypothetical AWARS metrics.\n\n\n\n\n# generate response\nFull_tool = Full %>% mutate(\n  m1 = kills + rnorm(nrow(df), mean=0, sd=1),\n  m2 = 2 + .01*P1 + 0.2*P2 + 0.2*P3 + 0.3*P4 + 0.05*P5 + 0.08*P6 + 0.4*P7 + 1.2*P8 + \n    rnorm(nrow(df), mean=0, sd=1),\n)\n\n# first AWARS metric linear model predictions\nlm1 = lm(m1 ~ P1 + P3 + P4 + P6 + P7 + P8 + P1:P4 + P3:P7, data=Full_tool)\nlm1preds = predict(lm1, newdata=Full_tool %>%\n                     dplyr::select(P1, P2, P3, P4, P5, P6, P7, P8))\n\n# second AWARS metric linear model predictions\nlm2 = lm(m2 ~ P1 + P2 + P3 + P4 + P5 + P6 + P7 + P8, data=Full_tool)\nlm2preds = predict(lm2, newdata=Full_tool %>% \n                     dplyr::select(P1, P2, P3, P4, P5, P6, P7, P8))\n\n\nFirst, I generate two notional AWARS metrics: m1 and m2. Then, I fit linear models to both metrics and get linear model predictions for each."
  },
  {
    "objectID": "presentations/doe.html#integration-example-2-of-2",
    "href": "presentations/doe.html#integration-example-2-of-2",
    "title": "DoE Methodology Overview",
    "section": "Integration Example (2 of 2)",
    "text": "Integration Example (2 of 2)\nAssume a decision-maker values Metric 2 twice as much as Metric 1 and displays the top 5% functional area score.\n\n# re-scale predictions from 0-100\nresults = Full_tool %>% mutate(\n  m1s = scales::rescale(lm1preds, to=c(0,100)),\n  m2s = scales::rescale(lm2preds, to=c(0,100))) %>% \n  dplyr::select(-kills, -m1, -m2)\n\n# if a decision-maker values metric 2 twice as much as m1,\n# and chooses the top 5% scores\nresults %>% mutate(\n  weighted_score = m1s + 2*m2s,\n  FA1_score = scales::rescale(weighted_score, to=c(0,100))) %>%\n  filter(FA1_score>95) %>% \n  dplyr::select(-m1s, -m2s, -weighted_score) %>% arrange(desc(FA1_score))\n\n  P1 P2 P3 P4 P5 P6 P7 P8 FA1_score\n1  1  1  1  1  0  1  1  1 100.00000\n2  1  1  1  1  1  1  1  1  98.79969\n3  1  1  0  1  0  1  1  1  95.46290\n4  0  1  1  1  0  1  1  1  95.30791\n\n\n\nNext, I re-scale the predictions so they are on a common scale (this accounts for the difference between metrics such as total kills versus final end strength). Assuming a decision-maker values metric #2 twice as much as metric #1, I apply those weights to the metrics and return the top 5% of the combined functional area score. From the table, we learn that to achieve a score this high, Programs 1, 4, 6, 7, and 8 must be upgraded. Depending on how high of a score you desire (or how high the cost of a program might be), it may or may not be necessary to upgrade Programs 2, 3, or 5."
  },
  {
    "objectID": "presentations/doe.html#talking-points",
    "href": "presentations/doe.html#talking-points",
    "title": "DoE Methodology Overview",
    "section": "Talking Points",
    "text": "Talking Points\n\n\nA Design of Experiments approach using combat simulations will leverage hundreds of simulation runs to fully explore multiple force structure, capability, and capacity alternatives.\nAnalysis of these results will provide mathematical evidence to evaluate system-specific relative contributions to operational effectiveness.\nSystematically varying capability characteristics and performance inputs enables examination of dependencies between capabilities and their combined impact to operational effectiveness.\nAnalysis of the simulation results leverages multiple well established and mathematically rigorous techniques to test and validate final results.\nQuantitative final results will underpin Army and Joint contribution findings and provide inputs into the greater AMA TRADES Tool to inform senior leader prioritization and budgetary decisions."
  },
  {
    "objectID": "presentations/doe.html#acknowledgements",
    "href": "presentations/doe.html#acknowledgements",
    "title": "DoE Methodology Overview",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis slide show was generated from RStudio using quarto, an open-source scientific and technical publishing system built on Pandoc.\nThe following packages were used to generate much of the content:\n\ntidyverse: Data manipulation.\nkableExtra: Table generation and display.\nplotly: Interactive plots.\nrpart: Regression trees.\nrpart.plot: Regression tree plots.\nrandomForest: Random forest models.\niml: Tools for interpretable machine learning."
  },
  {
    "objectID": "presentations/doe.html#machine-learning-back-up",
    "href": "presentations/doe.html#machine-learning-back-up",
    "title": "DoE Methodology Overview",
    "section": "Machine Learning Back-Up",
    "text": "Machine Learning Back-Up"
  },
  {
    "objectID": "presentations/doe.html#machine-learning",
    "href": "presentations/doe.html#machine-learning",
    "title": "DoE Methodology Overview",
    "section": "Machine Learning",
    "text": "Machine Learning\nWhen normality assumptions cannot be met, an alternative is to apply non-parametric machine learning techniques to fit a model. The notional data below represents the number of friendly kills achieved as a function of the presence or absence of eight notional modernization programs.\n\n\n\n\n \nNotional Data\n  \n    P1 \n    P2 \n    P3 \n    P4 \n    P5 \n    P6 \n    P7 \n    P8 \n    kills \n  \n \n\n  \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    15 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    35 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    10 \n  \n  \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    21 \n  \n  \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    19 \n  \n  \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    40 \n  \n  \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    25 \n  \n  \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    29 \n  \n  \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    44 \n  \n  \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    29 \n  \n  \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    18 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    12 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    10 \n  \n  \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    25 \n  \n  \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    11 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    32 \n  \n  \n    1 \n    0 \n    1 \n    1 \n    0 \n    1 \n    1 \n    0 \n    29 \n  \n  \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    10 \n  \n  \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    29 \n  \n  \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    25 \n  \n  \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n    1 \n    0 \n    21 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    20 \n  \n  \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    16 \n  \n  \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    40 \n  \n  \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    6 \n  \n  \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    35 \n  \n  \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    36 \n  \n  \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    1 \n    1 \n    26 \n  \n  \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    1 \n    0 \n    21 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    7 \n  \n  \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    30 \n  \n  \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    35 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    19 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    25 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    5 \n  \n  \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    24 \n  \n  \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    24 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    35 \n  \n  \n    1 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    31 \n  \n  \n    0 \n    1 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    16 \n  \n  \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    23 \n  \n  \n    1 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    27 \n  \n  \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n    1 \n    26 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    27 \n  \n  \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    20 \n  \n  \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    15 \n  \n  \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n    44 \n  \n  \n    0 \n    1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n    36 \n  \n  \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    1 \n    1 \n    34 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    25 \n  \n  \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    10 \n  \n  \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    35 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    20 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    30 \n  \n  \n    1 \n    0 \n    1 \n    1 \n    0 \n    1 \n    1 \n    1 \n    44 \n  \n  \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    1 \n    1 \n    36 \n  \n  \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    12 \n  \n  \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    27 \n  \n  \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    25 \n  \n  \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    25 \n  \n  \n    1 \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n    1 \n    34 \n  \n  \n    1 \n    0 \n    1 \n    0 \n    1 \n    1 \n    1 \n    0 \n    25 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    22 \n  \n  \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    10 \n  \n  \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    25 \n  \n  \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n    39 \n  \n  \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    29 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    22 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    35 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    15 \n  \n  \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n    26 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    20 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    25 \n  \n  \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    11 \n  \n  \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    23 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    19 \n  \n  \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    6 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    15 \n  \n  \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    16 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    20 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    10 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    25 \n  \n  \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    30 \n  \n  \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    15 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    27 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    20 \n  \n  \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    30 \n  \n  \n    1 \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    1 \n    41 \n  \n  \n    1 \n    1 \n    1 \n    1 \n    0 \n    1 \n    1 \n    1 \n    44 \n  \n  \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    40 \n  \n  \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    1 \n    33 \n  \n  \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    30 \n  \n  \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    16 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    35 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    1 \n    0 \n    1 \n    1 \n    34 \n  \n  \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    1 \n    39 \n  \n  \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    11 \n  \n  \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    8 \n  \n  \n    1 \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    22 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    10 \n  \n  \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    16 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    24 \n  \n  \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    8 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    10 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    29 \n  \n  \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    21 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    1 \n    0 \n    10 \n  \n  \n    1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    29 \n  \n  \n    1 \n    1 \n    1 \n    1 \n    0 \n    1 \n    1 \n    0 \n    29 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    15 \n  \n  \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n    34 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    5 \n  \n  \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    23 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    22 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    17 \n  \n  \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    1 \n    26 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    35 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    29 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    20 \n  \n  \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    6 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    20 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    29 \n  \n  \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    36 \n  \n  \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    16 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n    1 \n    1 \n    1 \n    44 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    30 \n  \n  \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    31 \n  \n  \n    0 \n    1 \n    0 \n    1 \n    0 \n    1 \n    0 \n    1 \n    31 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    35 \n  \n  \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    1 \n    33 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    20 \n  \n  \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n    26 \n  \n  \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    8 \n  \n  \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    20 \n  \n  \n    1 \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    19 \n  \n  \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    19 \n  \n  \n    0 \n    1 \n    0 \n    1 \n    0 \n    1 \n    1 \n    1 \n    36 \n  \n  \n    1 \n    0 \n    1 \n    0 \n    1 \n    0 \n    1 \n    0 \n    15 \n  \n  \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    22 \n  \n  \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    11 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    1 \n    1 \n    1 \n    0 \n    25 \n  \n  \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    15 \n  \n  \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    8 \n  \n  \n    0 \n    1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    21 \n  \n  \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    21 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    1 \n    0 \n    10 \n  \n  \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    21 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    14 \n  \n  \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    14 \n  \n  \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    1 \n    44 \n  \n  \n    1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    44 \n  \n  \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    15 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    22 \n  \n  \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    1 \n    1 \n    26 \n  \n  \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    25 \n  \n  \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    21 \n  \n  \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    34 \n  \n  \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    18 \n  \n  \n    1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    26 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    1 \n    1 \n    30 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    35 \n  \n  \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    21 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    22 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    40 \n  \n  \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    14 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    35 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    14 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    17 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    30 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n    39 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    30 \n  \n  \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    19 \n  \n  \n    1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    1 \n    41 \n  \n  \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    19 \n  \n  \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    1 \n    34 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    29 \n  \n  \n    1 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    1 \n    41 \n  \n  \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    20 \n  \n  \n    0 \n    1 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    16 \n  \n  \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    11 \n  \n  \n    1 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    16 \n  \n  \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    11 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    1 \n    1 \n    0 \n    1 \n    32 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    5 \n  \n  \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    1 \n    41 \n  \n  \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    23 \n  \n  \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    18 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    40 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n    34 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    20 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n    44 \n  \n  \n    0 \n    1 \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n    31 \n  \n  \n    1 \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    1 \n    37 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    25 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    25 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    12 \n  \n  \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    26 \n  \n  \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    30 \n  \n  \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    1 \n    36 \n  \n  \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    40 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    1 \n    1 \n    0 \n    1 \n    37 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    10 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    1 \n    32 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    15 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    20 \n  \n  \n    1 \n    0 \n    1 \n    0 \n    1 \n    0 \n    1 \n    1 \n    30 \n  \n  \n    1 \n    0 \n    1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    40 \n  \n  \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    26 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    37 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    1 \n    0 \n    15 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    7 \n  \n  \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    35 \n  \n  \n    0 \n    1 \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    21 \n  \n  \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    1 \n    33 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    1 \n    1 \n    25 \n  \n  \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    10 \n  \n  \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    26 \n  \n  \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    21 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    30 \n  \n  \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n    1 \n    1 \n    36 \n  \n  \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    19 \n  \n  \n    1 \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    26 \n  \n  \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n    31 \n  \n  \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    29 \n  \n  \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    21 \n  \n  \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    1 \n    31 \n  \n  \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    6 \n  \n  \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    18 \n  \n  \n    1 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    12 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n    1 \n    0 \n    1 \n    39 \n  \n  \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    37 \n  \n  \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    11 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    17 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    7 \n  \n  \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    40 \n  \n  \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    31 \n  \n  \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    25 \n  \n  \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    1 \n    33 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    17 \n  \n  \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    15 \n  \n  \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n    36 \n  \n  \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    11 \n  \n  \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    31 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    1 \n    1 \n    25 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    5 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    1 \n    0 \n    20 \n  \n  \n    1 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    26 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    32 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    1 \n    1 \n    1 \n    0 \n    20 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    25 \n  \n  \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    34 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    24 \n  \n  \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    25 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    7 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    22 \n  \n  \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    20 \n  \n\n\n\n\n\n\nLinear models are commonly used when the primary reason for creating the model is to understand the relationships between the predictors and the response (as opposed to making accurate predictions with new observations). In other words, linear models are highly interpretable once you understand what the coefficients represent. However, if any of the linear model assumptions are violated, an analyst should be prepared to replace the linear model with a non-parametric model. This family of models benefit by not having any underlying assumptions; however, they suffer from a loss in interpretability. Recent efforts machine learning field have improved the interpretability of some non-parametric models, and in the next several slides, I’ll demonstrate the application of a random forest model to notional combat model results."
  },
  {
    "objectID": "presentations/doe.html#linear-model",
    "href": "presentations/doe.html#linear-model",
    "title": "DoE Methodology Overview",
    "section": "Linear Model",
    "text": "Linear Model\nFor clarity, ease of interpretation, and comparison, no error was introduced to the response variable. The relationship between predictors and the response is:\n\\[\n\\small y = 5 + 5P_1 + 0P_2 + 2P_3 + P_4 + 0P_5 + 10P_6 + 5P_7 + 15P_8 - 2(P_3:P_7) + 3(P_1:P_4)\n\\]\nLinear Model Summary\n\n\n                 Estimate   Std. Error       t value   Pr(>|t|)\n(Intercept)  5.000000e+00 3.938500e-15  1.269519e+15 0.00000000\nP1           5.000000e+00 3.662730e-15  1.365102e+15 0.00000000\nP2           6.160437e-15 3.662730e-15  1.681925e+00 0.09400875\nP3           2.000000e+00 3.662730e-15  5.460408e+14 0.00000000\nP4           1.000000e+00 3.662730e-15  2.730204e+14 0.00000000\nP5           8.922105e-15 3.662730e-15  2.435917e+00 0.01565332\nP6           1.000000e+01 3.662730e-15  2.730204e+15 0.00000000\nP7           5.000000e+00 3.662730e-15  1.365102e+15 0.00000000\nP8           1.500000e+01 3.662730e-15  4.095306e+15 0.00000000\nP1:P2       -4.614364e-16 2.589941e-15 -1.781648e-01 0.85875823\nP1:P3       -2.317591e-15 2.589941e-15 -8.948429e-01 0.37185351\nP1:P4        3.000000e+00 2.589941e-15  1.158327e+15 0.00000000\nP1:P5        1.221245e-15 2.589941e-15  4.715340e-01 0.63772889\nP1:P6       -5.218048e-15 2.589941e-15 -2.014736e+00 0.04515494\nP1:P7       -5.842549e-15 2.589941e-15 -2.255861e+00 0.02506725\nP1:P8       -2.664535e-15 2.589941e-15 -1.028801e+00 0.30470793\nP2:P3       -3.608225e-15 2.589941e-15 -1.393169e+00 0.16498129\nP2:P4        2.012279e-15 2.589941e-15  7.769594e-01 0.43802115\nP2:P5       -3.885781e-16 2.589941e-15 -1.500335e-01 0.88087624\nP2:P6       -3.080869e-15 2.589941e-15 -1.189552e+00 0.23551074\nP2:P7       -2.581269e-15 2.589941e-15 -9.966513e-01 0.32003370\nP2:P8       -1.332268e-15 2.589941e-15 -5.144007e-01 0.60749058\nP3:P4       -5.412337e-16 2.589941e-15 -2.089753e-01 0.83466178\nP3:P5       -4.232725e-15 2.589941e-15 -1.634294e+00 0.10363381\nP3:P6        4.718448e-16 2.589941e-15  1.821836e-01 0.85560718\nP3:P7       -2.000000e+00 2.589941e-15 -7.722183e+14 0.00000000\nP3:P8        1.274147e-16 2.589941e-15  4.919600e-02 0.96080794\nP4:P5       -1.857457e-15 2.589941e-15 -7.171810e-01 0.47402665\nP4:P6       -2.047336e-15 2.589941e-15 -7.904950e-01 0.43009393\nP4:P7       -7.903633e-16 2.589941e-15 -3.051665e-01 0.76052918\nP4:P8       -3.206505e-15 2.589941e-15 -1.238061e+00 0.21701901\nP5:P6       -3.259134e-15 2.589941e-15 -1.258381e+00 0.20959379\nP5:P7       -3.770213e-15 2.589941e-15 -1.455713e+00 0.14690385\nP5:P8       -2.833834e-15 2.589941e-15 -1.094169e+00 0.27508284\nP6:P7        4.191099e-16 2.589941e-15  1.618222e-01 0.87159510\nP6:P8       -4.786606e-16 2.589941e-15 -1.848152e-01 0.85354500\nP7:P8        1.009687e-15 2.589941e-15  3.898493e-01 0.69702687\n\n\n\nBefore I fit a random forest model to the data, let’s look at a linear model fit. The equation shows the true relationships between predictors and the response, and note the inclusion of two interaction terms (P3:P7 and P1:P4). In the model summary, the red lines show the linear model coefficients exactly match the equation terms (because no error was introduced)."
  },
  {
    "objectID": "presentations/doe.html#regression-tree",
    "href": "presentations/doe.html#regression-tree",
    "title": "DoE Methodology Overview",
    "section": "Regression Tree",
    "text": "Regression Tree\nMethodology\n\nStarting with the full data set, iterate through each predictor, \\(k_i\\), and split the data into two subsets corresponding to \\(k_i=0\\) and \\(k_i=1\\).\nChoose the split that most decreases the total RSS (prediction error).\nRepeat the process for each resulting subset.\nStop splitting the data using some pre-defined criteria (# observations in a node, tree depth, etc.)\n\n\nA random forest model is classified as an ensemble model because it aggregates the results of many regression tree sub-models. So to understand the forest, we first need to understand each of the individual trees. The methodology for “growing” a tree is summarized on this slide."
  },
  {
    "objectID": "presentations/doe.html#the-first-split",
    "href": "presentations/doe.html#the-first-split",
    "title": "DoE Methodology Overview",
    "section": "The First Split",
    "text": "The First Split\n\nNode Contents: - Mean kills. - Number of observations in the node. - % of observations in the node.\n\nFollowing the methodology shown on the previous slide, we see that the first split is on the P8 factor. Since the factor consists of only two levels with an equal number of observations at each level, the only option is to split the data set into two nodes with 128 observations in each."
  },
  {
    "objectID": "presentations/doe.html#the-full-regression-tree",
    "href": "presentations/doe.html#the-full-regression-tree",
    "title": "DoE Methodology Overview",
    "section": "The Full Regression Tree",
    "text": "The Full Regression Tree\n\nNotice the first split is on P8, the second set of splits on P6, and the third set on P1. These correspond to the three highest multipliers in the predictor-response equation.\n\\[\n\\small y = 5 + 5P_1 + 0P_2 + 2P_3 + P_4 + 0P_5 + 10P_6 + 5P_7 + 15P_8 - 2(P_3:P_7) + 3(P_1:P_4)\n\\]\n\nIf we let the methodology continue to grow the full tree, we get the following set of splits. If we compare the hierarchy of the splits (first P8, then P6, and then P1) to the magnitude of the coefficients in the equation, we see a correlation. We’ll explore this idea in more detail on a later slide when we discuss variable importance.\nHow do we interpret this model? We take an individual observation and follow the rules of the decision tree until we end up in a terminal node. For example, if Programs 8, 6, and 1 are all upgraded to the future system, then P8 = 1, P6 = 1, and P1 = 1. Starting at the top, we first split right, then right again, and then right again until we end up in a terminal node where the mean number of kills is 40. Note that 40 is the highest number of mean kills out of all of the terminal nodes, so we can conclude that upgrading these three programs will result in maximizing the number of kills."
  },
  {
    "objectID": "presentations/doe.html#random-forests",
    "href": "presentations/doe.html#random-forests",
    "title": "DoE Methodology Overview",
    "section": "Random Forests",
    "text": "Random Forests\nRegression trees are relatively weak learners, but they can be improved by growing many trees using bootstrapping and then building an aggregated model (referred to as a bagged model.\nBootstrap Method\nRepeat 100s or 1000s of times: - Randomly sample 2/3 of the data set with replacement. - Fit a regression tree to the bootstrapped data and save the model.\nMake predictions using the mean of the individual model predictions."
  },
  {
    "objectID": "presentations/doe.html#example",
    "href": "presentations/doe.html#example",
    "title": "DoE Methodology Overview",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\n\nOn the left, we see predictions made by a single regression tree on a set of observations of air temperature and ozone levels in New York City over a 5-month period. The relationship between Temp and Ozone is clearly non-linear, and the since tree generally pick up the trend. On the right, 100 bootstrapped trees were grown, and their individual predictions are shown with the red lines. The aggregated random forest model makes predictions based on the average prediction of the 100 individual trees, and that average prediction is shown by the blue line. We can see that the jagged, boxy trend lines are aggregated into a smoother line that visually appears to better capture the nature of the relationship."
  },
  {
    "objectID": "presentations/doe.html#return-to-the-8-program-data-set",
    "href": "presentations/doe.html#return-to-the-8-program-data-set",
    "title": "DoE Methodology Overview",
    "section": "Return To the 8-Program Data Set",
    "text": "Return To the 8-Program Data Set\nRandom Forest Model Summary\n\nset.seed(42)\nlibrary(randomForest)\nfull.rf = randomForest(kills ~ ., data=Full, importance=TRUE)\nfull.rf\n\n\nCall:\n randomForest(formula = kills ~ ., data = Full, importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n          Mean of squared residuals: 8.866894\n                    % Var explained: 90.99\n\n\n\nModel with 500 trees.\n91% variance explained (due to stopping criteria to avoid over-fitting).\n\n\nReturning to the data set with 8 programs at two levels each, we fit a random forest model that consists of 500 trees. From the model summary, we see that the model explains 91% of the variance in the data set. We can think of this variance explained number in a similar way that we think about the r-squared associated with a linear model."
  },
  {
    "objectID": "presentations/doe.html#model-error",
    "href": "presentations/doe.html#model-error",
    "title": "DoE Methodology Overview",
    "section": "Model Error",
    "text": "Model Error\n\nConsider: - The error stabilized. - A forest of trees has less error than a single tree. - Really only need ~100 trees for this data."
  },
  {
    "objectID": "presentations/doe.html#variable-importance",
    "href": "presentations/doe.html#variable-importance",
    "title": "DoE Methodology Overview",
    "section": "Variable Importance",
    "text": "Variable Importance\nA measure of how much reduction in deviance each predictor contributes.\n\nNotice the order from top to bottom is the same as the magnitude of the multipliers in the original equation.\n\\[\n\\small y = 5 + 5P_1 + 0P_2 + 2P_3 + P_4 + 0P_5 + 10P_6 + 5P_7 + 15P_8 - 2(P_3:P_7) + 3(P_1:P_4)\n\\]\nInterpretation: Program 8 contributed the most to kills, followed by Program 6, etc. However, we don’t yet know whether it was a positive or negative contribution."
  },
  {
    "objectID": "presentations/doe.html#partial-dependence",
    "href": "presentations/doe.html#partial-dependence",
    "title": "DoE Methodology Overview",
    "section": "Partial Dependence",
    "text": "Partial Dependence\nShows whether the predictor variable increased or decreased the response. In this case, each program with a non-flat line increased the number of kills.\n\n\nWe can roughly equate these plots to the linear model plots shown earlier that showed the regression lines and associated \\(\\beta\\) values."
  },
  {
    "objectID": "presentations/doe.html#interaction",
    "href": "presentations/doe.html#interaction",
    "title": "DoE Methodology Overview",
    "section": "Interaction",
    "text": "Interaction\nTree-based models automatically include interactions, which we can visualize with the following plot.\n\nInterpretation: All predictors have less than 5% of their variation explained by an interaction.\n\nThe ability of random forest models to automatically include interactions is a great advantage over linear models. Recall the linear model summary on Slide 22. To identify statistically significant main effects and two-way interactions in a linear model with eight factors, we must sift through 37 potential terms, which can be a very challenging task. With random forest models, none of that is necessary. We simply plot the interaction strengths as shown and drill into the individual terms as shown on the next slide."
  },
  {
    "objectID": "presentations/doe.html#interaction-strength",
    "href": "presentations/doe.html#interaction-strength",
    "title": "DoE Methodology Overview",
    "section": "Interaction Strength",
    "text": "Interaction Strength\nWe can also identify what a predictor is interacting with. For example, the plot below shows the strength of interaction with the P1 predictor. As expected, we see the strong interaction with the P4 predictor.\n\n\nRecall from the mathematical equation, we defined an interaction between four programs: P1:P4 and P3:P7. Here we clearly see the interaction between P1 and P4 was picked up by the random forest model. If a linear model is our ultimate goal, we can return to the 37 terms being considered with some valuable information.\n\n\n\nDoE Methodology"
  }
]